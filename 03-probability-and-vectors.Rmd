---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

# Review of probability, random variables, and random vectors

## Probability Basics


Points $\omega$ in $\Omega$ are called **sample outcomes**, **realizations**, or **elements**.

A **set** is a (possibly empty) collection of elements.

* Sets are denoted as a set of elements between curly braces, i.e., $\{\omega_1, \omega_2, \ldots\}$, where the $\omega_i$ are elements of $\Omega$.

Set $A$ is a subset of set $B$ if every element of $A$ is an element of $B$.

* This is denoted as $A \subseteq B$, meaning that $A$ is a subset of $B$. 
* Subsets of $\Omega$ are **events**.

The **null set** or **empty set**, $\emptyset$, is the set with no elements, i.e., $\{\}$.

* The empty set is a subset of any other set.

A function $P$ that assigns a real number $P(A)$ to every event $A$ is a probability distribution if it satisfies three properties:

1. $P(A)\geq 0$ for all $A\in \Omega$
2. $P(\Omega)=P(\omega \in \Omega) = 1$
3. If $A_1, A_2, \ldots$ are disjoint, then $P\left(\bigcup_{i=1}^\infty A_i \right)=\sum_{i=1}^\infty P(A_i)$.

A set of events $\{A_i:i\in I\}$ are **independent** if 
$$P\left(\cap_{i\in J} A_i \right)=\prod_{i\in J} P(A_i ) $$
for every finite subset $J\subseteq I$.


## Random Variables

A **random variable** $Y$ is a mapping/function
$$Y:\Omega\to\mathbb{R}$$
that assigns a real number $Y(\omega)$ to each outcome $\omega$.

* We typically drop the $(\omega)$ part for simplicity.

The **cumulative distribution function (CDF)** of $Y$, $F_Y$, is a function $F_Y:\mathbb{R}\to [0,1]$ defined by $$F_Y (y)=P(Y \leq y).$$

* The subscript of $F$ indicates the random variable the CDF describes.
* E.g., $F_X$ denotes the CDF of the random variable $X$ and $F_Y$ denotes the CDF of the random variable $Y$.
* The subscript can be dropped when the context makes it clear what random variable the CDF describes.

The support of $Y$, $\mathcal{S}$, is the smallest set such that $P(Y\in \mathcal{S})=1$.

### Discrete random variables

$Y$ is a **discrete** random variable if it takes countably many values $\{y_1, y_2, \dots \} = \mathcal{S}$.  

The **probability mass function (pmf)** for $Y$ is $f_Y (y)=P(Y=y)$, where $y\in \mathbb{R}$, and must have the following properties:

1. $0 \leq f_Y(y) \leq 1$.
2. $\sum_{y\in \mathcal{S}} f_Y(y) = 1$.

Additionally, the following statements are true:

* $F_Y(c) = P(Y \leq c) = \sum_{y\in \mathcal{S}:y \leq c} f_Y(y)$.
* $P(Y \in A) = \sum_{y \in A} f_Y(y)$ for some event $A$.
* $P(a \leq Y \leq b) = \sum_{y\in\mathcal{S}:a\leq y\leq b} f_Y(y)$.

The expected value, mean, or first moment of $Y$ is defined as 
$$E(Y) = \sum_{y\in \mathcal{S}} y f_Y(y),$$
assuming the sum is well-defined.

The **variance** of $Y$ is defined as 
$$var(Y)=E(Y-E(Y))^2== \sum_{y\in \mathcal{S}} (y - E(Y))^2 f_Y(y).$$
The **standard deviation** of Y is $$SD(Y)=\sqrt{var(Y)  }.$$

#### Example (Bernoulli)
A random variable $Y\sim \mathsf{Bernoulli}(\pi)$ if $\mathcal{S} = {0, 1}$ and $P(Y = 1) = \pi$, where $\pi\in (0,1)$.

The pmf of a Bernoulli random variable is $$f_Y(y) = \pi^y (1-\pi)^{(1-y)}.$$

Determine $E(Y)$ and $var(Y)$.

### Continuous random variables

$Y$ is a continuous random variable if there exists a function $f_Y (y)$ such that: 

1. $f_Y (y)\geq 0$ for all $y$,
2. $\int_{-\infty}^\infty f_Y (y)  dy = 1$,
3. $a\leq b$, $P(a<Y<b)=\int_a^b f_Y (y)  dy$.  

The function $f_Y$ is called the **probability density function (pdf)**.  

Additionally, $F_Y (y)=\int_{-\infty}^y f_Y (y)  dy$ and $f_Y (y)=F'_Y(y)$ for any point $y$ at which $F_Y$ is differentiable. 

The expected value of a continuous random variables $Y$ is defined as 
$$E(Y)= \int_{-\infty}^{\infty} y f_Y(y)  dy = \int_{y\in\mathcal{S}} y f_Y(y).$$
assuming the integral is well-defined.

The **variance** of a continuous random variable $Y$ is defined by 
$$var(Y)=E(Y-E(Y))^2=\int_{-\infty}^{\infty} (y - E(Y))^2 f_Y(y)  dy = \int_{y\in\mathcal{S}} (y - E(Y))^2 f_Y(y) dy.$$
The **standard deviation** of Y is $$SD(Y)=\sqrt{var(Y)  }.$$

### Useful facts for transformation of random variables

Let $Y$ be a random variable and $c\in\mathbb{R}$ be a constant. Then:

* $E(cY) = c E(Y)$
* $E(c + Y) = c + E(Y)$
* $var(cY) = c^2 var(Y)$
* $var(c + Y) = var(Y)$

## Multivariate distributions

### Basic properties

Let $Y_1,Y_2,\ldots,Y_n$ denote $n$ random variables with supports $\mathcal{S}_1,\mathcal{S}_2,\ldots,\mathcal{S}_n$, respectively.

If the random variables are jointly (all) discrete, then the joint pmf $f(y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$  satisfies the following properties:

1. $0\leq f(y_1,\ldots,y_n )\leq 1$,
2. $\sum_{y_1\in\mathcal{S}_1}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\sum_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n)$.

In this context,

$$E(Y_1 \cdots Y_n)=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n}y_1 \cdots y_n  f(y_1,\ldots,y_n).$$

In general,
$$E(g(Y_1,\ldots,Y_n))=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n),$$
where $g$ is a function of the random variables.

If the random variables are jointly continuous, then $f(y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$  is the joint pdf if it satisfies the following properties:

1. $f(y_1,\ldots,y_n ) \geq 0$,
2. $\int_{y_1\in\mathcal{S}_1}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) dy_n \cdots dy_1 = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\int \cdots \int_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n) dy_n\ldots dy_1$.

In this context,

$$E(Y_1 \cdots Y_n)=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} y_1 \cdots y_n  f(y_1,\ldots,y_n) dy_n \ldots dy_1.$$

In general,
$$E(g(Y_1,\ldots,Y_n))=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n) dy_n \cdots dy_1,$$
where $g$ is a function of the random variables.

### Marginal distributions

If the random variables are jointly discrete, then the marginal pmf of $Y_1$ is $$f_{Y_1}(y_1)=\sum_{y_2\in\mathcal{S}_2}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n).$$

Similarly, if the  random variables are jointly continuous, then the marginal pdf of $Y_1$ is $$f_{Y_1}(y_1)=\int_{y_2\in\mathcal{S}_2}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n) dy_n \cdots dy_2.$$


### Independence of random variables
Random variables $X$ and $Y$ are independent if
$$F(x, y) = F_X(x) F_Y(y).$$

Alternatively, $X$ and $Y$ are independent if
$$f(x, y) = f_X(x)f_Y(y).$$

### Conditional distributions
Let $X$ and $Y$ be random variables. The conditional distribution of $X$ given $Y = y$, denoted $X|Y=y$ is
$$ f(x|y) = f(x, y)/f_{Y}(y).$$

### Covariance 

The covariance between random variables $X$ and $Y$ is 
$$cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).$$

### Useful facts for transformations of multiple random variables

Let $a$ and $b$ be scalar constants.  Then:

-	$E(aY)=aE(Y)$
-	$E(a+Y)=a+E(Y)$
-	$E(aY+bZ)=aE(Y)+bE(Z)$
-	$var(aY)=a^2 var(Y)$
-	$var(a+Y)=var(Y)$
-	$cov(aY,bZ)=ab cov(Y,Z).$
-	$var(Y+Z)=var(Y)+var(Z)+2cov(Y,Z).$

## Random vectors

### Definition
Let $\mathbf{y}=(Y_1,Y_2,\dots,Y_n )^T$ be an $n\times1$ vector of random variables.  $\mathbf{y}$ is a random vector.

* A vector is always defined to be a column vector, even if the notation is ambiguous.

### Mean, variance, and covariance

The mean of a random vector is
$$E(\mathbf{y})=\begin{pmatrix}E(Y_1)\\E(Y_2)\\\vdots\\E(Y_n)\end{pmatrix}.$$

The variance (covariance) of a random vector is
$$\begin{aligned}
var(\mathbf{y}) &= E(\mathbf{y}\mathbf{y}^T )-E(\mathbf{y})E(\mathbf{y})^T\\
&= \begin{pmatrix}var(Y_1) & cov(Y_1,Y_2) &\dots &cov(Y_1,Y_n)\\cov(Y_2,Y_1 )&var(Y_2)&\dots&cov(Y_2,Y_n)\\\vdots&\vdots&\vdots&\vdots\\
cov(Y_n,Y_1)&cov(Y_n,Y_2)&\dots&var(Y_n)\end{pmatrix}\end{aligned}.$$

Let $\mathbf{x} = (X_1, X_2, \ldots, X_n)^T$ and $\mathbf{y} = (Y_1, Y_2, \ldots, Y_n)^T$ be $n\times 1$ random vectors.

The covariance between two random vectors is 
$$cov(\mathbf{x}, \mathbf{y}) = E(\mathbf{x}, \mathbf{y}^T) - E(\mathbf{x}) E(\mathbf{y})^T.$$

## Properties of transformations of random vectors

Define:

* $\mathbf{a}$ to be  an $n\times 1$ vector of constants
* $A$ to be an $m\times n$ matrix of constants
* $\mathbf{x}=(X_1,X_2,\ldots,X_n)^T$ to be an $n\times 1$ random vector
* $\mathbf{y}=(Y_1,Y_2,\ldots,Y_n)^T$ to be an $n\times 1$ random vector
*	$\mathbf{z}=(Z_1,Z_2,\ldots,Z_n)^T$ to be an $n\times 1$ random vector
* $0_{n\times n}$ to be an $n\times n$ matrix of zeros.

Then:

*	$E(A\mathbf{y})=AE(\mathbf{y}), E(\mathbf{y}A^T )=E(\mathbf{y}) A^T.$
*	$E(\mathbf{x}+\mathbf{y})=E(\mathbf{x})+E(\mathbf{y})$
*	$var(A\mathbf{y})=A\ var(\mathbf{y}) A^T$
*	$cov(\mathbf{x}+\mathbf{y},\mathbf{z})=cov(\mathbf{x},\mathbf{z})+cov(\mathbf{y},\mathbf{z})$
*	$cov(\mathbf{x},\mathbf{y}+\mathbf{z})=cov(\mathbf{x},\mathbf{y})+cov(\mathbf{x},\mathbf{z})$
*	$cov(A\mathbf{x},\mathbf{y})=A\ cov(\mathbf{x},\mathbf{y})$
* $cov(\mathbf{x},A\mathbf{y})=cov(\mathbf{x},\mathbf{y}) A^T$
* $var(a)= 0_{n\times n}$
* $cov(\mathbf{a},\mathbf{y})=0_{n\times n}$
* $var(\mathbf{a}+\mathbf{y})=var(\mathbf{y})$

## Multivariate normal (Gaussian) distribution

### Definition
$\mathbf{y}=(Y_1,\dots,Y_n )^T$ has a multivariate normal distribution with mean $\mathbf{\mu}$ (an $n\times 1$ vector) and covariance $\mathbf{\Sigma}$ (an $n\times n$ matrix) if the joint pdf is

$$f(\mathbf{y})=\frac{1}{(2\pi)^{n/2} |\mathbf{\Sigma}|^{1/2} }  \exp\left(-\frac{1}{2} (\mathbf{y}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{y}-\mathbf{\mu})\right).$$
Note that $\mathbf{\Sigma}$ must be symmetric and positive definite.

We would denote this as $\mathbf{y}\sim N(\mathbf{\mu},\mathbf{\Sigma)}$.

### Useful facts

**Important fact**:  A linear function of a multivariate normal random vector (i.e., $\mathbf{a}+A\mathbf{y}$) is also multivariate normal (though it could collapse to a single random variable if $A$ is a $1\times n$ vector).  

**Application**:  Suppose that $\mathbf{y}\sim N(\mu,\Sigma)$. For an $m\times n$ matrix of constants $A$, $A\mathbf{y}\sim N(A\mu,A\Sigma A^T)$.

## Example 1

### Bernoulli distribution

A random variable $Y\sim \mathrm{Bernoulli}(\theta)$ when $\mathcal{S} = \{0, 1\}$ and the pmf of a Bernoulli random variable is
$$f(y\mid\theta)=\theta^y (1-\theta)^{(1-y)}.$$

* Determine $E(Y)$\vspace{2in}$$\\[4in]$$
* Determine $var(Y)$\vspace{2in}$$\\[4in]$$

### Binomial distribution

A random variable $Y\sim \mathrm{Bin}(n,\theta)$ when $\mathcal{S}=\{0,1,2,\ldots,n\}$ and the pmf is $$f(y\mid\theta) = {n \choose y} \theta^y (1-\theta)^{(n-y)}.$$

Alternatively, let $Y_1,Y_2,\ldots,Y_n\stackrel{i.i.d.}{\sim} \mathrm{Bernoulli}(\theta)$. Then $Y=\sum_{i=1}^n Y_i \sim \mathrm{Bin}(n,\theta)$.

* Determine $E(Y)$ $$\\[4in]$$
* Determine $var(Y)$ $$\\[4in]$$

Assume $Y\sim \mathrm{Bin}(20,0.4)$. 

* Determine $F(8)$. $$\\[4in]$$
* Determine $P(8\leq Y \leq 10)$. $$\\[4in]$$

### Poisson Distribution

$Y \sim \mathrm{Poisson}(\theta)$ when $\Omega=\{0,1,2,\ldots\}$ and $$f(y\mid\theta)=\frac{1}{y!} \theta^y e^{-\theta}.$$

* Determine $E(Y)$ $$\\[4in]$$
* Determine $var(Y)$ $$\\[4in]$$

Assume $Y\sim \mathrm{Poisson}(4)$. 

* Determine $F(12)$ $$\\[4in]$$
* Determine $P(15\leq Y \leq 20)$. $$\\[4in]$$

## Example 2

Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers.  Let $Y_1$ denote the proportion of the capacity of the bulk tank that is available after the tank is stocked at the beginning of the week.  Because of the limited supplies, $Y_1$ varies from week to week.  Let $Y_2$ denote the proportion of the capacity of the bulk tank that is sold during the week.  Because $Y_1$ and $Y_2$ are both proportions, both variables are between 0 and 1.  Further, the amount sold, $y_2$, cannot exceed the amount available, $y_1$.  Suppose the joint density function for $Y_1$ and $Y_2$ is given by 
$$f(y_1,y_2 )=3y_1;\ 0 \leq y_2\leq y_1\leq 1.$$

### Problem 1
Determine $P(0\leq Y_1\leq 0.5;\ 0.25\leq Y_2)$
$$\\[4in]$$

### Problem 2
Determine $f_{Y_1 }$ and $f_{Y_2 }$
$$\\[4in]$$

### Problem 3
Determine $E(Y_1)$ and $E(Y_2)$
$$\\[4in]$$

### Problem 4
Determine $var(Y_1)$ and $var(Y_2)$
$$\\[4in]$$

### Problem 5
Determine $E(Y_1 Y_2)$
$$\\[4in]$$

### Problem 6
Determine $cov(Y_1,Y_2)$
$$\\[4in]$$

### Problem 7
Determine the mean and variance of $\mathbf{a}^T \mathbf{y}$, where $\mathbf{a}=(1,-1)^T$ and $\mathbf{y}=(Y_1,Y_2 )^T$.  This is the expectation and variance of the difference between the amount of gas available and the amount of gas sold.
$$\\[4in]$$