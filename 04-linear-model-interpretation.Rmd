# Interpreting a fitted linear model {#interp-chapter}

```{r, include=FALSE}
# change Console output behavior
# knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
knitr::opts_chunk$set(collapse = TRUE)
```

Interpreting a fitted model is a critical part of a regression analysis and aids us in determining the roles and impact each variable plays in describing the behavior of the response variable.

## Standard mathematical interpretation

The standard approach to interpreting the coefficients of a fitted linear model is to consider the expected change in the response in relation to changes in the regressors in the model.

Consider the typical multiple linear regression model of the response
\[
\begin{equation}
Y=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon.(\#eq:mlr-equation-ch4)
\end{equation}
\]

As discussed in Chapter \@ref(linear-model-estimation), we treat the values of our regressor variables as being fixed, known values. The error term is treated as a random variable, and consequently, the response variable is also a random variable. Additionally, we assume that the errors all have mean 0, conditional on the values of the regressor variables. More formally, we write this assumption as

\begin{equation}
E(\epsilon \mid X_1, X_2, \ldots, X_{p-1})=0.(\#eq:mean-error-assumption)

\end{equation}

Recall that we use the notation $\mathbb{X} = \{X_1,\ldots,X_{p-1}\}$ to denote the set of all regressors, which will help us simplify the derivations below. Thus, the assumption in Equation \@ref(eq:mean-error-assumption) can be expressed as $E(\epsilon \mid \mathbb{X})=0$. Using the assumption in Equation \@ref(eq:mean-error-assumption) and applying it to the model in Equation \@ref(eq:mlr-equation-ch4), we see that
\[
\begin{align}
& E(Y\mid X_1, X_2, \ldots, X_{p-1}) \\
&= E(Y \mid \mathbb{X}) \\
&= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon \mid \mathbb{X}) \\
&= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}\mid \mathbb{X}) + E(\epsilon \mid \mathbb{X}) \\
&=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}
\end{align}
\]
since all terms in the first summand of line 4 are fixed, non-random values conditional on $\mathbb{X}$ and the second summand is 0 by assumption. If you are rusty with properties of random variables, consider reviewing the material in Appendix \@ref(prob-review).

Using the facts above, we discuss interpretation of simple linear regression models, multiple linear regression models with basic numeric predictors as regressors, and interpretation for parallel and separate lines regression model. 

## Coefficient interpretation in simple linear regression
Suppose we have a simple linear regression model, so that
\begin{equation}
E(Y\mid X)=\beta_0 + \beta_1 X. (\#eq:slr-equation)
\end{equation}
The interpretations of the coefficients are:

- $\beta_0$ is the expected response when the regressor is 0, i.e., $\beta_0=E(Y\mid X=0)$.
- $\beta_1$ is the expected change in the response when the regressor increases 1 unit, i.e., $\beta_1=E(Y\mid X=x^*+1)-E(Y\mid X=x^*)$, where $x^*$ is a fixed real number.

Regarding the interpretation of $\beta_0$, from the regression model in Equation \@ref(eq:slr-equation), notice that
\[
\begin{align}
E(Y\mid X = 0) &= \beta_0 + \beta_1 \cdot 0 \\
&= \beta_0.
\end{align}
\]
This is why $\beta_0$ is the expected value of the response variable when the regressor is zero.

Similarly, for $\beta_1$, we notice that 
\[
\begin{align}
E(Y\mid X=x^*+1)-E(Y\mid X=x^*) &= [\beta_0 + \beta_1 (x^* + 1)] - [\beta_0 + \beta_1 x^*] \\
&= \beta_1.
\end{align}
\]
Thus, $\beta_1$ literally equals the change in the expected response when the regressor increases by 1 unit.

To apply the interpretations given above, we interpret the simple linear regression model fit to the `penguins` data in Section \@ref(s:penguins-slr). From Section \@ref(s:penguins-slr), the fitted simple linear regression model of $\mathtt{body\_mass\_g}$ regressed on $\mathtt{body\_mass\_g}$ is
\[
\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g})=26.9+0.004 \,\mathtt{body\_mass\_g}.
\]
Some basic interpretations of the coefficients are:

- Intercept: The expected bill length of a penguin with a body mass of 0 grams is 26.9 mm. We discussed the absurdity of this interpretation in Section \@ref(s:penguins-slr).
- $\mathtt{body\_mass\_g}$: If two penguins are identical except that one penguin is 1 gram heavier, then we would expect the heavier penguin to have a bill length 0.004 mm longer.

## Coefficient interpretation for first-order multiple linear regression models {#interp-1st-order-ml}
Suppose we have a multiple linear regression model with $p-1$ regressors, so that
\begin{equation}
E(Y\mid X_1,\ldots,X_{p-1})=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1}.(\#eq:mlr-equation)
\end{equation}
Relying on the definition of $\mathbb{X}$, we denote the set of regressors without $X_j$ as $\mathbb{X}_{-j} = \mathbb{X}\setminus\{X_j\}$.

The interpretations of the coefficients from the model in Equation \@ref(eq:mlr-equation) are:

- $\beta_0$ is the expected response when all regressors are 0, i.e., $\beta_0=E(Y\mid X_1=0,\ldots,X_{p-1}=0)$.
- $\beta_j$, $j = 1,\ldots,p-1$, represents the expected change in the response when regressor $j$ increases 1 unit and the other regressors stay the same, i.e., $\beta_j=E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*)$ where $\mathbf{x}_{-j}^*=[x^*_1,\ldots,x_{j-1}^*,x_{j+1}^*,\ldots,x_{p-1}^*]\in \mathbb{R}^{p-2}$ is a vector with $p-2$ fixed values (the number of regressors excluding $X_j$) and $x_j^*$ is a fixed real number. The non-intercept coefficients of a multiple linear regression model are known as **partial slopes**. 

Regarding the interpretation of $\beta_0$, from the regression model in Equation \@ref(eq:mlr-equation), notice that
\[
\begin{align}
E(Y\mid X_1=0,\ldots,X_{p-1}=0) &= \beta_0 + \beta_1 \cdot 0 + \cdots + \beta_{p-1} \cdot 0\\
&= \beta_0.
\end{align}
\]

It is quite common for the mathematical interpretation of the intercept to be nonsensical because we are extrapolating outside the range of the observed data.

For $\beta_j$, $j = 1,\ldots, p-1$, we notice that 
\[
\begin{align}
& E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j})\\
&=  \biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_j(x^*_j+1) + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl] \\
&\quad -\biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_jx^*_j + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl]\\
&= \beta_j.
\end{align}
\]

A notable problem with the mathematical interpretation of multiple regression models is that a single predictor can be used more than once in the model. E.g., in the 2nd-degree polynomial regression model \[E(Y\mid X) = \beta_0 + \beta_1 X + \beta_2 X^2,\] $X$ is used in both the second and third terms. So it is not possible to increase $X$ while keeping $X^2$ fixed. The mathematical interpretation given in this section is applicable to first-order linear regression models (cf. Section \@ref(model-types)), where a *first-order linear regression model* is a multiple linear regression model in which no regressor is a function of any other regressor.

The regressors used in our model are often observational in nature, meaning that we do not control them. Thus, it doesn't make sense to say "we increase variable $X$ by 1 unit". An alternative approach, alluded to by @lmwr2, is to consider the expected response difference between observations that are identical with respect to all attributes (or at least the attributes we include in our model) except the variable under consideration, which varies by only a single unit. While mathematically, the result is the same, the interpretation is more philosophically palatable.

Regardless, interpretation is a bit of an art. There can be many correct ways to interpret a coefficient or the impact of a variable. Always double-check that the mathematics of your model supports your conclusions.

To illustrate the interpretations give above, we interpret the first-order multiple linear regression model fit to the `penguins` data in \@ref(s:penguins-mlr). The fitted multiple linear regression model is
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm})\\
&=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\end{align}
\]

Some basic interpretations of the coefficients are:

- Intercept: We expect a penguin with a body mass of 0 grams and a flipper length of 0 mm to to have a bill length of -3.44 mm.
- $\mathtt{body\_mass\_g}$: For two penguins that are identical except that one penguin has a body mass 1 gram larger, we expect the heavier penguin to have a bill length 0.0007 mm longer than the other penguin.
- $\mathtt{flipper\_length\_mm}$: For two penguins that are identical except that one penguin has a flipper length 1 mm longer, we expect the penguin with longer flippers to have a bill length 0.22 mm longer.

## Effect plots

An effect

## Roles of regressor variables

Did you notice that the estimated coefficients for the intercept and $\mathtt{body\_mass\_g}$ changed between the fitted simple linear model and the fitted multiple linear regression model in the Penguins example in the sections above? What is happening?

Unless all the regressors are orthogonal to each other, which essentially means that they don't affect the estimation of the coefficients of the other regressors (see Section \@ref(orthogonality)), then adding or removing regressors from a regression model will impact the estimated coefficients. The estimated coefficient for a regressor tends to change dramatically when it is highly correlated with the added or deleted regressor, but regardless, it will change at least somewhat when the regressors aren't orthogonal.

The role a regressor plays in a regression model depends on what other regressors are in the model. Recall a team setting you've been in where you had to work with others to accomplish something; it could be related to school, work, sports, etc. Depending on the skills and knowledge your team members have, you will try to find a role in which you can effectively help the team. E.g., I end up doing most of the house painting in our family because I have a lot of experience, so it's better for my family to serve in other roles. But if I was working with a more experienced painter, then it's likely that I would serve in a different capacity. Something similar happens in regression models. The regressors most effective in explaining the response tend to dominate the regressors with less explanatory power, but it always depends on the regressors in the model.

## Interpretation for categorical predictors

We now discuss the interpretation of regression coefficients in the context of a parallel lines and separate lines models.

### Coefficient interpretation for parallel lines models {#pl-interp}

Consider a parallel lines model with numeric regressor $X$ and categorical predictor $C$ with levels $L_1$, $L_2$, and $L_3$. Following the discussion in Section \@ref(categorical-predictors), predictor $C$ must be transformed into two indicator variables, $D_2$ and $D_3$, for category levels $L_2$ and $L_3$, to be included in our linear model. $L_1$ is the reference level. The parallel lines model is formulated as
\[
\begin{equation}
E(Y \mid X, C) = \beta_{int} + \beta_{X} X + \beta_{L_2} D_2 +  \beta_{L_3} D_3, (\#eq:pl-def-interp)
\end{equation}
\]
where we replace the usual $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ with notation the indicates the regressor each coefficient is associated with.

When an observation has level $L_1$ and $X=0$, then the expected response is
\[
\begin{align}
E(Y|X = 0, C=L_1) &= \beta_{int} + \beta_X \cdot 0 + \beta_{L_2} \cdot 0 + \beta_{L_3} \cdot 0 \\
&= \beta_{int}.
\end{align}
\]
Thus, $\beta_{int}$ is the expected response for an observation with level $L_1$ when $X=0$.

When an observation has a fixed level $L_j$ (it doesn't matter which level) and $X$ increases from $x^*$ to $x^*+1$, then the change in the expected response is
\[
\begin{align}
E(Y|X = x^* + 1, C=L_j) - E(Y|X = x^*, C=L_j)&= \beta_X.
\end{align}
\]
Thus, $\beta_X$ is the expected change in the response for an observation with fixed level $L_j$ when $X$ increases by 1 unit.

When an observation has level $L_2$, the expected response is
\[
\begin{align}
E(Y\mid X = x^*, C=L_2) &= \beta_{int} + \beta_X x^* + \beta_{L_2} \cdot 1 + \beta_{L_3} \cdot 0 \\
&= \beta_{int} + \beta_X x^* + \beta_{L_2}.
\end{align}
\]

Thus, 
\[
\begin{align}
E(Y\mid X=x^*, C=L_2) - E(Y\mid X=x^*, C=L_1) &= \\ (\beta_{int} + \beta_X x^* + \beta_{L_2}) - (\beta_{int} + \beta_X x^*)&=  \beta_{L_2}.
\end{align}
\]
A similar result holds for $\beta_{L_3}$ when $C=L_3$. Thus, $\beta_{L_2}$ is the expected change in the response for a fixed value of $X$ when comparing on observation having level $L_1$ to level $L_2$ of predictor $C$. A similar interpretation holds for $\beta_{L_3}$. $L_1$ is known as as the reference level because we must refer to it to interpret our model with respect to other levels of $C$.

To summarize the interpretation of the coefficients in parallel lines models like Equation \@ref(eq:pl-def-interp), assuming categorical predictor $C$ has $K$ levels instead of 3:

- $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
- $\beta_X$ is the expected change in the response when $X$ increases by 1 unit for a fixed level of $C$.
- $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X$ fixed at the same value.

### Coefficient interpretation for separate lines models {#sl-interp}

Consider a separate lines model with numeric regressor $X$ and categorical predictor $C$ with levels $L_1$, $L_2$, and $L_3$. The predictor $C$ will be transformed into two indicator variables, $D_2$ and $D_3$, for category levels $L_2$ and $L_3$, with $L_1$ being the reference level. The separate lines model is formulated as
\[
\begin{equation}
E(Y \mid X, C) = \beta_{int} + \beta_{X} X + \beta_{L_2} D_2 +  \beta_{L_3} D_3 + \beta_{XL_2} XD_2+\beta_{XL_3}XD_3. (\#eq:sl-def-interp)
\end{equation}
\]

When an observation has level $L_1$ and $X=x^*$, then the expected response is
\[
\begin{align}
& E(Y\mid X = x^*, C=L_1) \\
&= \beta_{int} + \beta_X \cdot x^* + \beta_{L_2} \cdot 0 + \beta_{L_3} \cdot 0  + \beta_{X L_2} \cdot x^* \cdot 0 + \beta_{X L_3}\cdot x^* \cdot 0 \\
&= \beta_{int} + \beta_X x^*.(\#eq:slr-mean-L1)
\end{align}
\]

Using Equation \@ref(\#eq:slr-mean-L1), we can verify that:

- $\beta_{int} = E(Y\mid X = 0, C=L_1)$.
- $\beta_{X} = E(Y\mid X = x^* + 1, C=L_1) - E(Y\mid X = x^*, C=L_1)$.


Additionally, for $j=2$,
\[
\begin{align}
& E(Y|X = x^*, C=L_2) \\
&= \beta_{int} + \beta_X \cdot x^* + \beta_{L_3} \cdot 1 + \beta_{L_3} \cdot 0  + \beta_{X L_2} \cdot x^* \cdot 1 + \beta_{X L_3}\cdot x^* \cdot 0 \\
&= \beta_{int} + \beta_X x^* + \beta_{L_2} + \beta_{XL_2}x^*\\
&= (\beta_{int} + \beta_{L_2}) + (\beta_X + \beta_{XL_2})x^*.(\#eq:slr-mean-L2)
\end{align}
\]
Similarly,
\[
\begin{equation}
E(Y|X = x^*, C=L_3) = (\beta_{int} + \beta_{L_3}) + (\beta_X + \beta_{XL_3})x^*. (\#eq:slr-mean-L3)
\end{equation}
\]
A similar results holds for $C=L_3$.

Using Equations \@ref(eq:slr-mean-L1), \@ref(eq:slr-mean-L2), and \@ref(eq:slr-mean-L3), we can verify that:

- For $j=2,3$, $\beta_{L_j}= E(Y\mid X = 0, C=L_j) - E(Y\mid X = 0, C=L_1)$.
- For $j=2,3$,
\[\beta_{XL_j}= [E(Y\mid X = x^*+1, C=L_j) - E(Y\mid X = x^*, C=L_j)]-[E(Y\mid X = x^*+1, C=L_1) - E(Y\mid X = x^*, C=L_1)].
\]

To summarize the interpretation of the coefficients in separate lines models like Equation \@ref(eq:sl-def-interp), assuming categorical predictor $C$ has $K$ levels instead of 3:

* $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
* $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X=0$.
* $\beta_X$ represents the expected change in the response when $X$ increases by 1 unit for observations having the reference level.
* $\beta_X L_j$, for $j=2,\ldots,K$, represents the difference in the expected rate of change when $X$ increases by 1 unit for observations have the reference level in comparison to level $L_j$.

### More Penguins examples {#more-penguins-examples}

In Section \@ref(s:penguins-mlr2), we fit a parallel lines to the `penguins` data. Letting $D_C$ denote the indicator variable for the `Chinstrap` level and $D_G$ denote the indicator variable for the `Gentoo` level, the fitted parallel lines model was
\[
\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) = 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G.
\]
In the context of this model:

- The expected bill length for an Adelie penguin with a body mass of 0 grams is 24.92 mm.
- A penguin identical in all ways with another penguin (including the same species) except that it has a body mass 1 gram larger is expected to have a bill length 0.004 mm longer than the smaller penguin.
- A Chinstrap penguin is expected to have a bill length 9.92 mm longer than an Adelie penguin, assuming their other characteristics are the same.
- A Gentoo penguin is expected to have a bill length 3.56 mm longer than an Adelie penguin, assuming their other characteristics are the same.

We also fit a separate lines model to the `penguins` data in Section in Section \@ref(s:penguins-mlr2). The fitted separate lines model was
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&\quad + 0.001 D_C \mathtt{body\_mass\_g} + 0.0009 D_G \mathtt{body\_mass\_g}
\end{align}
\]
In the context of this model:

- The expected bill length for an Adelie penguin with a body mass of 0 grams is 26.99 mm.
- If two Adelie penguins are identical in all ways except that one has a body mass 1 gram larger, the larger penguins is expected to have a bill length 0.003 mm longer than the smaller penguin.
- A Chinstrap penguin is expected to have a bill length 5.18 mm longer than an Adelie penguin when both have a body mass of 0 grams.
- A Gentoo penguin is expected to have a bill length 0.25 mm shorter than an Adelie penguin when both have a body mass of 0 grams.
- For each 1 gram increase in body mass, we expect the bill length of Chinstrap penguins to grow 0.001 mm more than the bill length growth of Adelie penguins.
- For each 1 gram increase in body mass, we expect the bill length of Gentoo penguins to grow 0.0009 mm more than the bill length growth of Adelie penguins.

## Effect plots

An effect plot is a visual display that aids in helping us intuitively interpret the impact of a *predictor* in a model. Specifically, an **effect plot** is a plot of the estimated mean response as a function of a *predictor* with the other *predictors* being held at their observed sample mean. The emphasis on *predictor* is intentional. Recall from Section \@ref(a-simple-motivating-example) that a predictor variable is a variable available to model the response variable. A **regressor** variable is a variable used in our regression model, whether that is an unmodified predictor variable, some transformation of a predictor, some combination of predictors, etc. Thus, for the models considered so far in this chapter, `body_mass_g`, `flipper_length_mm`, and `species` are predictor variables. The original information available to build a model. Additionally, based on how we used them, `body_mass_g` and `flipper_length_mm` were also regressors in certain models. Additional regressors were `speciesChinstrap` and `speciesGentoo` (the indicator variables for Chinstrap and Gentoo penguins based on the variable `species`, respectively) and `body_mass_g:speciesChinstrap` and `body_mass_g:speciesGentoo`, the regressors created by multiplying the indicator variables with the `body_mass_g` predictor. This distinction is important when discussing effect plots, because we can create effect plots for each predictor but not necessarily each regressor.

As stated by @fox2020predictor:

> Summarization of the effects of predictors using tables of coefficient estimates is often incomplete. Effects, and particularly plots of effects, can in many instances reveal the relationship of the response to the predictors more clearly. This conclusion is especially true for models with linear predictors
that include interactions and multiple-coefficient terms such as regression splines and polynomials ....

### Effect plots for numeric predictors

Consider the the fitted multiple linear regression model for the `penguins` data from Section \@ref(s:penguins-mlr):
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm})\\
&=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\end{align}
(\#eq:mlr-effect-equation)
\]

The sample means of the observed $\mathtt{body\_mass\_g}$ and $\mathtt{flipper\_length\_mm}$ for the `penguins` data are $\overline{\mathtt{body\_mass\_g}}=4201.75$ and $\overline{\mathtt{flipper\_length\_mm}}=200.92$, as demonstrated in the code below.

```{r}
# load penguins data since it hasn't been loaded in this chapter
data(penguins, package = "palmerpenguins")
# compute sample means of body_mass_g and flipper_length_mm
mean(penguins$body_mass_g, na.rm = TRUE)
mean(penguins$flipper_length_mm, na.rm = TRUE)
```
The effect plot for $\mathtt{body\_mass\_g}$ (on the response $\mathtt{bill\_length\_mm}$) is a plot of
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm} = 200.92)\\
&=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\cdot 200.92 \\
&=41.14+0.0007 \,\mathtt{body\_mass\_g}
\end{align}
\]
as a function of $\mathtt{body\_mass\_g}$. We used exact values in the result above. The intercept will be 40.76 instead of 41.14 if you use the rounded values.

Similarly, the effect plot for $\mathtt{flipper\_length\_mm}$ is a plot of
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g} = 4201.75,\mathtt{flipper\_length\_mm}) \\
&=-3.44+0.0007 \cdot 4201.75+0.22\mathtt{flipper\_length\_mm} \\
&=41.14+0.0007 \,\mathtt{body\_mass\_g}
\end{align}
\]
as a function of $\mathtt{flipper\_length\_mm}$.

The **effects** package [@R-effects] can be used to generate effect plots for the predictors of a fitted linear model. We start by using the `effects::predictorEffect` function to compute the information needed to draw the plot, then the `plot` function to display the information.

The `predictorEffect` function computes the estimated mean response for different values of the focal predictor while holding the other predictors at their sample mean (or at a fixed level of a categorical predictor). The main arguments of `predictorEffect` are:

- `predictor`: the name of the predictor you want to plot. This is the "focal predictor".
- `mod`: the fitted model. The function works with `lm` objects, but also many other types of fitted models.

The `plot` function will take the output of the `predictorEffect` function and produce the desired effect plot.

We refit the model in Equation \@ref(eq:mlr-effect-equation) below for completeness.

```{r}
# refit model
mlmod <- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins)
# extract estimated coefficients
coef(mlmod)
```
In the code below, we load the **effects** package (so that we can use the `predictorEffect` function) and then combine calls to the `plot` and `predictorEffect` functions to create an effect plot for `body_mass_g`, which is shown in Figure \@ref(fig:effect-plot-body-mass). We see from Figure \@ref(fig:effect-plot-body-mass) that there is a clear positive association between `body_mass_g` and `bill_length_mm`. The shaded area indicates the 95% confidence interval bands for the estimated mean response. We do not discuss confidence interval bands here, except to say that they provide a visual picture of the uncertainty of our estimated mean (wider bands indicates more uncertainty. Chapter \@ref(inference) discusses confidence intervals for linear models in some detail. The many tick marks along the the x-axis of the effect plot indicate observed values of the x-axis variable.

```{r effect-plot-body-mass, fig.cap = "Effect plot for body mass based on the fitted model in Equation \\@ref(eq:mlr-effect-equation)."}
# load effects package
library(effects)
# draw effect plot for body_mass_g
plot(predictorEffect("body_mass_g", mlmod))
```

We next create an effect plot for `flipper_length_mm`, which is shown in Figure \@ref(fig:effect-plot-body-mass), using the code below. There is a clear positive association between `flipper_length_mm` and `bill_length_mm`.

```{r effect-plot-flipper-length, fig.cap = "Effect plot for flipper length based on the fitted model in Equation \\@ref(eq:mlr-effect-equation)."}
# draw effect plot for flipper_length_mm
plot(predictorEffect("flipper_length_mm", mlmod))
```

Alternatively, we call use `effects::allEffects` to compute the necessary effect plot information for all predictors simultaneously, then use `plot` to create a display of the effect plots for all predictors in one graphic. This approach is quicker, but the individual effect plots can sometimes be too small for practical use. We demonstrate this faster approach in the code below, which produces Figure \@ref(fig:mlmod-effect-plots-all).

```{r mlmod-effect-plots-all, fig.cap = "All effect plots for predictors of the fitted model in Equation \\@ref(eq:mlr-effect-equation)."}
plot(allEffects(mlmod))
```

In Section \@ref(more-penguins-examples), the fitted parallel lines for the `penguins` data was
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&= 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G,
\end{align}
(\#eq:lmodp-effect-plot)
\]
The fitted model is recreated and assigned the name `lmodp` in the code below.

```{r}
# refit the parallel lines model
lmodp <- lm(bill_length_mm ~ body_mass_g + species, data = penguins)
# double-check coefficients
coef(lmodp)
```

### Effect plots with categorical predictors

We now construct effect plots for models with categorical predictors, specifically, the parallel lines and separate lines models for the penguins examples previously discussed.

The effect plot of `body_mass_g` for the parallel lines model should technically depend on the level of  `species`. To get a "typical" effect for a categorical predictor, the **effects** package displays the "average effect" based on a weighted average of the results for each level (run `vignette("predictor-effects-gallery", package = "effects")` in the Console for more details). We see the result of this in Figure \@ref(fig:effect-plot-body-mass-lmodp). The association between body mass and bill length is positive for this model.

```{r, effect-plot-body-mass-lmodp, fig.cap = "Effect plot for body mass based on the fitted model in Equation \\@ref(eq:lmodp-effect-plot)."}
plot(predictorEffect("body_mass_g", lmodp))
```

The effect plot of `species` is a plot of the function
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g =4201.75}, \mathtt{species}) \\
&= 24.92 + 0.004 \cdot 4201.75 + 9.92 D_C + 3.56 D_G \\
&= 40.67 + 9.92 D_C + 3.56 D_G.
\end{align}
\]
Based on the levels of `species` this simplifies to
\[
\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g =4201.75}, \mathtt{species}=\mathtt{Adelie}) = 40.67,
\]
\[
\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g =4201.75}, \mathtt{species}=\mathtt{Chinstrap}) = 40.67 + 9.92 = 50.59,
\]
and
\[
\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g =4201.75}, \mathtt{species}=\mathtt{Gentoo}) = 40.67 + 3.56 = 44.23.
\]
Thus, the effect plot for `species` will be a series of single values that depends on the level of `species`. This is shown in Figure \@ref(fig:effect-plot-species-lmodp), which is created using the code below. For a fixed value of body mass, Adelie penguins will typically have the smallest bill length, Chinstrap penguins the largest bill length, and Gentoo will typically be somewhere in between.

```{r, effect-plot-species-lmodp, fig.cap = "Effect plot for species based on the fitted model in Equation \\@ref(eq:lmodp-effect-plot)."}
plot(predictorEffect("species", lmodp))
```

We now consider the effect plots for a separate lines model using the separate lines model fit the `penguins` data in Section \@ref(s:penguins-mlr2). The fitted separate lines model previously fit for the `penguins` data was
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&\quad + 0.001 D_C \mathtt{body\_mass\_g} + 0.0009 D_G \mathtt{body\_mass\_g},
\end{align}
(\#eq:sl-model-penguins-effect-plot)
\]
which we refit and assign the name `lmods` in the code below.
```{r}
# fit separate lines model
lmods <- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species,
            data = penguins)
# extract estimated coefficients
coef(lmods)
```

We determined that the model simplifies depending on the level of species. We previously derived in Section \@ref(s:penguins-mlr2) that:
\[
\begin{align}
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&= 26.99 + 0.003 \mathtt{body\_mass\_g},\\
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&= 31.17 + 0.004 \mathtt{body\_mass\_g},\\
&\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&= 26.74 + 0.004 \mathtt{body\_mass\_g}.
\end{align}
(\#eq:separate-lines-equations-effects-plot)
\]

The effect plot of `body_mass_g` for the separate lines model will be a plot of each equation given in Equation \@ref(eq:separate-lines-equations-effects-plot). Figure \@ref(fig:effect-plot-body-mass-lmods) displays this effect plot, which was created using the code below.

```{r, effect-plot-body-mass-lmods, fig.cap = "Effect plot for body mass based on the equations in Equation \\@ref(eq:separate-lines-equations-effects-plot)."}
# effect plot of body mass for separate lines model
# axes ... rotates the x-axis labels 90 degrees
plot(predictorEffect("body_mass_g", lmods), axes = list(x = list(rotate = 90)))
```

```{r, effect-plot-species-lmods, fig.cap = "Effect plot for species based on the equations in Equation \\@ref(eq:separate-lines-equations-effects-plot)."}
# effect plot of body mass for separate lines model
# axes ... rotates the x-axis labels 90 degrees
plot(predictorEffect("species", lmods), axes = list(x = list(rotate = 90)), lines=list(multiline=TRUE))
```


## Added variable plots

## Going deeper

### Orthogonality {#orthogonality}

Let
\[\mathbf{X}_[j]=[x_{1,j},\ldots,x_{n,j}]\]
denote the $n\times 1$ column vector of observed values for regressor $X_j$. (We can't use the notation $\mathbf{x}_j$ because that is the $p\times 1$ vector of regressor values for the $j$th observation). Regressors, $\mathbf{X}_{[j]}$ and $\mathbf{X}_{[k]}$ are **orthogonal** if $\mathbf{X}_{[j]}^T \mathbf{X}_{[k]}=0$.

Let $\boldsymbol{1}_{n\times1}$ denote an $n\times 1$ column vector of 1s. The definition of orthogonal vectors above implies that $\mathbf{X}_{[j]}$ is orthogonal to $\boldsymbol{1}_{n\times1}$ if $$
\mathbf{X}_{[j]}^T \boldsymbol{1}_{n\times1} = \sum_{i=1}^n x_{i,j} = 0,$$
i.e., if the values in $\mathbf{X}_{[j]}$ sum to zero.

Let $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$ denote the sample mean of $\mathbf{X}_{[j]}$ and $\bar{\mathbf{x}}_j = \bar{x}_j \boldsymbol{1}_{n\times 1}$ denote the column vector that repeats $\bar{x}_j$ $n$ times.

**Centering** $\mathbf{X}_{[j]}$ involves subtracting the sample mean of $\mathbf{X}_{[j]}$ from $\mathbf{X}_{[j]}$, i.e., $\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j$.

Regressors $\mathbf{X}_{[j]}$ and $\mathbf{X}_{[k]}$ are **uncorrelated** if they are orthogonal after being centered, i.e., if
\[
(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k)=0.
\]
Note that the sample covariance between vectors $\mathbf{X}_{[j]}$ and $\mathbf{X}_{[k]}$ is
\[
\begin{align*}
\widehat{\mathrm{cov}}(\mathbf{X}_{[j]}, \mathbf{X}_{[k]}) &= \frac{1}{n-1}\sum_{i=1}^n (x_{i,j} - \bar{x}_j)(x_{i,k} - \bar{x}_k) \\
 &= \frac{1}{n-1}(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k).
\end{align*}
\]
Thus, two centered regressors are orthogonal if their covariance is zero.

It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response.  Specifically:

*If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors.*

Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s.

We consider a simple example with $n=5$ observations to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below:

- `y` is a vector of response values.
- `ones` is the column vector of 1s.
- `X1` is a column vector of regressor values.
- `X2` is a column vector of regressor values chosen to be orthogonal to `x1` but not to `ones`.
- `X3` is a column vector of regressor values orthogonal to both `x1` and `ones`.
- `X4` is a column vector of regressor values orthogonal to `ones`, `x1`, and `x3`, but not `x2`.
- `X5` is a column vector of regressor values orthogonal to `ones` and `x1`, but not the other regressor vectors.

```{r, include = FALSE}
y <- c(1, 4, 6, 8, 9) # response vector
ones <- rep(0, 5) # column of 1s
x1 <- c(7, 5, 5, 7, 7) # regressor 1
x2 <- c(-1, 2, -3, 1, 5/7) # create regressor 2 to be orthogonal to x1
crossprod(x1, x2) # crossproduct is zero
x3 <- c(0, -1, 1, 0, 0) # orthogonal to ones, x1
x4 <- c(0, 0, 0, 1, -1) # orthogonal to ones, x1, x3
x5 <- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4

crossprod(cbind(1, x1, x4, x5))

lm(y ~ x1)
lm(y ~ x4 + x5 - 1)
lm(y ~ x1 + x4 + x5)
```

In the code below, we define vectors `y`, `X1`, and `X2`.
```{r}
y <- c(1, 4, 6, 8, 9)       # create an arbitrary response vector
X1 <- c(7, 5, 5, 7, 7)      # create regressor 1
X2 <- c(-1, 2, -3, 1, 5/7)  # create regressor 2 to be orthogonal to x1
```

Note that the `crossprod` function computes the crossproduct of two vectors or matrices, so that `crossprod(A, B)` computes $\mathbf{A}^T B$, where the vectors or matrices must have the correct dimension for the multiplication to be performed.

The regressor vectors `X1` and `X2` are orthogonal since their crossproduct $\mathbf{X}_{[1]}^T \mathbf{X}_{[2]}$ (in R, `crossprod(X1, X2)`) equals zero, as shown in the code below.
```{r}
# crossproduct is zero, so X1 and X2 are orthogonal
crossprod(X1, X2)
```
In the code below, we regress `y` on `x1` without an intercept (`lmod1`). The estimated coefficient for `X1` is $\hat{\beta}_1=0.893$. Next, we then regress `y` on `X1` and `X2` without an intercept (`lmod2`). The estimated coefficients for `X1` and `X2` are $\hat{\beta}_1=0.893$ and $\hat{\beta}_2=0.221$, respectively. Because `X1` and `X2` are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for `X1` stays the same in both models.
```{r}
# y regressed on X1 without an intercept
lmod1 <- lm(y ~ x1 - 1)
coef(lmod1)
# y regressed on X1 and X2 without an intercept
lmod2 <- lm(y ~ x1 + x2 - 1)
coef(lmod2)
```

The previous models (`lmod1` and `lmod2`) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our $\mathbf{X}$ matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s.

However, neither `X1` nor `X2` is orthogonal with the column of ones. We define the vector `ones` below, which is a column of 1s, and compute the crossproduct between `ones` and the two regressors. Since the crossproducts are not zero, `X1` and `X2` are not orthogonal to the column of ones.
```{r}
ones <- rep(1, 5)   # column of 1s
crossprod(ones, X1) # not zero, so not orthogonal
crossprod(ones, X2) # not zero, so not orthogonal
```

We create `lmod3` by adding adding a column of ones to `lmod2` (i.e., if we include the intercept in the model). The the coefficients for both `X1` and `X2` change when going from `lmod2` to `lmod3` because these regressors are not orthogonal to the column of 1s. Comparing the coefficients `lmod2` above and `lmod3`, $\hat{\beta}_1$ changes from $0.893$ to $0.397$ and $\hat{\beta}_2$ changes from $0.221$ to $0.279$.

```{r}
coef(lmod2) # coefficients for lmod2
# y regressed on X1 and X2 with an intercept
lmod3 <- lm(y ~ x1 + x2)
coef(lmod3) # coefficients for lmod3
```
For orthogonality of our regressors to be most impactful, the model's regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesn't impact the estimated coefficients of the other regressors. In the code below, we define centered regressors `x3` and `x4` to be uncorrelated, i.e., `X3` and `X4` have sample mean zero and are orthogonal to each other.

```{r}
X3 <-  c(0, -1, 1, 0, 0) # sample mean is zero
X4 <- c(0, 0, 0, 1, -1)  # sample mean is zero
cov(X3, X4)              # 0, so X3 and X4 are uncorrelated and orthogonal
```

If we fit linear regression models with any combination of `ones`, `X3`, or `X4` as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below.

```{r, eval = FALSE}
coef(lm(y ~ 1))           # only column of 1s
coef(lm(y ~ x3 - 1))      # only x3
coef(lm(y ~ x4 - 1))      # only x4
coef(lm(y ~ x3))          # 1s and x3
coef(lm(y ~ x4))          # 1s and x4
coef(lm(y ~ x3 + x4 - 1)) # x3 and x4
coef(lm(y ~ x3 + x4))     # 1s, x3, and x4
```
We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesn't impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were $\hat{\beta}_{0}=5.6$, $\hat{\beta}_{3}=1.0$, $\hat{\beta}_{4}=-0.5$ when the relevant regressor was included in the model.

The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the $\mathbf{X}$ matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, `x1`, `x2`, `x3`, and `x4`.

```{r}
crossprod(model.matrix(~ X1 + X2 + X3 + X4))
```
Consider the sequence of models below.
```{r}
coef(lm(y ~ 1))
```
The model with only an intercept has an estimated coefficient of $\hat{\beta}_{int}=5.6$. If we add the `X1` to the model with an intercept, then both coefficients change because they are not orthogonal to each other.
```{r}
lmod4 <- lm(y ~ x1) # model with 1s and x1
coef(lmod4)
```
If we add `X2` to `lmod4`, we might think that only $\hat{\beta}_{0}$ will change because `X1` and `X2` are orthogonal to each other. However, because `X2` is not orthogonal to all of the other regressors in the model (`X1` and the column of 1s), both $\hat{\beta}_{0}$ and $\hat{\beta}_1$ will change. The easiest way to realize this is to look at `lmod2` above with only `x1` and `x2`. When we add the column of 1s to `lmod2`, both $\hat{\beta}_1$ and $\hat{\beta}_2$ will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term.

```{r}
coef(lm(y ~ x1 + x2))
```

However, note that `X3` is orthogonal to the column of 1s and `X1`. Thus, if we add `X3` to `lmod4`, which includes both a column of 1s and `X1`, `X3` will not change the estimated coefficients for the intercept or `X1`.

```{r}
coef(lm(y ~ x1 + x3))
```

Additionally, since `X4` is orthogonal to the column of 1s, `x1`, and `x3`, adding `X4` to the previous model will not change the estimated coefficients for any of the other variables already in the model.

```{r}
coef(lm(y ~ x1 + x3 + x4))
```

Lastly, if we can partition our $\mathbf{X}$ matrix such that $\mathbf{X}^T \mathbf{X}$ is a block diagonal matrix, then none of the blocks of variables will affect the estimated coefficients of the other variables.

Define a new regressor `X5` below. `X5` is orthogonal to the column of 1s and `X1`, but not `X4`.

```{r}
X5 <- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4
# note block of 0s
crossprod(cbind(ones, X1, X4, X5))
```

Note the block of zeros in the lower left and upper right corners of the crossproduct matrix above. The block containing `ones` and `X1` is orthogonal to the block containing `X4` and `X5`. This means that if we fit the model with only the column of 1s and `X1`, the model only with `X4` and `X5`, and then fit the model with the column of 1s, `x1`, `x4`, and `x5`, then the coefficients $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are not impacted when `X4` and `X5` are added to the model. Similarly, $\hat{\beta}_{4}$ and $\hat{\beta}_{5}$ are not impacted when the column of 1s and `X1` are added to the model with `X4` and `X5`. See the output below.

```{r}
lm(y ~ x1)           # model with 1s and x1
lm(y ~ x4 + x5 - 1)  # model with x4 and x5 only
lm(y ~ x1 + x4 + x5) # model with 1s, x1, x4, x5
```

<!-- ## Example: Fuel Consumption Data -->

<!-- ```{r} -->
<!-- data(fuel2001, package = "alr4") -->
<!-- ``` -->

<!-- The variables (for the year 2001 unless otherwise noted) are: -->

<!-- - `Drivers`: Number of Licensed drivers in the state -->
<!-- - `FuelC`: Gasoline sold for road use (1000s of gal.) -->
<!-- - `Miles`: Miles of Federal-aid highway miles in the state -->
<!-- - `Pop`: 2001 population age 16 and over -->
<!-- - `Tax`: Gasoline state tax rate (cents/gallon) -->

<!-- ## Setting Up a Linear Model -->

<!-- What is the relationship between fuel consumption and various regressors for the 50 United States and the District of Columbia?   -->

<!-- ### Adjusting Units -->

<!-- - Some of the variables are adjusted for population. Others are are not. -->
<!-- - Some dollar values are given in thousands of dollars. Others are given in dollars. -->
<!-- - Some units of fuel are given in gallons. Others are given in 1000's of gallons. -->
<!-- - Our model should have regressor variables with compatible units with the response variable. -->

<!-- 1. **Create a new variable called `Fuel` that converts units of `FuelC` from 1000's of gallons to gallons per person**. -->
<!-- 2. **Create a new variable called `Income1k`that converts the units of `Income` from dollars per capita to 1000's of dollars per capita.** -->
<!-- 3. **Convert the units of `Drivers` from number of drivers to number of drivers per capita**. -->

<!-- - `Fuel`: Average amount of gasoline sold for road use per person (Gallons/person) -->
<!-- - `Income1K`: Average personal income (in thousands) for the year 2000 per person ($1K/person) -->
<!-- - `Dlic`: Number of licensed drivers per 1000 persons (licensed drivers/1K persons)  -->

<!-- ```{r} -->
<!-- # create new regressors/transformed responses to fuel2001 data frame -->
<!-- fuel2001$Fuel <- 1000*fuel2001$FuelC/fuel2001$Pop -->
<!-- fuel2001$Dlic <- 1000*fuel2001$Drivers/fuel2001$Pop -->
<!-- fuel2001$Income1K <- fuel2001$Income/1000 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- summary(fuel2001) -->
<!-- ``` -->

<!-- ### Fitting a Model -->

<!-- We will set up a regression model to determine how `Fuel` (gallons per person) is related to `Tax` (cents per gallon), -->
<!-- `Dlic` (drivers per capita), `Income1k` (thousands of dollars of income per capita), and `Miles` (federal highway miles). -->

<!-- $$E( \mbox{Fuel} \ | \ \mbox{Tax, Dlic, Income1K, Miles})=\beta_0+\beta_1 (\mbox{Tax}) +\beta_2 (\mbox{Dlic}) + \beta_3 (\mbox{Income1K}) + \beta_4 \log{(\mbox{Miles})}$$ -->

<!-- ```{r} -->
<!-- # fit model -->
<!-- lmod <- lm(Fuel ~ Tax + Dlic + Income1K + log(Miles), data = fuel2001) -->

<!-- # summarize model -->
<!-- faraway::sumary(lmod) -->
<!-- ``` -->

<!-- We see the fitted model is: -->

<!-- $$\widehat{\mbox{E}}(\mbox{Fuel} \ | \ \mbox{Tax, Dlic, Income1k, Miles}) = 154.19 - 4.24 (\mbox{Tax}) + 0.47 (\mbox{Dlic}) - 6.14(\mbox{Income1K}) + 26.76 \log{(\mbox{26.76})}$$ -->

<!-- This equation represents the estimated conditional mean of Fuel given fixed values of the regressors Tax, Dlic, Income1K, and Miles. -->

<!-- ## Interpreting the Coefficients -->

<!-- Estimated coefficients are usually interpreted as a **rate of change.**  -->

<!-- - If we increase a regressor by 1 unit (and hold all others constant), what is the predicted change in the response variable? -->

<!-- 4. **Interpret the practical meaning of $\beta_1 = -4.24$. Pay attention to units when giving your interpretation.** -->



<!-- - The sign of a parameter estimate indicates the direction of the relationship between the regressor and the response (when all other regressors are constant). -->
<!-- - The sign of the effect of a regressor is often more important than its magnitude. -->
<!-- - If regressors are highly correlated with other regressors, both the magnitude and sign of an estimated coefficient may change depending on the values of the other regressors are in the model. -->

<!-- ## Example: Berkeley Guidance Study -->

<!-- Data from the Berkeley guidance study of children born in 1928-29 in Berkeley, CA. BGSgirls contains data from just the girls in the study. -->

<!-- ```{r} -->
<!-- data(BGSgirls, package = "alr4") -->
<!-- head(BGSgirls) -->
<!-- ``` -->

<!-- ## Dictionary of Data -->

<!-- - `BMI18`: the body mass index at age 18 -->
<!-- -  `WT2`, `WT9`, and `WT18`: the weights at ages 2, 9, and 18 (in kg) for the $n=70$ girls in the study. -->

<!-- ## Analyzing Relations Between Regressors -->

<!-- ```{r} -->
<!-- # basic scatterplot matrix -->
<!-- pairs(~ BMI18 + WT2 + WT9 + WT18, data = BGSgirls) -->
<!-- ``` -->

<!-- 5. **Based on the scatter plot matrix above, does there seem to be any relations among the regressors? Explain why these relations make practical sense.** -->


<!-- 6. **How can we adjust our model to account for the relations between the regressors?** -->


<!-- ## Adjusting the Regressors -->

<!-- 7. **Create a new regressor called `DW9` that is the weight gain from age 2 to 9.** -->

<!-- 8. **Create a new regressor called `DW18` that is the weight gain from age 9 to 18.** -->

<!-- ```{r} -->
<!-- BGSgirls$DW9 <- BGSgirls$WT9-BGSgirls$WT2 -->
<!-- BGSgirls$DW18 <- BGSgirls$WT18-BGSgirls$WT9 -->
<!-- BGSgirls$DW218 <- BGSgirls$WT18-BGSgirls$WT2 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # basic scatterplot matrix -->
<!-- pairs(~ BMI18 + DW9 + DW18, data = BGSgirls) -->
<!-- ``` -->

<!-- 9. **Based on the scatter plot matrix above, how can you tell that the new regressors seem to be more independent from each other?** -->

<!-- ## Comparing Different Models -->

<!-- ### BMI relation to WT2, WT9 and WT18 -->

<!-- ```{r} -->
<!-- m1 <- lm(BMI18 ~ WT2 + WT9 + WT18, BGSgirls) -->
<!-- faraway::sumary(m1) -->
<!-- ``` -->

<!-- ### BMI relation to WT2, DW9 and DW18 -->

<!-- ```{r} -->
<!-- m2 <- lm(BMI18 ~ WT2 + DW9 + DW18, BGSgirls) -->
<!-- faraway::sumary(m2) -->
<!-- ``` -->

<!-- ### BMI relation to WT2, WT9, WT18, DW9 and DW18 -->

<!-- ```{r} -->
<!-- m3 <- lm(BMI18 ~ WT2 + WT9 + WT18 + DW9 + DW18, BGSgirls) -->
<!-- faraway::sumary(m3) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- coef(m1) -->
<!-- coef(m2) -->
<!-- coef(m3) -->
<!-- ``` -->

<!-- Regressor | Model 1 | Model 2 | Model 3     -->
<!-- ----------|---------|---------|----------       -->
<!-- Intercept  | 8.298         | 8.298        | 8.298    -->
<!--   CI         | (5.00,11.62)  | (5.00,11.62) | (5.00,11.62) -->
<!-- WT2  | -0.383        | -0.065       | -0.383 -->
<!--  CI  | (-0.69,-0.08) | (-0.32,0.19) | (-0.69,-0.08) -->
<!-- WT9  | 0.032         | --           | 0.032  -->
<!--  CI  | (-0.06,0.13)  | --           | (-0.06,0.13) -->
<!-- WT18 | 0.287         | --           | -- -->
<!-- CI   | (0.23,0.34)   | --           | -- -->
<!-- DW9  |  --           | 0.318        | NA -->
<!--  CI  |  --           | (0.24,0.40)  | NA -->
<!-- DW18 |  --           | 0.287        | NA -->
<!--   CI |  --           | (0.23,0.34)  | NA -->

<!-- 10. **Comment on how the WT2 coefficient is the same/different in the different models.** -->


<!-- When regressors are correlated, interpretation of the effect of a regressor depends not only on the other regressors in the model, but also upon the linear transformation of the variables used. -->

<!-- 11. **Why are their NAs in Model 3?** -->



<!-- ## Effect Plots -->

<!-- An **effect plot** displays effect of a regressor on the mean response while holding the other regressors at their mean values. -->

<!-- $$ \hat{y} = \beta_0 + \beta_1 (\bar{X}_1) + \beta_2 (\bar{X}_2) + \ldots + \beta_{i-1} (\bar{X}_{i-1}) + \beta_i (X_i) + \beta_{i+1} (\bar{X}_{i+1}) + \ldots + + \beta_{p-1} (\bar{X}_{p-1})$$ -->

<!-- ```{r} -->
<!-- summary(lmod)$coefficients -->
<!-- ``` -->

<!-- 12. **Complete the code below to extract each of the coefficients in the Fuel Consumption model `lmod` from the coefficient array above.** -->

<!-- ```{r} -->
<!-- b0 <- summary(lmod)$coefficients[1] #beta_0 -->
<!-- b1 <- summary(lmod)$coefficients[2] #beta_1 -->
<!-- b2 <- summary(lmod)$coefficients[3] #beta_2 -->
<!-- b3 <- summary(lmod)$coefficients[4] #beta_3 -->
<!-- b4 <- summary(lmod)$coefficients[5] #beta_4 -->
<!-- ``` -->

<!-- 12. Complete the R code below to compute the sample means for each of the regressors. -->

<!-- ```{r} -->
<!-- xbar.Tax <- mean(fuel2001$Tax) -->
<!-- xbar.Dlic <- mean(fuel2001$Dlic) -->
<!-- xbar.Income1K <- mean(fuel2001$Income1K) -->
<!-- xbar.Miles <- mean(fuel2001$Miles) -->
<!-- ``` -->

<!-- 13. **What is the effect of `Tax` on expected Fuel consumption when the other regressors are fixed at the sample mean values? Write a formula to express this relation.** -->



<!-- Thus we have the model -->
<!-- $$\mbox{E}(\mbox{Fuel} \ | \ \mbox{Tax, Dlic=??, Income1K=??, log(Miles) = ??}) = ?? - ??(\mbox{Tax})$$ -->

<!-- ```{r} -->
<!-- library(effects) # for Effect function -->
<!-- # effect plot for Tax regressor -->
<!-- plot(Effect("Tax", lmod)) -->
<!-- ``` -->

<!-- 14. **If instead of fixing the values of the regressors at their mean, we choose other values such as the minimum value of each of regressors. What effect (if any) would this have on the graph above?** -->


<!-- ## Regressors on Logarithmic Scale -->

<!-- Logarithms are commonly used both for the response and for regressors. -->

<!-- ```{r} -->
<!-- summary(fuel2001) -->
<!-- ``` -->


<!-- 15. **Based on the summary output above, why do you think we used a log scale on `Miles`?** -->


<!-- In the code block below, we create new variables that are the natural log and log base 10 of Miles and recreate the linear model using each of these new variables. -->

<!-- ```{r} -->
<!-- fuel2001$LnMiles <- log(fuel2001$Miles) -->
<!-- fuel2001$LogMiles <- log10(fuel2001$Miles) -->

<!-- lmod.ln <- lm(Fuel ~ Tax + Dlic + Income1K + LnMiles, data = fuel2001) -->
<!-- lmod.log <- lm(Fuel ~ Tax + Dlic + Income1K + LogMiles, data = fuel2001) -->

<!-- faraway::sumary(lmod.ln) #Check that model is the same -->
<!-- faraway::sumary(lmod.log) #Check that model is the same -->
<!-- ``` -->

<!-- ## Below we create an effects plot on a natural log scale. -->

<!-- ```{r} -->
<!-- plot(Effect("LnMiles", lmod.ln),  -->
<!--      main = "ln(Miles) effect plot") -->
<!-- ``` -->

<!-- ## Below we create an effects plot on a log10 scale. -->

<!-- ```{r} -->
<!-- plot(Effect("LogMiles", lmod.log),  -->
<!--      main = "log(Miles) effect plot") -->
<!-- ``` -->

<!-- ## Below we create an effects plot on a regular scale. -->

<!-- ```{r} -->
<!-- plot(Effect("Miles", lmod,  -->
<!--             xlevels = list(Miles = seq(1, 3e5, len = 301)))) -->
<!-- ``` -->

<!-- The effect of increasing Miles is greater in states with fewer miles of roadway, with relatively little change in states with the most roadway.  -->

<!-- This is the usual effect of logarithms: the fitted effect changes most rapidly when the regressor is small and less rapidly when the predictor is large. -->

<!-- ## Interpreting Coefficients with Log Scale on Regressor -->

<!-- ### Natural Log Scale  -->

<!-- Regressor $X_j$ increasing by 1% while the other regressors remain constant is associated with a $\beta_j/100$ increase in the response variable, on average. -->

<!-- 16. **Interpret the meaning of the coefficient associated to the natural log of Miles which you can find below.** -->

<!-- ```{r} -->
<!-- summary(lmod.ln)$coefficients[5, 1] #ln coeff -->
<!-- ``` -->

<!-- ### Common Log (base 10) Scale  -->

<!-- Regressor $X_j$ increasing by a factor of 10 (an increase of 900%) while the other regressors remain constant is associated with a $\beta_j$ increase in the response variable, on average. -->

<!-- 17. **Interpret the meaning of the coefficient associated to the log base 10 of Miles which you can find below.** -->

<!-- ```{r} -->
<!-- summary(lmod.log)$coefficients[5, 1] #log coeff -->
<!-- ``` -->

<!-- ## Log-Level Interpretation -->

<!-- It is common for responses to be transformed to a logarithmic scale for theoretical or practical considerations.   -->

<!-- $$\mbox{E}( \log{Y} \ | \ X)= \beta_0 + \beta_1 X.$$ -->

<!-- This is sometimes called a **log-level model**.  -->

<!-- - A unit increase in $X$ is associated with a change in the mean $Y$ by the multiplicative effect $\exp^{\beta_1}$. -->
<!-- - **Thus $beta_1$ is the continuous exponential growth/decay rate.** -->

<!-- It is often acceptable to approximate the expected value of a log by the log of the expected value: -->

<!-- $$\log{( \mbox{E}(Y \ | \ X=x) )} \approx\mbox{E}(\log{Y} \ | \ X=x)$$ -->

<!-- Thus, we have -->

<!-- $$\mbox{E}(Y \ | \ X=x) \approx e^{\rm{E}(\log{Y} \ | \ X=x)} = e^{\beta_0 + \beta_1 X}=e^{\beta_0}e^{\beta_1X}.$$ -->

<!-- ## Log-log Interpretation -->

<!-- Consider the log-log simple linear regression model -->

<!-- $$\mbox{E}( \log{Y} \ | \ X) = \beta_0 + \beta_1 \log{X}.$$ -->

<!-- When we scale $X$ by a factor of $c$, the response is predicted to grow by a factor of $c^{\beta_1}$, on average. -->

<!-- ## Summary of Interpretations (Simple Linear Regression) -->

<!-- **Level-level**: $\mathbf{\mbox{E}(Y \ | \ X=x)} = \boldsymbol\beta_0 + \boldsymbol\beta_1 X$: The predicted change in the response is $\beta_1$ when we increase $X$ by 1 unit, on average. -->

<!-- **Level-log**: $\mathbf{\mbox{E}(Y \ | \ X=x)} = \boldsymbol\beta_0 + \boldsymbol\beta_1 \log{X}$: When we increase $X$ by 1%, the response is predicted to increase by  $\beta_j/100$, on average. -->

<!-- **Log-level**: $\mathbf{\mbox{E}(\log{Y} \ | \ X=x) = \boldsymbol\beta_0 + \boldsymbol\beta_1 X}$: A unit increase in $X$ is predicted to change the response by a factor of $e^{\beta_1}$, on average. The continuous growth rate is $\beta_1$. -->

<!-- **Log-log**: $\mathbf{\mbox{E}(\log{Y} \ | \ X=x) = \boldsymbol\beta_0 + \boldsymbol\beta_1 \log{X}}$: When we scale $X$ by a factor of $c$, the response is predicted to grow by a factor of $c^{\beta_1}$, on average. -->

<!-- ## More Practice -->

<!-- 1. For a log-level model, interpret the relationship between $X$ and the mean of $Y$ when $X$ increases by 1 unit and $\beta_j=0.3$ and the other predictors do not change. -->

<!-- 2. For a log-log model, what is the expected change in $Y$ if we multiply $X$ by a factor of $c$. -->

<!-- <!-- 3. For a log-log model, interpret the relationship between $X$ and the expected value of $Y$ when $X$ increases by $10\%$ and $\beta_1=0.3$. --> -->

<!-- ## Exercises -->

<!-- 1. If given a set of data with several variables, how would you decide what the response variable and the predictor variables would be? -->
<!-- 1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are considered random? Which are considered fixed? -->
<!-- 1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are observable? Which are not observable? -->
<!-- 1. What are the typical goals of a regression analysis? -->
<!-- 1. List the typical assumptions made for the errors in a linear model. -->
<!-- 1. Without using a formula, what is the basic difference between a linear model and a non-linear model? -->
<!-- 1. Assuming that $\boldsymbol{\epsilon} ~ N(\mathbf{0}_{n\times 1}, \sigma^2 I_n)$ and $\mathbf{y}  = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, show that: -->
<!--     a. $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$. -->
<!--     a. $\mathrm{var}(\mathbf{y}) = \sigma^2 I_n$. -->
<!-- 1. In the context of simple linear regression under the standard assumptions, show that: -->
<!--     a. $\beta_0=E(Y|X=0)$. -->
<!--     a. $\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)$. -->
<!-- 1. In the context of multiple linear regression under the standard assumptions, show that: -->
<!--     a. $\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)$. -->
<!--     b. For $j=1,2,\ldots,p-1$, $\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})$ where $\mathbf{x}^*$ is a fixed vector of the appropriate size. -->
