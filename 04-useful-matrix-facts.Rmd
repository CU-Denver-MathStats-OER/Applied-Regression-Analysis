---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_notebook
---

A **matrix** is a two-dimensional array of values, symbols, or other objects (depending on the context). We will assume that our matrices contain numbers or random variables. Context will make it clear which is being represented.

* Matrices are commonly denoted by bold capital letters like $\mathbf{A}$ or $\mathbf{B}$, but this will sometimes be simplified to capital letters like $A$ or $B$.

# Useful matrix facts

## Notation
A matrix $\mathbf{A}$ with $m$ rows and $n$ columns (an $m\times n$ matrix) will be denoted as 
$$\mathbf{A} = \begin{bmatrix}
\mathbf{A}_{1,1} & \mathbf{A}_{2,1} & \cdots & \mathbf{A}_{1,n} \\
\mathbf{A}_{2,1} & \mathbf{A}_{2,1} & \cdots & \mathbf{A}_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{A}_{m,1} & \mathbf{A}_{m,2} & \cdots & \mathbf{A}_{m,n} \\
\end{bmatrix},
$$

where $\mathbf{A}_{i,j}$ denotes the element in row $i$ and column $j$ of matrix $\mathbf{A}$.

A **column vector** is a matrix with a single column. A **row vector** is a matrix with a single row. 

* Vectors are commonly denoted with bold lowercase letters such as $\mathbf{a}$ or $\mathbf{b}$, but this may be simplified to lowercase letters such as $a$ or $b$.

A $p\times 1$ column vector $\mathbf{a}$ may constructed as 
$$\mathbf{a} = [a_1, a_2, \ldots, a_p]^T = 
\begin{bmatrix}
a_1 & a_2 & \cdots & a_p
\end{bmatrix}^T = \begin{bmatrix}
a_1 \\ a_2 \\ \vdots \\ a_p
\end{bmatrix}.$$

## Basic mathematical properties

### Addition and subtraction

Consider matrices $\mathbf{A}$ and $\mathbf{B}$ with identical sizes $m\times n$. 

We add $\mathbf{A}$ and $\mathbf{B}$ by adding the element in position $i,j$ of $\mathbf{B}$ with the element in position $i,j$ of $A$, i.e.,

$$(\mathbf{A} + \mathbf{B})_{i,j} = \mathbf{A}_{i,j} + \mathbf{B}_{i,j}.$$

Similarly, if we subtract $\mathbf{B}$ from matrix $\mathbf{A}$, then we subtract the element in position $i,j$ of $\mathbf{B}$ from the element in position $i,j$ of $\mathbf{A}$, i.e., 

$$(\mathbf{A} - \mathbf{B})_{i,j} = \mathbf{A}_{i,j} - \mathbf{B}_{i,j}.$$

Example:

$$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix} + 
\begin{bmatrix}
2 & 9 & 1 \\
1 & 3 & 1 \\
\end{bmatrix} = 
\begin{bmatrix}
3 & 11 & 4 \\
5 & 8 & 7 \\
\end{bmatrix}.$$

$$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix} - 
\begin{bmatrix}
2 & 9 & 1 \\
1 & 3 & 1 \\
\end{bmatrix} = 
\begin{bmatrix}
-1 & -7 & 2 \\
3 & 2 & 5 \\
\end{bmatrix}.$$

### Scalar multiplication

A matrix multiplied by a scalar value $c\in\mathbb{R}$ is the matrix obtained by multiplying each element of the matrix by $c$. If $\mathbf{A}$ is a matrix and $c\in \mathbb{R}$, then 
$$(c\mathbf{A})_{i,j} = c\mathbf{A}_{i,j}.$$
Example: $$3\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}=
\begin{bmatrix}
3\cdot 1 & 3\cdot 2 & 3\cdot 3 \\
3\cdot 4 & 3\cdot 5 & 3\cdot 6 \\
\end{bmatrix}=
\begin{bmatrix}
3 & 6 & 9 \\
12 & 15 & 18 \\
\end{bmatrix}.$$

### Matrix multiplication

Consider two matrices $\mathbf{A}$ and $\mathbf{B}$. The matrix product $\mathbf{AB}$ is only defined if the number of columns in $\mathbf{A}$ matches the number of rows in $\mathbf{B}$. 

Assume $\mathbf{A}$ is an $m\times n$ matrix and $\mathbf{B}$ is an $n\times p$ matrix. $\mathbf{AB}$ will be an $m\times p$ matrix and $$(\mathbf{AB})_{i,j} = \sum_{k=1}^{n} \mathbf{A}_{i,k}\mathbf{B}_{k,j}.$$

Example: $$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
\begin{bmatrix}
1 & 4\\
2 & 5\\
3 & 6
\end{bmatrix}=
\begin{bmatrix}
1\cdot 1 +  2 \cdot 2 + 3 \cdot 3 & 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6\\
4\cdot 1 +  5 \cdot 2 + 6 \cdot 3 & 4 \cdot 4 + 5 \cdot 5 + 6 \cdot 6\\
\end{bmatrix}=
\begin{bmatrix}
14 & 32\\
32 & 77\\
\end{bmatrix}.$$

### Associative property

Addition and multiplication satisfy the associative property for matrices. Assuming that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to do the operations below, then

$$(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$$
and
$$(\mathbf{AB})\mathbf{C}=\mathbf{A}(\mathbf{BC}).$$

### Distributive property

Matrix operations satisfy the distributive property. Assuming that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to do the operations below, then

$$\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{AB} + \mathbf{AC}\quad\mathrm{and}\quad (\mathbf{A}+\mathbf{B})\mathbf{C} = \mathbf{AC} + \mathbf{BC}.$$

### No commutative property
In general, matrix multiplication does not satisfy the commutative property, i.e., 
$$AB \neq BA,$$ even when the matrix sizes allow the operation to be performed.

Example:

$$\begin{bmatrix}
1 & 2
\end{bmatrix}
\begin{bmatrix}
1\\
2
\end{bmatrix}
=
\begin{bmatrix}
5
\end{bmatrix}$$

$$
\begin{bmatrix}
1\\
2
\end{bmatrix}
\begin{bmatrix}
1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 2\\
2 & 4
\end{bmatrix}$$

## Transpose and related properties

### Definition
The **transpose** of a matrix, denoted $T$ as a superscript, exchanges the rows and columns of the matrix. More formally, the $i,j$ element of $\mathbf{A}^T$ is the $j,i$ element of $\mathbf{A}$, i.e., $(\mathbf{A}^T)_{i,j} = \mathbf{A}_{j,i}$.

Example:

$$\begin{bmatrix}
2 & 9 & 3 \\
4 & 5 & 6
\end{bmatrix}^T = 
\begin{bmatrix}
2 & 4\\
9 & 5\\
3 & 6
\end{bmatrix}.$$

### Transpose and mathematical operations

Assume that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to perform the operations below. Additionally, assume that $c\in \mathbb{R}$ is a scalar constant.

The following properties are true:

* $c^T = c$
* $(\mathbf{A} + \mathbf{B})^T = \mathbf{A}^T + \mathbf{B}^T$
* $(\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T$, which can be extended to $(\mathbf{ABC})^T=\mathbf{C}^T \mathbf{B}^T \mathbf{A}^T$, etc.
* $(\mathbf{A}^T)^T=\mathbf{A}$

## Special matrices

### Square matrices

A matrix is **square** if the number of rows equals the number of columns. The **diagonal elements** of an $n\times n$ square matrix $\mathbf{A}$ are the elements $\mathbf{A}_{i,i}$ for $i = 1, 2, \ldots, n$. Any non-diagonal elements of $\mathbf{A}$ are called off-diagonal elements.

### Identity matrix
The $n\times n$ identity matrix $\mathbf{I}_{n\times n}$ is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so $\mathbf{I}_{n\times n}$ is often simplified to $\mathbf{I}$ or $I$.

Example:

$$\mathbf{I}_{3\times 3} = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

### Symmetric

A matrix $\mathbf{A}$ is **symmetric** if $\mathbf{A} = \mathbf{A}^T$, i.e., $\mathbf{A}_{i,j} = \mathbf{A}_{j,i}$ for all potential $i,j$.

* A symmetric matrix must be square.

### Idempotent

A matrix is **idempotent** if $\mathbf{AA} = \mathbf{A}$

* An idempotent matrix must be square.

## Matrix inverse

An $n\times n$ matrix $\mathbf{A}$ is invertible if there exists a matrix $\mathbf{B}$ such that $\mathbf{AB}=\mathbf{BA}=\mathbf{I}_{n\times n}$. The inverse of $\mathbf{A}$ is denoted $\mathbf{A}^{-1}$.

* Inverse matrices only exist for square matrices.

Some other properties related to the inverse operator:

* If $n\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are invertible then $(\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A} ^{-1}$.
* If $\mathbf{A}$ is invertible then $(\mathbf{A}^{-1})^T = (\mathbf{A}^T)^{-1}$.

## Matrix derivatives

We start with some basic calculus results.

Let $f(b)$ be a function of a scalar value $b$ and $\frac{df(b)}{db}$ denote the derivative of the function with respect to $b$. Assume $x$ is a fixed value. Then the following is true:

$f(b)$ | $\frac{df(b)}{db}$
---|---
$bx$ | $x$
$b^2$ | $2b$
$x b^2$ | $2bx$

Now lets look at the deriviate of a scalar function with respect to a vector.

Let $f(\mathbf{b})$ be a function of a $p\times 1$ column vector $\mathbb{b}=[b_1, b_2, \ldots,  b_p]^T$. The derivative of $f(\mathbf{b})$ with respect to $\mathbf{b}$ is denoted $\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}}$ and 
$$\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}} = \begin{bmatrix}
\frac{\partial f(\mathbf{b})}{\partial b_1}\\
\frac{\partial f(\mathbf{b})}{\partial b_2}\\
\vdots \\
\frac{\partial f(\mathbf{b})}{\partial b_p}
\end{bmatrix}.$$

Assume $\mathbf{X}$ is a fixed matrix. The following is true:

$f(\mathbf{b})$ | $\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}}$
---|---
$\mathbf{b}^T \mathbf{X}$ | $\mathbf{X}$
$\mathbf{b}^T \mathbf{b}$ | $2\mathbf{b}$
$\mathbf{b}^T \mathbf{X} \mathbf{b}$ | $2\mathbf{X}\mathbf{b}$


<!-- ## Matrix differentiation -->

<!-- Let $\mathbf{y} = (y_1, y_2, \ldots, y_n) -->

<!-- ## Matrix Differentiation 1 -->

<!-- Let $$\mathbf{y=Ax},$$ -->
<!-- where $\mathbf{y}$ is $m\times 1$, $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $m\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- $$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=A$$ -->

<!-- ## Matrix Differentiation 1 (Proof) -->

<!-- Since $i$th element of $\mathbf{y}$ is given by  -->
<!-- $$y_i=\sum\limits_{k=1}^{n}a_{ik}x_k,$$ -->
<!-- it follows that  -->
<!-- $$\frac{\partial y_i}{\partial x_j}=a_{ij}$$ -->
<!-- for all $i=1,\dots ,m,\quad j=1,\dots ,n$. Hence -->
<!-- $$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=A$$ -->


<!-- ## Matrix Differentiation 2 -->

<!-- Let the scalar $\alpha$ be defined by $$\alpha =\mathbf{y}^T\mathbf{Ax},$$ -->
<!-- 	where $\mathbf{y}$ is $m\times 1$, $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $m\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$ and $\mathbf{y}$, then -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{x}}=\mathbf{y}^T\mathbf{A}$$ -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T$$ -->

<!-- ## Matrix Differentiation 2 (Proof)	 -->

<!-- Define $\mathbf{w}^T=\mathbf{y}^T\mathbf{A}$ -->
<!-- 	and note that $\alpha =\mathbf{w}^T\mathbf{x}$ -->

<!-- Hence,  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{w}^T=\mathbf{y}^T\mathbf{A}.$$ -->
<!-- 	Since $\alpha$ is a scalar we can write -->
<!-- 	$$\alpha =\alpha^T=\mathbf{x}^T\mathbf{A}^T\mathbf{y}$$ -->
<!-- 	hence,  -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T$$ -->

<!-- ## Matrix Differentiation 3 -->
<!-- For the special case in which the scalar $\alpha$ is given by the quadratic form$$\alpha=\mathbf{x}^T\mathbf{A}\mathbf{x}$$ -->
<!-- 	where  $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $n\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{x}^T(\mathbf{A}+\mathbf{A}^T)$$ -->

<!-- ## Matrix Differentiation 3 (Proof) -->
<!-- By definition,$$\alpha =\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{n}a_{ij}x_ix_j$$ -->
<!-- 	Differentiating with respect to the $k$th element of $x$ we have -->
<!-- 	$$\frac{\partial \alpha}{\partial x_k}=\sum\limits_{j=1}^{n}a_{kj}x_j+\sum\limits_{i=1}^{n}a_{ik}x_i$$ -->
<!-- 	for all $k=1,\dots ,n$, and consequently, -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{x}}=\mathbf{x}^T\mathbf{A}^T+\mathbf{x}^T\mathbf{A}=\mathbf{x}^T(A^T+A)$$ -->

<!-- ## Matrix Differentiation 3.5 -->

<!-- For the special case where $\mathbb{A}$ is a symmetric matrix and  -->
<!-- 	$$\alpha=\mathbf{x}^T\mathbf{A}\mathbf{x}$$ -->
<!-- 	where  $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $n\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=2\mathbf{x}^T\mathbb{A}.$$ -->

<!-- ## Matrix Differentiation 4 -->

<!-- Let the scalar $\alpha$ be defined by$$\alpha =\mathbf{y}^T\mathbf{x}$$ -->
<!-- 	where $\mathbf{y}$ is $n\times 1$, $\mathbf{x}$ is $n\times 1$, and both $\mathbf{y}$ and $\mathbf{x}$ are functions of the vector $\mathbf{z}$. Then -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{z}}=\mathbf{x}^T\frac{\partial \mathbf{y}}{\partial \mathbf{z}}+\mathbf{y}^T\frac{\partial \mathbf{x}}{\partial \mathbf{z}}$$ -->

<!-- ## Matrix Differentiation 4.5 -->

<!-- Let the scalar $\alpha$ be defined by -->
<!-- 	$$\alpha =\mathbf{x}^T\mathbf{x}$$ -->
<!-- 	where $\mathbf{x}$ is $n\times 1$, and $\mathbf{x}$ is a function of the vector $\mathbf{z}$. Then -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{z}}=2\mathbf{x}^T\frac{\partial \mathbf{x}}{\partial \mathbf{z}}.$$ -->

