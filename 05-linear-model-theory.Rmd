# Basic theoretical results for linear models {#linear-model-theory}

In this chapter we discuss many basic theoretical results for linear models. The results are not interesting in themselves, but are foundational for the inferential results discussed in Chapter \@ref(linear-model-inference). Appendices \@ref(overview-of-matrix-facts) and \@ref(prob-review) provide an overview of properties related to matrices and random vectors that are needed for the derivations below.

We assume the responses can be modeled as
\[
Y_i=\beta_0+\beta_1 x_{i,1} +\ldots + \beta_{p-1}x_{i,[-1}+\epsilon_i,\quad i=1,2,\ldots,n,
\]
or using matrix formulation, as
\[
\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}.(\#eq:linear-model-def-matrix)
\end{equation}
\]
using the notation defined in Chapter \@ref(linear-model-estimation).

## Standard assumptions

We assume that the components of our linear model have the characteristics previously described in Table \@ref(tab:term-df). For the results we will derive below, we also need to make several specific assumptions about the errors. We have mentioned some of them previously, but discuss them all for completeness.

The first error assumption is that conditional on the regressors, the mean of the errors is zero. This means that $E(\epsilon_i \mid \mathbb{X} = \mathbf{x}_i)=0$ for $i=1,2,\ldots,n$, or using matrix notation,
\[
E(\boldsymbol{\epsilon}\mid \mathbf{X}) = 0_{n\times 1},
\]
where "$\mid \mathbf{X}$ is notation meaning "conditional on knowing the regressor values for all observations".

We also assume that the errors have constant variances and are uncorrelated, conditional on knowing the regressors, i.e., that \[\mathrm{var}(\epsilon_i\mid \mathbb{X}=\mathbf{x}_i) = \sigma^2, \quad i=1,2,\ldots,n,\]
and 
\[
\mathrm{cov}(\epsilon_i, \epsilon_j\mid \mathbf{X}) = 0, \quad i,j=1,2,\ldots,n,\quad i\neq j.
\]
In matrix notation, this is stated as
\[
\mathrm{var}(\mathbf{y})=\sigma^2\mathbf{I}_{n\times n}.
\]

Additionally, we assume that the errors are identically distributed. Formally, this may be written as
\begin{equation}
\epsilon_i \sim F, i=1,2,\ldots,n,
(\#eq:errordist)
\end{equation}
where $F$ is some arbitrary distribution. The $\sim$ is read as "distributed as". In other words, Equation \@ref(eq:errordist) means, "$\epsilon_i$ is distributed as $F$ for $i$ equal to $1,2,\ldots,n$". However, it is more common to assume the errors have a normal (Gaussian) distribution. Two uncorrelated normal random variables are also independent (this is true for normal random variables, but is not generally true for other distributions). Thus, we may concisely state the typical error assumptions as
\[
\epsilon_1,\epsilon_2,\ldots,\epsilon_n \mid \mathbf{X}\stackrel{i.i.d.}{\sim} N(0, \sigma^2),
\]
or using matrix notation as
\[
\begin{equation}
\mathbf{y}\mid \mathbf{X}\sim \mathsf{N}(0_{n\times 1},\sigma^2 \mathbf{I}_{n\times n}), (\#eq:error-assumptions-matrix)
\end{equation}
\]
where $0_{n\times 1}$ is the $n \times 1$ vector of zeros and $\mathbf{I}_{n\times n}$ is the $n\times n$ identity matrix. Equation \@ref(eq:error-assumptions-matrix)
combines the following assumptions:

1. $E(\epsilon_i \mid \mathbb{X}=\mathbf{x}_i)=0$ for $i=1,2,\ldots,n$.
1. $\mathrm{var}(\epsilon_i\mid \mathbb{X}=\mathbf{x}_i)=\sigma^2$ for $i=1,2,\ldots,n$.
1. $\mathrm{cov}(\epsilon_i,\epsilon_j\mid \mathbf{X})=0$ for $i\neq j$ with $i,j=1,2,\ldots,n$.
1. $\epsilon_i$ has a normal distribution for $i=1,2,\ldots,n$.

## Summary of results

For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation  \@ref(eq:error-assumptions-matrix), we have the following results:

1. $\mathbf{y}\mid \mathbf{X}\sim \mathsf{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_{n\times n})$.
1. $\hat{\boldsymbol{\beta}}\mid \mathbf{X}\sim \mathsf{N}(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1})$.
1. $\hat{\boldsymbol{\epsilon}}\mid \mathbf{X}\sim \mathsf{N}(0_{n\times n}, \sigma^2 (\mathbf{I}_{n\times n} - \mathbf{H}))$.
1. $\hat{\boldsymbol{\beta}}$ has the minimum variance among all unbiased estimators of $\boldsymbol{\beta}$ with the additional assumptions that the model is correct and $\mathbf{X}$ is full-rank.

We prove these results in the sections below.

## Results for $\mathbf{y}$

::: {.theorem #mean-y}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation  \@ref(eq:error-assumptions-matrix), 
\[
\begin{equation}
E(\mathbf{y}\mid \mathbf{X})=\mathbf{X}\boldsymbol{\beta}. (\#eq:mean-y)
\end{equation}
\]
:::

:::{.proof}
\[
\begin{align}
E(\mathbf{y}|\mathbf{X})&=E(\mathbf{X}\boldsymbol{\beta}+\epsilon|\mathbf{X})&\tiny\text{(by definition)}\\
&=E(\mathbf{X}\boldsymbol{\beta}|\mathbf{X})+E(\epsilon|\mathbf{X})&\tiny\text{(linearity of expectation)}\\
&=E(\mathbf{X}\boldsymbol{\beta}|\mathbf{X})+\mathbf{0}&\tiny\text{(by assumption about }\epsilon)\\
&=\mathbf{X}\boldsymbol{\beta}&\tiny\text{(since }\mathbf{X}\text{ and } \boldsymbol{\beta} \text{ are constant})\\
&\square
\end{align}
\]
:::

::: {.theorem #var-y}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation  \@ref(eq:error-assumptions-matrix), 
\[
\begin{equation}
\mathrm{var}(\mathbf{y}\mid \mathbf{X})=\sigma^2 \mathbf{I}_{n\times n}.(\#eq:var-y)
\end{equation}
\]
:::

:::{.proof}
\[
\begin{align}
\text{var}(\mathbf{y}|\mathbf{X})&=\text{var}(\mathbf{X}\boldsymbol{\beta}+\epsilon|\mathbf{X})&\tiny\text{(bydefinition)}\\
&=\text{var}(\epsilon|\mathbf{X})&\tiny(\mathbf{X}\boldsymbol{\beta}\text{ is constant)}\\
&=\sigma^2\mathbf{I}&\tiny\text{(by assumption)}\\
&\square
\end{align}
\]
:::

::: {.theorem #dist-properties-y}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix), 
\[
\begin{equation}
\mathbf{y}\mid \mathbf{X}\sim \mathsf{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I}_{n\times n}).(\#eq:dist-properties-y)
\end{equation}
\]
:::

:::{.proof}
How do we know that Equation \@ref(eq:dist-properties-y) is true? Show that:

* $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$ and
* $\mathrm{var}(\mathbf{y}) = \sigma^2 I_n$.
* Note $\mathbf{y}$ is a linear function of the multivariate normal vector $\boldsymbol{\epsilon}$, so $\mathbf{y}$ must also have a multivariate normal distribution.
:::

## Results for $\hat{\boldsymbol{\beta}}$

::: {.theorem #unbiasedness-betahat}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix), the OLS estimator for $\boldsymbol{\beta}$, 
\[
\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^T\mathbf{X}^T\mathbf{y},
\]
is an unbiased estimator for $\boldsymbol{\beta}$, i.e.,
\[
\begin{equation}
E(\hat{\boldsymbol{\beta}}\mid \mathbf{X})=\boldsymbol{\beta}.(\#eq:unbiasedness-betahat)
\end{equation}
\]

**Lemma: ** $E(y|X)=X\beta$

\[
\begin{align}
E(y|\beta)&=E(X\beta+\epsilon|X)&\tiny\text{(definition of }y)\\
&=E(X\beta|X)+E(\epsilon|X)&\tiny\text{(linearity of expectation)}\\
&=E(X\beta|X)+0&\tiny\text{(by assumption)}\\
&=X\beta&\tiny(X \text{ and } \beta\text{ are constant objects)}
\end{align}
\]

**Lemma: ** $\text{var}(y|X)=\sigma^2I$

\[
\begin{align}
\text{var}(y|X)&=\text{var}(X\beta+\epsilon|X)\\
&=var(\epsilon|X)&\tiny(X\beta \text{ is a constant object)}\\
&=\sigma^2I&\tiny\text{(by assumption)}
\end{align}
\]
:::

:::{.proof}

\[
\begin{align}
E(\hat\beta|X)&=E((X^TX)^{-1}X^Ty|X)&\tiny\text{(by OLS formula)}\\
&=(X^TX)^{-1}X^TE(y|X)&\tiny(X \text{ is a constant matrix)}\\
&=(X^TX)^{-1}X^TX&\tiny\text{(above result)}\\
&=I\beta&\tiny\text{(property of inverse matrices)}\\
&=\beta\\
&\square
\end{align}
\]

:::

::: {.theorem #var-betahat}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix), 
\[
\begin{equation}
\mathrm{var}(\hat{\boldsymbol{\beta}}\mid \mathbf{X})=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}.(\#eq:var-betahat)
\end{equation}
\]
:::

:::{.proof}

\[
\begin{align}
\text{var}(\hat\beta|X)&=\text{var}((X^TX)^{-1}X^Ty|X)&\tiny\text{(by OLS formula)}\\
&=(X^TX)^{-1}X^T\text{var}(y|X)((X^TX)^{-1}X^T)^T&\tiny\text{(pull constants out of variance)}\\
&=(X^TX)^{-1}X^T\text{var}(y|X)X(X^TX)^{-1}&\tiny\text{(simplification)}\\
&=(X^TX)^{-1}X^T(\sigma^2I)X(X^TX)^{-1}&\tiny\text{(previous result)}\\
&=\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}&\tiny(\sigma^2 \text{ is a scalar)}\\
&=\sigma^2(X^TX)^{-1}&\tiny\text{(simplpification)}\\
&\square
\end{align}
\]

:::

::: {.theorem #dist-properties-betahat}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix),
\[
\begin{equation}
\hat{\boldsymbol{\beta}}\mid \mathbf{X}\sim \mathsf{N}(\boldsymbol{\beta}, \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}).(\#eq:dist-properties-betahat)
\end{equation}
\]
:::

:::{.proof}
Since $\hat\beta=(X^TX)^{-1}X^Ty$ is a linear combination of $y$, and $y$ is a multivariate normal random vector, then $\hat\beta$ is also a multivariate normal random vector. Using the previous two results for the expectation and variance, 

\[
\hat\beta|X \sim N(\beta,\sigma^2(X^TX)^{-1})
\]

:::

## Results for the residuals

The residual vector can be expressed in various equivalent ways, such as
\[
\begin{align}
\hat{\boldsymbol{\epsilon}} &= \mathbf{y}-\hat{\mathbf{y}} \\
&= \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}.
\end{align}
\]

The **hat** matrix is denoted as
\[
\begin{equation}
\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T.(\#eq:hat-matrix-def)
\end{equation}
\]

Thus, using the substitution $\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ and the definition for $\mathbf{H}$ in Equation \@ref(eq:hat-matrix-def), we see that
\[
\begin{align}
\hat{\boldsymbol{\epsilon}} &= \mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}} \\ 
&= \mathbf{y} - \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\
&= \mathbf{y} - \mathbf{H}\mathbf{y} \\
&= (\mathbf{I}-\mathbf{H})\mathbf{y}. (\#eq:residual-hat-def)
\end{align}
\]

The hat matrix is an important theoretical matrix, as it projects $\mathbf{y}$ into the space spanned by the vectors in $\mathbf{X}$. It also has some properties that we will exploit in some of the derivations below.

::: {.theorem #h-properties}
The hat matrix $\mathbf{H}$ is symmetric and idempotent.
:::

:::{.proof}

Notice that,
\[
\begin{align}
H^T&=(X(X^TX)^{-1}X^T)^T&\tiny\text{(definition of }H)\\
&=(X^T)^T((X^TX)^{-1})^TX^T&\tiny\text{(apply transpose to matrix product)}\\
&=X((X^TX)^T)^{-1}X^T&\tiny\text{(simplification, reversibility of inverse and transpose)}\\
&=X(X^T(X^T)^T)^{-1}X^T&\tiny\text{(apply transpose to matrix product)}\\
&=X(X^TX)^{-1}X^T&\tiny\text{(simplification)}\\
&=H\\
&\square
\end{align}
\]
Thus, $H$ is symmetric.

Additionally,
\[
\begin{align}
HH&=(X(X^TX)^{-1}X^T)(X(X^TX)^{-1}X^T)&\tiny\text{(definition)}\\
&=X(X^TX)^{-1}(X^TX)(X^TX)^{-1}X^t&\tiny\text{(associative property of matrices)}\\
&=XI(X^TX)^{-1}X^T&\tiny\text{(property of inverse matrices)}\\
&=X(X^TX)^{-1}X^T&\tiny\text{(simplification)}\\
&=H\\
&\square
\end{align}
\]

Therefore, $H$ is idempotent.

:::

::: {.theorem #i-h-properties}
The matrix $\mathbf{I}_{n\times n} - \mathbf{H}$ is symmetric and idempotent.
:::

:::{.proof}

First, notice that,
\[
\begin{align}
(I-H)^T &= I^T-H^T&\tiny\text{(transpose to matrix sum)}\\
&= I-H&\tiny\text{(since I and H are symmetric)}\\
&\square
\end{align}
\]
Thus, $I-H$ is symmetric.

Next,
\[
\begin{align}
(I-H)(I-H)&=I-2H+HH&\tiny\text{(transpose to matrix sum)}\\
&=I-2H+H&\tiny\text{(since H is idempotent)}\\
&=I-H&\tiny\text{(simplification)}\\
&\square
\end{align}
\]
Thus, $I-H$ is idempotent.

:::

::: {.theorem #mean-residuals}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix),
\[
\begin{equation}
E(\hat{\boldsymbol{\epsilon}}\mid \mathbf{X})=0_{n\times 1}.(\#eq:mean-residuals)
\end{equation}
\]
:::

:::{.proof}

\[
\begin{align}
E(\hat\epsilon|X)&=E((I-H)y|H)\\
&=(I-H)E(y|X)&\tiny(I-H\text{ is non-random)}\\
&=(I-H)X\beta&\tiny\text{(earlier result)}\\
&=X\beta-HX\beta&\tiny\text{(distribute the product)}\\
&=X\beta-X^T(X^TX)^{-1}X^TX\beta&\tiny\text{(definition of H)}\\
&=X\beta-IX\beta&\tiny\text{(property of inverse matrix)}\\
&=X\beta-X\beta&\tiny\text{(simplification)}\\
&=0_{n\times1}&\tiny\text{(simplification)}\\
&\square
\end{align}
\]

:::

::: {.theorem #var-residuals}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix),
\[
\begin{equation}
\mathrm{var}(\hat{\boldsymbol{\epsilon}}\mid \mathbf{X})=\sigma^2 (\mathbf{I}_{n\times n} - \mathbf{H}).(\#eq:var-residuals)
\end{equation}
\]
:::

:::{.proof}

\[
\begin{align}
\text{var}(\hat\epsilon|X)&=\text{var}((I-H)y|X)\\
&=(I-H)\text{var}(y|X)(I-H)^T&\tiny(I-H\text{is nonrandom)}\\
&=(I-H)\sigma^2(I-H)^T&\tiny\text{(earlier result)}\\
&=\sigma^2(I-H)(I-H)&\tiny(I-H\text{ is symmetric)}\\
&=\sigma^2(I-H)&\tiny(I-H\text{ is idempotent)}\\
&\square
\end{align}
\]

:::

::: {.theorem #dist-properties-residuals}
For the linear model given in Equation \@ref(eq:linear-model-def-matrix) and under the assumptions summarized in Equation \@ref(eq:error-assumptions-matrix),
\[
\begin{equation}
\hat{\boldsymbol{\epsilon}}\mid \mathbf{X}\sim \mathsf{N}(0_{n\times n}, \sigma^2 (\mathbf{I}_{n\times n} - \mathbf{H})).(\#eq:dist-properties-residuals)
\end{equation}
\]

:::

:::{.proof}

Since $\hat\epsilon$  is a linear combination of multivariate normal vectors, and using previous results, it has mean $0_{n\times1}$ and variance-covariance matrix $\sigma^2(I-H)$.

The RSS can be represented as,
\[
y^T(I-H)y
\]

Thus,

$$
\begin{align}
RSS &= \hat\epsilon^T\hat\epsilon\\
&=((I-H)y)^T(I-H)y&\tiny\text{(previous result)}\\
&=y^T(I-H)^T(I-H)y&\tiny\text{(apply transpose)}\\
&=y^T(I-H)(I-H)y&\tiny(I-H \text{ is symmetric)}\\
&=y^T(I-H)y&\tiny(I-H \text{ is idempotent)}\\
\end{align}
$$

:::

## The Gauss-Markov Theorem

Suppose we will fit the regression model
\[
\mathbf{y}=\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]

Assume that 

1. $E(\boldsymbol{\epsilon}\mid \mathbf{X}) = 0$.
1. $\mathrm{var}(\boldsymbol{\epsilon}\mid \mathbf{X}) = \sigma^2 \mathbf{I}_{n\times n}$, i.e., the errors have constant variance and are uncorrelated.
1. $E(\mathbf{y}\mid \mathbf{X})=\mathbf{X}\boldsymbol{\beta}$
1. $\mathbf{X}$ is a full-rank matrix.

Then the **Gauss-Markov** states that the OLS estimator of $\boldsymbol{\beta}$, 
\[
\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^T\mathbf{X}^T\mathbf{y},
\]
has the minimum variance among all unbiased estimators of $\boldsymbol{\beta}$ and this estimator is unique.

Some comments:

- Assumption 3 guarantees that we have hypothesized the correct model, i.e., that we have included exactly the correct regressors in our model. Not only are we fitting a linear model to the data, but our hypothesized model is actually correct.
- Assumption 4 ensures that the OLS estimator can be computed (otherwise, there is no unique solution).
- The Gauss-Markov theorem only applies to unbiased estimators of $\boldsymbol{\beta}$. Biased estimators could have a smaller variance.
- The Gauss-Markov theorem states that no unbiased estimator of $\boldsymbol{\beta}$ can have a smaller variance than $\hat{\boldsymbol{\beta}}$.
- The OLS estimator uniquely has the minimum variance property, meaning that if an $\tilde{\boldsymbol{\beta}}$ is another unbiased estimator of $\boldsymbol{\beta}$ and $\mathrm{var}(\tilde{\boldsymbol{\beta}}) = \mathrm{var}(\hat{\boldsymbol{\beta}})$, then in fact the two estimators are identical and $\tilde{\boldsymbol{\beta}}=\hat{\boldsymbol{\beta}}$.

We do not prove this theorem.