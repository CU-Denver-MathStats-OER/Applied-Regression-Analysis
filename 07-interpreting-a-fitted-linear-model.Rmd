---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
---

# Interpreting a fitted linear model

<!-- ## Simple Linear Regression -->

<!-- ### Model description -->
<!-- The simple linear regression model is described by the mean function  -->
<!-- $$E(Y│X=x)=\beta_0+\beta_1 x,$$ -->

<!-- and variance function -->

<!-- $$var(Y│X=x)=\sigma^2,$$ -->
<!-- where: -->

<!-- * $Y$ is the response variable -->
<!-- * $X$ is a regressor variable -->
<!-- * $\beta_0$ and $\beta_1$ are known as *regression parameters* or *regression coefficients*. -->

<!-- Note:  -->

<!-- * The values that $Y$ takes are modeled as random variables. -->
<!-- * The values that $X$ takes are modeled as known, non-random values. -->

<!-- ### Interpreting the regression coefficients -->

<!-- * The *intercept*, $\beta_0$, is the mean response when $X=0$. -->
<!--   * i.e., $\beta_0=E(Y|X=0)$. -->
<!-- * The *slope*, $\beta_1$, is the mean change in the response when $X$ increases by 1 unit. -->
<!--   * i.e., $\beta_1 = E(Y|X=x+1)-E(Y|X=x)$. -->

<!-- ```{r} -->
<!-- plot(c(-1, 3), c(-1, 3), type = "n", xlab = expression(italic(X)), ylab = expression(italic(Y)), asp = 1) -->
<!-- abline(a = 1, b = 1) -->
<!-- lines(c(0, 0.25), c(0, 0)) -->
<!-- lines(c(0, 0.25), c(1, 1)) -->
<!-- lines(c(0.25, 0.25), c(0, 1)) -->
<!-- lines(c(0.25, 0.5), c(0.5, 0.5)) -->
<!-- text(0.5, 0.5, expression(italic(beta)[0]), pos = 4) -->
<!-- lines(c(1, 2), c(2, 2)) -->
<!-- lines(c(2, 2), c(2, 3)) -->
<!-- lines(c(2, 2.25), c(2.5, 2.5)) -->
<!-- text(2.25, 2.5, expression(italic(beta)[1]), pos = 4) -->

<!-- ``` -->
<!-- #  -->
<!-- # ## Simple Linear Regression -->
<!-- #  -->
<!-- # The simple linear regression model consists of the mean function  -->
<!-- # $$E(Y│X=x)=\beta_0+\beta_1 x,$$ -->
<!-- # and variance function -->
<!-- #  -->
<!-- # $$var(Y│X=x)=\sigma^2,$$ -->
<!-- # where: -->
<!-- #  -->
<!-- # - $Y$ is the response -->
<!-- # - $X$ is a regressor variable -->
<!-- # - $\beta_0$ and $\beta_1$ are known as **regression parameters** or **coefficients**. -->
<!-- #  -->
<!-- # # Simple Linear Regression -->
<!-- #  -->
<!-- # Chapter 2 of LMWR2, Chapter 2 and 3 of ALR4 -->
<!-- #  -->
<!-- #  -->
<!-- # ## Simple Linear Regression -->
<!-- #  -->
<!-- # The simple linear regression model consists of the mean function  -->
<!-- # $$E(Y│X=x)=\beta_0+\beta_1 x,$$ -->
<!-- #  -->
<!-- # and variance function -->
<!-- #  -->
<!-- # $$var(Y│X=x)=\sigma^2,$$ -->
<!-- # where: -->
<!-- #  -->
<!-- # - $Y$ is the response -->
<!-- # - $X$ is a regressor variable -->
<!-- # - $\beta_0$ and $\beta_1$ are known as **regression parameters** or **coefficients**. -->
<!-- #  -->
<!-- # ## Understanding the model -->
<!-- # ```{r} -->
<!-- # set.seed(1) -->
<!-- # x = runif(10, 60, 100) -->
<!-- # y = x + rnorm(10) -->
<!-- # plot(x,y, pch=20, xlab = 'Midyear Evaluation', ylab = 'Year-End Evaluation', main = 'Y = -0.74 +1.01 X') -->
<!-- # abline(lm(y~x), lwd = 2, col = 'red') -->
<!-- # ``` -->
<!-- #  -->
<!-- #  -->
<!-- # ## Understanding the model  -->
<!-- # ```{r out.width='1000px'} -->
<!-- # library(knitr) -->
<!-- # include_graphics('images/simple_lin_reg.png') -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## Understand Coefficients -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # include_graphics('images/coefficients.png', dpi=80) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## View from single observation -->
<!-- # Assume that we have n observations or cases for which the response and predictor variables are measured. -->
<!-- #  -->
<!-- # -	The responses are denoted $y_1,y_2,\dots ,y_n$. -->
<!-- # -	The regressor values for regressor X are denoted $x_1,x_2,\dots,x_n$. -->
<!-- #  -->
<!-- # Each response will deviate from its associated mean, so our statistical model must include an additional source of variation. -->
<!-- #  -->
<!-- # The statistical model for each response is -->
<!-- # $$y_i=\beta_0+\beta_1 x_i+\epsilon_i,\quad   i=1,2,\dots,n,$$ -->
<!-- #  -->
<!-- # where $\epsilon_i$ denotes the deviation of $y_i$ from its mean. -->
<!-- #  -->
<!-- # -	The $\epsilon_i$ are known as errors. -->
<!-- #  -->
<!-- # ## Conditions on Error -->
<!-- #  -->
<!-- # Conditional on knowing the regressor values, the errors have:  -->
<!-- #  -->
<!-- # -	Mean 0 -->
<!-- # -	Variance $\sigma^2$ (constant) -->
<!-- # -	And are uncorrelated. -->
<!-- #  -->
<!-- # Mathematically, this is the same as:  -->
<!-- #  -->
<!-- # -	$E(\epsilon_i│X=x_i )=0$  -->
<!-- # -	$var(\epsilon_i│X=x_i )=\sigma^2$ -->
<!-- # -	$cov(\epsilon_i,\epsilon_j )=0$  when $i\neq j$. -->
<!-- #  -->
<!-- # ## What about the response? -->
<!-- #  -->
<!-- # - Mean: -->
<!-- #  -->
<!-- #   $$E[Y_i|X_i] = E[\beta_0+\beta_1X_i + \epsilon_i]= \beta_0 + \beta_1X_i$$ -->
<!-- #   $$Var(Y_i|X_i) = Var(\beta_0+\beta_1X_i + \epsilon_i) = Var(\epsilon_i) = \sigma^2$$ -->
<!-- #   $$cov(Y_i, Y_j) = 0,\quad i \neq j$$ -->
<!-- #   The responses $Y_i$ come from probability distributions whose means are $\beta_0+\beta_1X_i$ and whose variances are $\sigma^2$, the same for all levels of $X$. Further, any two responses $Y_i$ and $Y_j$ are uncorrelated.  -->
<!-- #    -->
<!-- # ## Example -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # include_graphics('images/simple_lin_reg_values.png',dpi = 100) -->
<!-- # ``` -->
