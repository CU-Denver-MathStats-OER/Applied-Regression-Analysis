---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
---

# Categorical predictors

Categorical  predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of a categorical variable.

Let $X$ denote a numeric regressor, with $x_i$ denoting the value of $X$ for observation $i$.

Let $F$ denote a categorical variable with levels $L_1, L_2, \ldots, L_K$. The $F$ stands for "factor", while the $L$ stands for "level". The notation $F_i$ denotes the value of $F$ for observation $i$. A regression model can only be fit for numeric regressors. In order to use a categorical predictor in a regression model, we must convert it to a set of one or more indicator variables. 

An indicator function takes the value 1 of a certain property is true and 0 otherwise. An indicator variable is the variable that results from applying an indicator function to each observation in a data set. Many notations exist for indicator functions. We will adopt the notation 

\begin{equation*}
I_S(x) = \begin{cases}
1 & \textrm{if}\;x \in S\\
0 & \textrm{if}\; \notin S
\end{cases},
\end{equation*}

which is shorthand for a function that returns 1 if $x$ is in the set $S$ and 0 otherwise. We define $D_j$ to be the indicator (dummy) variable for factor level $L_j$ of $F$. The value of $D_j$ for observation $i$ is denoted $d_{i,j} = I_{\{L_j\}}(F_i)$, i.e., $d_{i,j}$ is 1 if observation $i$ has factor level $L_j$ and 0 otherwise.

## Interpretation

In the examples below, we use notation $\beta_X$ to denote the regression coefficient for regressor $X$ instead of notation like $\beta_3$ to more closely connect each coefficient with the regressor to which it is related.

### One-way ANOVA

A one-way ANOVA assumes a constant mean for each level of a categorical variable. In this context, the linear model is formulated as
$$ Y_i = \beta_{int} + \beta_{L_2} d_{i,2} + \cdots + \beta_{L_K} d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$

When an observation has level $L_1$, then the expected response is $\beta_{int}$. More specifically, $$E(Y|F=L_1) = \beta_{int} + \beta_{L_2} 0 + \cdots + \beta_{L_K} 0 = \beta_{int}.$$

When an observation has level $L_2$, the expected response is $\beta_{int} + \beta_{L_2}$. More formally, 
$$E(Y|F=L_2) = \beta_{int} + \beta_{L_2} 1 + \beta_{L_3} 0 + \cdots + \beta_{L_K} 0 = \beta_{int} + \beta_{L_2}.$$ Thus, the mean response for observations having level $L_2$ is $\beta_{int} + \beta_{L_2}$. Alternatively, we can say that $\beta_{L_2}$ represents the expected change in the response when comparing observations having level $L_1$ and $L_2$. In general, $$E(Y|F=L_j) = \beta_{int} + \beta_{L_j},\quad j=2,\ldots,K.$$

In the context of a one-way ANOVA:

* $\beta_{int}$ represents the expected response for observations having the reference level.
* $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having the reference level and level $L_j$. 

### Main effects models

A main effects model is also called a parallel lines model since the regression equations for each factor level produce lines parallel to one another.

A parallel lines model is formulated as
$$ Y_i = \beta_{int} + \beta_{X} x_i + \beta_{L_2} d_{i,2} + \cdots + \beta_{L_K} d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$

When an observation has level $L_1$, then the expected response is $\beta_{int} + \beta_1 X$. More specifically, $$E(Y|X = x, F=L_1) = \beta_{int} + \beta_X x + \beta_{L_2} 0 + \cdots + \beta_{L_K} 0 = \beta_{int} + \beta_X x.$$ Thus, $E(Y|X = 0, F=L_1) = \beta_{int}.$

When an observation has level $L_2$, the expected response is $\beta_{int} + \beta_{X} X + \beta_{L_2}$. More formally, 
$$E(Y|X = x, F=L_2) = \beta_{int} + \beta_X x + \beta_{L_2} 1 + \beta_{L_3} 0 + \cdots + \beta_{L_K} 0 = \beta_{int} + \beta_X x + \beta_{L_2}.$$ Thus, the mean response for observations having level $L_2$ is $\beta_{int} + \beta_{L_2} + \beta_{X} x$. In general, 
$$E(Y|X = x, F=L_j) = \beta_{int} + \beta_X x + \beta_{L_j},\quad j = 2,\ldots,K.$$ Thus,

$$E(Y|X=x, F=L_j) - E(Y|X=x, F=L_1) = (\beta_{int} + \beta_X x + \beta_{L_j}) - (\beta_{int} + \beta_X x) = \beta_{L_j}.$$

In the context of parallel lines models:

* $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
* $\beta_X$ is the expected change in the response when $X$ increases by 1 unit for a fixed level of $F$.
* $\beta_{L_j}$, for $j=2,\ldots,K$ represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X$ fixed at the same value. 

### Interaction models 

An interaction model is also called a separate lines model since the regression equations for each factor level produce lines that are distinct and separate.

A separate lines model is formulated as
$$ Y_i = \beta_{int} + \beta_{X} x_i + \beta_{L_2} d_{i,2} + \cdots + \beta_{L_K} d_{i,K} + + \beta_{X L_2} x_i d_{i,2} + \cdots +  \beta_{X L_j} x_i d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$
When an observation has level $L_1$, then the expected response is $\beta_{int} + \beta_1 X$. More specifically, $$E(Y|X = x, F=L_1) = \beta_{int} + \beta_X x + \beta_{L_2} 0 + \cdots + \beta_{L_K} 0  + \beta_{X L_2} x_i 0 + \cdots +  \beta_{X L_K} x_i 0 = \beta_{int} + \beta_X x.$$ Thus, $E(Y|X = 0, F=L_1) = \beta_{int}.$

When an observation has level $L_j$, for $j=2,\ldots,K$, the expected response is $\beta_{int} + \beta_{X} X + \beta_{L_j} + \beta_{X L_J} X.$ More formally, 
$$E(Y|X = x, F=L_j) = \beta_{int} + \beta_X x + \beta_{L_j} + \beta_{X L_j} x.$$


Note that $$E(Y|X=0, F=L_1) = \beta_{int}.$$ 
Additionally, we note that $$E(Y|X=0, F=L_j) - E(Y|X=0, F=L_1) = (\beta_{int} + \beta_X 0 + \beta_{L_j} + \beta_{X L_J} 0) - (\beta_{int} + \beta_X 0) = \beta_{L_j}.$$

In the context of separate lines models:

* $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
* $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X=0$.
* $\beta_X$ represents the expected change in the response when $X$ increases by 1 unit for observations having the reference level.
* $\beta_X L_j$, for $j=2,\ldots,K$, represents the difference in the expected rate of change when $X$ increases by 1 unit for observations have the baseline level in comparison to level $L_j$.

### Extensions

In the models above, we have only discussed possibilities with a single numeric variable and a single factor variable. Naturally, one can consider models with multiple factor variables, multiple numeric variables, interactions between factor variables, interactions between numeric variables, etc. The models become more complicated, but the ideas are similar. One simply has to keep track of what role each coefficient plays in the model.


<!-- We could fit other ANCOVA models, such as the common intercept model, but the parallel lines and separate lines models are the most common. -->

<!-- You can add additional interactions, squared and cubic explanatory variables to the model, etc. -->
