---
output:
  html_document:
    css: style.css
  pdf_document:
    pandoc_args: --listings
    includes:
      in_header: preamble.tex
bibliography:
- book.bib
- packages.bib
link-citations: yes
editor_options: 
  markdown: 
    wrap: 72
monofont: "Lucida Console"
---

```{r, include = FALSE}
# change Console output behavior
# knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
knitr::opts_chunk$set(collapse = TRUE)
library(formatR)
# knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 55), tidy = TRUE)
options(width = 55,
        str = strOptions(strict.width = "cut"))
library(kableExtra)

# https://bookdown.org/yihui/rmarkdown-cookbook/hook-truncate.html
# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # truncate the output
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```

# Review of Estimation and Inference {#est-infer-review}

A primary purpose of statistics is taking a sample of values from a population and using the sample to draw conclusions about that population. In what follows, we discuss statistical concepts related to estimation, hypothesis testing, and confidence intervals.

## Estimation

A **parameter** is a numeric characteristic that describes a population. E.g., the population mean, standard deviation, or cumulative distribution function.

The **target** parameter or **parameter of interest** is the population parameter we would like to estimate.

There are different kinds of estimates:

- A **point estimate** is a single number that we *hope* is close to the true value of the the target parameter.
- An **interval estimate** is an interval of numbers that we *hope* will contain the target parameter.

an estimate and an estimator are different, but related concepts.

- An estimate is a specific number (for a point estimate) or a specific range of numbers (for an interval estimate).
- An **estimator** is a formula we use to calculate an estimate (once we get a sample of data).

Once the data are observed, an estimate is fixed. An estimator is a random variable. An estimator produces different estimates based on the sample of data we obtain from the population.

The **sampling distribution** of an estimator is the distribution of the estimates you get when you use the estimator to compute estimates from all possible samples of a fixed size $n$ from the population of interest.

A point estimator, $\hat{\theta}$, is an **unbiased estimator** of a target parameter, $\hat{\theta}$, if $E(\hat{\theta})=\theta$. An estimator is biased if it is not biased.

The **bias** of an estimator is defined as
\[
B(\hat{\theta})=E(\hat{\theta})-\theta.
\]

The **variance of an estimator** is defined as
\[
\mathrm{var}(\hat{\theta})=E[\hat{\theta}-E(\hat{\theta})]^2.
\]
The **standard error of an estimator** is the standard deviation of the estimator, i.e.,
\[
\mathrm{sd}(\hat{\theta})=\sqrt{\mathrm{var}(\hat{\theta})}.
\]
Typically, we cannot compute the standard error of an estimator because it is a function of parameters that we do not know. Instead, we use the sample data to estimate the standard error. When we hear or read the term "standard error", we must carefully evaluate whether the "standard error" presented is the theoretical standard error or the estimated standard error (and it's nearly always the latter).

The **mean square error** of a point estimator is 
\[
MSE(\hat{\theta})=E(\hat{\theta}-\theta)^{2},
\]
which is equivalent to 
\[
MSE(\hat{\theta})=\mathrm{var}(\hat{\theta})+[B(\hat{\theta})]^{2}.
\]

The MSE formula makes it clear that there is a "bias-variance trade off" when choosing between point estimators. Typically, unbiased point estimators will have larger variance (and correspondingly, MSE). Biased estimators will often have smaller MSE, but are (obviously) biased. It's a trade off we have to balance.

## Hypothesis Testing

A **statistical test of hypotheses** or **hypothesis test** is a
statistical procedure used to decide between a null hypothesis, $H_0$, and an alternative hypothesis, $H_a$ or $H_1$. The null hypothesis is usually a hypothesis that "nothing interesting is going on". The alternative hypothesis is (generally) the complement of the null hypothesis and is usually what we want to show is true.

A **test statistic** is a number used to decide between $H_0$ and $H_a$. A test statistic is a function of the data, and generally, parameters in the hypotheses. A test statistic measures the compatibility of the observed data with $H_0$: a "small" test statistic suggests the observed data are consistent with $H_0$, while an "extreme" test statistic indicates that the observed data are inconsistent with $H_0$, which we take as evidence that $H_a$ is true.

The **null distribution** allows us to determine values of the test statistic are typical or unusual when $H_0$ is true. Formally, the null distribution is the distribution of our test statistic under the assumption that $H_0$ is true.

There are two types of errors we can make when doing hypothesis testing:

-   A Type I error is rejecting $H_0$ when $H_0$ is true.
-   A Type II error is failing to reject $H_0$ when $H_a$ is true.

We can control the Type I error rate at a specified level, $\alpha$, called the **significance level**, since we know the distribution of our
test statistic under the assumption that $H_0$ is true. We reject $H_0$
and conclude that $H_a$ is true if the test statistic falls in the
**rejection region** of the null distribution, which is the set of test
statistics that are the $100\alpha\%$ most unlikely test statistics if
$H_0$ is true.

Instead of using the test statistic directly to decide between $H_0$ and
$H_a$, we generally use the test statistic to compute a p-value. The
**p-value** of a test statistic is the probability of seeing a test
statistic at least as supportive of $H_a$ when $H_0$ is true. If we
specify the significance level, $\alpha$, prior to performing our
hypothesis test (which is the ethical thing to do), then we reject $H_0$
and conclude $H_a$ is true when the p-value $<\alpha$. Otherwise, we
fail to reject $H_0$.

Researchers sometimes say that the smaller the p-value, the stronger the
evidence that $H_a$ is true and $H_0$ is false. This isn't definitively
true because the p-value doesn't have the ability to tell us whether
$H_0$ is true but our observed data were very unlikely or $H_0$ is
false. When $H_0$ is true, then the test statistic has a
$\mathsf{U}(0,1)$ (for simple hypotheses and continuous test
statistics). However, if the $H_a$ is true, then the p-value is more
likely to be small, which makes us think $H_a$ is true for small
p-values. However, unless we know the **power** of our test, which is
the probability that we reject $H_0$ when $H_a$ is true, then it is very
difficult to assess how much evidence for $H_a$ a small p-value
provides. However, @gibson_pvalue point out the p-values can be
interpreted naturally on a $\log_{10}$ scale. @gibson_pvalue states:

> The p-value can be expressed as $p=c\times 10^{-k}$ so that
> $\log_{10}(p)=-\log_{10}(c)+k$, where $c$ is a constant and $k$ is an
> integer, which implies that only the magnitude *k* measures the actual
> strength of evidence [@boos_stefanski_2011_pvalues]. $\ldots$ This
> would suggest that $p=0.01$ ($k=2$) could be interpreted as twice the
> evidence [for $H_a$ as] $p=0.10$ ($k=1$).

@gibson_pvalue provides a thorough review of p-value interpretation.
Table \@ref(tab:pvalue-interp) summarizes common strength of evidence
interpretations for p-values.

```{r pvalue-interp, echo = FALSE}
interp_df <- cbind(pvalue = c("more than -
                              $0.10$",
                              "$\\leq 0.10$",
                              "$\\leq 0.05$",
                              "$\\leq 0.01$",
                              "$\\leq 0.001$"),
                   interpretation = c("no evidence for $H_a$",
                                     "weak evidence for $H_a$",
                                     "moderate evidence for $H_a$",
                                     "strong evidence for $H_a$",
                                     "very strong evidence for $H_a$"))
kbl(interp_df,
    caption = "An summary of common strength-of-evidence interpretations for p-values.",
    booktabs = TRUE,
    escape = FALSE,
    col.names = c("p-value", "Interpretation"),
    align = c('l', 'l')) |>
  column_spec(column = 2, width = "3in")  |>
  kable_styling(full_width = FALSE)
```

**Example**

Suppose that $Y_1,\ldots,Y_n$ is a random sample (in other words,
independent and identically distributed sample) from a population having
a normal distribution with unknown mean $\mu$ and variance $\sigma^2=1$.

We would like to decide between the following two hypotheses:
$H_0:\mu=0$ and $H_a:\mu\neq 0$.

If $H_0$ is true, then the statistic $\bar{Y}\sim \mathcal{N}(0,1/n)$ and
the test statistic

$$
Z^*=\bar{Y}/(1/\sqrt{n})=\sqrt{n}\bar{Y}\sim \mathcal{N}(0, 1),
$$

i.e., the null distribution of our test statistic is $\mathcal{N}(0,1)$.

If
$\alpha=0.10$, then the 10% of test statistics that are most unlikely if $H_0$
is true (i.e., most supportive of \$H_a\$) are more extreme than
$z^{0.95}$ and $z^{0.05}$, the 0.05 and 0.95 quantiles of a standard
normal distribution, respectively. (The superscript in the quantile
notation indicates the area to the right of the quantile in the CDF).
The 0.05 quantile of the standard normal distribution is -1.65 and the
0.95 quantile is 1.65. In R, we can find these quantile using the
`qnorm` function, where the first argument of `qnorm` is `p`, which is
the vector of probabilities to the LEFT of the quantile. We verify these
quantiles using the code below.

```{r}
qnorm(c(0.05,0.95))
```

$H_0$ should be rejected when $Z^*$ is less than -1.65 or more than
1.65, i.e., the rejection region is $(-\infty, -1.65)\cup(1.65,\infty)$.

Alternatively, we could compute the p-value using the formula
$2P(Z\geq |Z^*|)$ in order to make our choice between the hypotheses.

Suppose $z^*=1.74$ and $\alpha=0.10$.  The test statistic is in the rejection region, so we would conclude that $H_a$
is true. The p-value is $2P(Z\geq 1.74)=0.082$. Using the p-value approach we would conclude that $H_a$ is true. (Note that the rejection region and p-value approaches to deciding between hypotheses must agree). In R, we can compute the p-value using the code below.

```{r}
2*(1 - pnorm(1.74))
```

In straightforward language, our interpretation could be: there is weak evidence that the population mean differs from 0.

## Confidence Intervals

A **confidence interval** provides us with plausible values of a target parameter. It is the most common type of interval estimator.

A confidence interval procedure has an associated **confidence level**. When independent random samples are taken repeatedly from the population, a confidence interval procedure will produce intervals containing the target parameter with probability equal to the confidence level. Confidence level is associated with a confidence interval *procedure*, not a specific interval. A 95% confidence interval procedure will produce intervals that contain the target parameter 95% of the time. A specific interval estimate will either contain the target parameter or it will not.

The formulas for confidence intervals are usually derived from a pivotal quantity. A **pivotal quantity** is a function of the data and the target parameter whose distribution does not depend on the value of the target
parameter. 

**Example:**

Suppose $Y_1,Y_2,\ldots,Y_n \stackrel{i.i.d.}{\sim} \mathcal{N}(\mu, 1)$.

The random variable $Z=(\bar{Y}-\mu)/(1/\sqrt{n})\sim \mathcal{N}(0,1)$ is a pivotal quantity.

Since $P(-1.96\leq Z\leq 1.96)=0.95$, we can derive that $P(\bar{Y}-1.96\times 1/\sqrt{n}\leq \mu \leq \bar{Y}+1.96\times 1/\sqrt{n})=0.95$.

Our 95% confidence interval for $\mu$ (in this context) is [P(\bar{Y}-1.96\times 1/\sqrt{n} ,P(\bar{Y}+1.96\times 1/\sqrt{n}].

If \bar{Y}=0.551, then the associated 95% confidence interval for \mu when $n=10$ is [-0.070,1.171].

If we used the CI formula given above to produce 100 intervals from
independent data sets, then about 95% of them would contain the true mean, but about 5% would not.

## Linking Hypothesis Tests and Confidence Intervals

CIs are directly linked to hypothesis tests.

A $100(1-\alpha)\%$ two-sided confidence interval for target parameter $\theta$ is linked with a hypothesis test of $H_0:\theta = c$ versus $H_a:\theta \neq c$ tested at level
$\alpha$.

- Any point that lies within the $100(1-\alpha)\%$ confidence interval for $\theta$ represents a value of $c$ for which the associated null hypothesis would not be rejected at significance level $\theta$.
- Any point outside of the confidence interval is a value of $c$ for which the associated null hypothesis would be rejected.

Similar relationships hold for one-sided CIs and hypothesis tests.

**Example:**

Consider the 95% confidence interval for $\mu$ we previously
constructed: [-0.070,1.171].

That interval is conceptually linked to the statistical test of $H_0:\mu = c$ versus $H_a:\mu \neq c$ using $\alpha =0.05$.

We would reject $H_0$ for any hypothesized values of $c$ less than -0.070 or more than 1.171. We would fail to reject $H_0$ for any values of $c$ between -0.70 and 1.171. 

A CI provides us with much of the same information as a hypothesis test, but it doesn't provide the p-value or allow us to do hypothesis tests at different significance levels.

Confidence regions are often preferred over hypothesis tests because they provide additional information in the form of plausible parameters values while giving us enough information to perform a hypothesis test.

<!-- ## Bootstrap Confidence Intervals -->

<!-- The conventional parametric confidence interval assumes we know the -->
<!-- distribution of the population in order to find a pivotal quantity. The -->
<!-- population distribution is needed to determine the sampling distribution -->
<!-- of our statistic. -->

<!-- A bootstrap confidence interval can be constructed if the population -->
<!-- distribution is unknown. -->

<!-- How would we estimate the sampling distribution of a statistic without -->
<!-- statistical theory? -->

<!-- Estimating the Sampling Distribution of a statistic for a Known -->
<!-- Population -->

<!--     Obtain a random sample of size n from the population. -->
<!--     Compute the statistic for the random sample. -->
<!--     Perform steps 1 and 2 many times. -->
<!--     Determine the empirical distribution of the statistics from the independent samples. -->

<!-- Consider a comparison of the estimated sampling distribution (the -->
<!-- empirical distribution) and the true sampling distribution of Y ̅ when -->
<!-- sampling n=10 observations from a N(0,1) population. -->

<!-- The bootstrap method allows us to approximate the sampling distribution -->
<!-- of a statistic by using the observed data to produce simulated data -->
<!-- sets. -->

<!-- The bootstrap method uses the observed data to approximate the shape, -->
<!-- spread, and bias of the sampling distribution of a statistic. -->

<!-- A bootstrap sample is a sample with replacement of size n from the -->
<!-- observed data. -->

<!-- Estimating the Sampling Distribution Using the Bootstrap Method Obtain a -->
<!-- bootstrap sample of the observed data by selecting with replacement a -->
<!-- sample of size n from the observed data. Compute the statistic for the -->
<!-- random sample. Perform steps 1 and 2 many times. Determine the empirical -->
<!-- distribution of the statistics from the independent bootstrap samples -->
<!-- (a.k.a., the bootstrap distribution). -->

<!-- Consider a comparison of the bootstrap, empirical, and true sampling -->
<!-- distributions of Y ̅ when the data are obtained from a N(0,1) -->
<!-- population. -->

<!-- A 100(1-α)% confidence interval for a target parameter θ can be obtained -->
<!-- by determining the α/2 and 1-α/2 quantiles of the bootstrap distribution -->
<!-- for θ ̂. E.g., a 95% CI for a population mean μ could be obtained by -->
<!-- taking the 0.025 and 0.975 quantiles of the bootstrap distribution for y -->
<!-- ̅. -->

<!-- Example (continued): Continuing our previous example, the 95% bootstrap -->
<!-- confidence interval for μ is [-0.162, 1.109]. -->

<!-- > quantile(boot_ybar, prob = c(0.025, 0.975)) 2.5% 97.5% -0.1610048 -->
<!-- > 1.1085468 -->

<!-- The parametric 95% confidence interval is [-0.070,1.171]. -->

<!-- The parametric and bootstrap methods of constructing confidence -->
<!-- intervals will NOT produce identical intervals, though the intervals -->
<!-- should be similar if the distributional assumptions are satisfied. -->

<!-- If you are unsure whether the distributional assumptions are satisfied, -->
<!-- you should use the bootstrap method to construct your confidence -->
<!-- interval. -->

<!-- Example: Grogan and Wirth (1981) provide data on the wing length in -->
<!-- millimeters of nine members of a species of midge (small, two-winged -->
<!-- flies). From these nine measurements, we wish to make inference about -->
<!-- the population mean μ. Assume that the data are an i.i.d. sample from a -->
<!-- N(μ,σ\^2) population with μ and σ\^2 unknown. The data are: 1.64, 1.70, -->
<!-- 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08 -->

<!--     Construct a 98% parametric confidence interval for μ. -->
<!--     Construct a 98% bootstrap confidence interval for μ. -->
<!--     Perform a hypothesis test of whether μ>2 at α=0.02. -->
