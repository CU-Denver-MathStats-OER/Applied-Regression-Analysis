---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# Defining and fitting a linear model

In this chapter, we define a linear model and discuss the basic estimation of its parameters. We leave discussion of more theoretical aspects of the model to subsequent chapters.

## Background and terminology

Regression models are used to model the relationship between:

* one or more **response** variables and
* one or more **predictor** variables.

The distinction between these two types variables is their purpose in the model.

* Predictor variables are used to predict the value of the response variable.

Response variables are also known as **outcome**, **output**, or **dependent** variables.

Predictor variables are also known as **explanatory**, **regressor**, **input**, **dependent**, or **feature** variables.

Note:  Because the variables in our model are often interrelated, describing these variables as independent or dependent variables is vague and is best avoided.

A distinction is sometimes made between **regression models** and **classification models**. In that case:

* Regression models attempt to predict a numerical response.
* Classification models attempt to predict the category level a response will have.

## Goals of regression

The basic goals of a regression model are to:

1. *Predict* future or unknown response values based on specified values of the predictors.
    * What will the selling price of a home be?
2. *Describe* relationships (associations) between predictor variables and the response.
    * What is the general relationship between the selling price of a home and the number of bedrooms the home has?

With our regression model, we also hope to be able to:

1. *Generalize* our results from the sample to the a larger population of interest.
    * E.g., we want to extend our results from a small set of college students to all college students.
2. *Infer causality* between our predictors and the response.
    * E.g., if we give a person a vaccine, then this causes the person's risk of catching the disease to decrease.

**A "true model" doesn't exist for real data**. The data-generating process is far more complex than the models we can realistically fit to the data. Thus, finding the true model should not be the goal of a regression analysis. A regression analysis should attempt to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.)

## Definition of a linear model

A **linear model** is a regression model in which the regression coefficients (to be discussed later) enter the model linearly.

* A linear model is just a specific type of regression model.

### Basic construction and relationships

We begin by defining notation for the objects we will need and clarifying some of their important properties.

* $Y$ denotes the response variable.
  * The response variable is treated as a random variable.
  * We will observe realizations of this random variable for each observation in our data set.
* $X$ denotes a single predictor variable. $X_1$, $X_2$, \ldots, $X_{p-1}$ denote the predictor variables $1,2,\ldots,p$.
  * The predictor variables are treated as non-random variables.
  * We will observe values of the predictors variables for each observation in our data set.
* $\beta_0$, $\beta_1$, \ldots, $\beta_{p-1}$ denote **regression coefficients**.
  * Regression coefficients are statistical parameters that we will estimate from our data.
  * Like all statistical parameters, regression coefficients are treated as fixed (non-random) but unknown values.
  * Regression coefficients are not observable.
* $\epsilon$ denotes **error**.
  * The error is not observable.
  * The error is treated as a random variable.
  * The error is assumed to have mean 0, i.e., $E(\epsilon) = 0$.
  * Since $E(\epsilon) = 0$ and $X$ is non-random, the expectation of $\epsilon$ conditional on $X$ is also 0, i.e., $E(\epsilon | X) = 0$.
  * In this context, error doesn't mean "mistake" or "malfunction". $\epsilon$ is simply the deviation of the response from its mean.

A **linear model** for $Y$ is defined by the equation
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1} + \epsilon.
(\#eq:lmdef)
\end{equation}

We now emphasize the relationship between the response, the mean response, and the error. The mean of the response variable will depend on the values of the predictor variables. Thus, we can only discuss the expectation of the response variable conditional on the values of the predictor variables. This is denoted as $E(Y | X_1, \ldots, X_{p-1})$.

For simplicity, assume our linear model only has a single predictor (this is an example of simple linear regression). Based on what we've presented, we have that

\begin{align}
E(Y|X) &= E(\beta_0 + \beta_1  X + \epsilon | X) \\
 &= E(\beta_0 | X) + E(\beta_1 X | X) + E(\epsilon | X) \\
 &= \beta_0 + \beta_1 X + 0\\
 &= \beta_0 + \beta_1 X.
\end{align}

The second line follows from the fact that the expectation of a sum of random variables is the sum of the expectation of the random variables. The third line follows from the fact that the expected value of a constant (non-random) value is the constant (the regression coefficients and $X$ are non-random) and by our assumption that the errors have mean 0 (unconditionally or conditionally on the predictor variable.)

Thus, we see that we see that for a simple linear regression model
$$ Y = E(Y|X) + \epsilon.$$
For a model with multiple predictors, this extends to
$$Y = E(Y|X_1, X_2, \ldots, X_{p-1}) + \epsilon.$$
Thus, our response may be written as the sum of the mean response conditional on the predictors, $E(Y|X_1, X_2, \ldots, X_{p-1})$, and the error. This is why previously we discussed the fact that the error is simply the deviation of the response from its mean.

Alternatively, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values, i.e.,
$$E(Y|X_1, X_2, \ldots, X_{p-1}) = \sum_{j=0}^{p-1} c_j \beta_j,$$
where $c_0, c_1, \ldots, c_{p-1}$ are known values. In fact, the $c_i, i = 1,2,\ldots,n$ can be any function of $X_1,X_2,\ldots,X_n$! e.g., $c_1 = X_1 * X_2 * X_3$, $c_3 = X_2^2$, $c_8 = ln(X_1)/X_2^2$.

Some examples of linear models:

* $E(Y|X) = \beta_0 + +\beta_1 X + \beta_2 X^2$.
* $E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2$.
* $E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 * X_2$.
* $E(Y|X_1, X_2) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2^{-1}$.
* $E(\ln(Y)|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2$.
* $E(Y^{-1}|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2$.

Some examples of non-linear models:

* $E(Y|X) = \beta_0 + e^{\beta_1 X}$.
* $E(Y|X) = \beta_0 + \beta_1 X/(\beta_2 + X)$.

<!-- The model in Equation \@ref(eq:lmdef) is a **statistical model** because there is uncertainty in the response.  -->

### As a system of equations
A linear regression analysis will model the data using a linear model. Suppose we have sampled $n$ observations from a population. We now introduce some additional notation:

* $Y_1, Y_2, \ldots, Y_n$ denote the response values for the $n$ observations.
* $x_{i,j}$ denotes the observed value of predictor $j$ for observation $i$.
  * We use lowercase $x$ to indicate that this is the observed value of the predictor.
* $\epsilon_1, \epsilon_2, \ldots, \epsilon_n$ denote the errors for the $n$ observations.

The linear model relating the responses, the predictors, and the errors is defined by the system of equations
\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
(\#eq:lmSystem)
\end{equation}

Based on our previous work, we can also write Equation \@ref(eq:lmSystem) as
\begin{equation}
Y_i = E(Y_i | X_1 = x_{i,1}, \ldots, X_{p-1} = x_{i,p-1}) + \epsilon_i,\quad i=1,2,\ldots,n.
\end{equation}

### Using matrix notation
The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices. We define the following notation:

* $\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]^T$ denotes the column vector containing the $n$ responses.
* $\mathbf{X}$ denotes the matrix containing a column of 1s and the observed predictor values, specifically, $$\mathbf{X} = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p-1} \\
1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p-1} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p-1}
\end{bmatrix}.$$
* $\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]^T$ denotes the column vector containing the $p$ regression coefficients.
* $\boldsymbol{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]^T$ denotes the column vector contained the $n$ errors.
Then the system of equations defining the linear model in \@ref(eq:lmSystem) can be written as
$$\mathbf{y} = \mathbf{X}\mathbf{\beta} + \boldsymbol{\epsilon}.$$
Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model.

## Summarizing the components of a linear model

We have already introduced a lot of objects. To aid in making sense of their notation, their purpose in the model, whether they can be observed, and whether they are modeled as a random variable (vector) or fixed, non-random values, we summarize things below.

We've already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable.

On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation  \@ref(eq:lmSystem) is for the errors to be random.

We summarize this information in the table below for the objects previously discussed using the various notations introduced.

Notation | Description | Observable | Random
--- | --- | --- |---
$Y$ | response variable | Yes | Yes
$Y_i$ | response value for the $i$th observation | Yes | Yes
$\mathbf{y}$ | the $n\times 1$ column vector of response values | Yes | Yes
$X$ | predictor variable | Yes | No
$X_j$ | the $j$th predictor variable | Yes | No
$x_{i,j}$ | the value of the $j$th predictor variable for the $i$th observation | Yes | No
$\mathbf{X}$ | the $n\times p$ matrix of predictor values | Yes | No
$\beta_j$ | the regression coefficient associated with the $j$th predictor variable | No | No
$\boldsymbol{\beta}$ | the $p\times 1$ column vector of regression coefficients | No | No
$\epsilon$ | the error | No | Yes
$\epsilon_i$ | the error associated with observation $i$ | No | Yes
$\boldsymbol{\epsilon}$ | the $n\times 1$ column vector of errors | No | Yes

## Types of regression models

The are many "named" types of regression models. You may hear or see people use these terms when describing their model. Here is a brief overview of some common regression models.

Name | Defining characteristics
--- | ---
Simple | an intercept term and one predictor variable
Multiple | more than one predictor variable
Multivariate | more than one response variable
Linear | the regression coefficients enter the model linearly
Analysis of variance (ANOVA) | predictors are all categorical
Analysis of covariance (ANCOVA) | at least one quantitative predictor and at least one categorical predictor
Generalized linear model (GLM) | a type of "generalized" regression model when the responses do not come from a normal distribution.

## Standard linear model assumptions and implications

The formulation of a linear model typically makes additional assumptions beyond the
ones previously mentioned, specifically, about the errors, $\epsilon_1,\epsilon_2,\ldots,\epsilon_n$.

We have already mentioned that fact that we are assuming $E(\epsilon_i)=0$ for $i=1,2,\ldots,n$.

We also typically assume that the errors have constant variances, i.e., $$var(\epsilon_i) = \sigma^2, \quad i=1,2,\ldots,n,$$
and that the errors are uncorrelated, i.e., $$cov(\epsilon_i, \epsilon_j) = 0, \quad i,j=1,2,\ldots,n,\quad i\neq j.$$

Additionally, we assume that the errors are identically distributed. Formally, that may be written as
\begin{equation}
\epsilon_i \sim F, i=1,2,\ldots,n,
(\#eq:errordist)
\end{equation}
where $F$ is some arbitrary distribution. The $\sim$ means "distributed as". In other words, Equation \@ref(eq:errordist) means, "$\epsilon_i$ is distributed as $F$ for $i$ equal to $1,2,\ldots,n$". However, it is more common to assume the errors have a normal (Gaussian) distribution. Two uncorrelated normal random variables are also independent (this is true for normal random variables, but is not generally true for other distributions). Thus, we may concisely state the typical error assumptions as
$$\epsilon_1,\epsilon_2,\ldots,\epsilon_n \stackrel{i.i.d.}{\sim} N(0, \sigma^2),$$
which combines the following assumptions:

1. $E(\epsilon_i)=0$ for $i=1,2,\ldots,n$.
1. $var(\epsilon_i)=\sigma^2$ for $i=1,2,\ldots,n$.
1. $cov(\epsilon_i,\epsilon_j)=0$ for $i\neq j$ with $i,j=1,2,\ldots,n$.
1. $\epsilon_i$ has a normal distribution for $i=1,2,\ldots,n$.

Using the notation previously developed, the assumptions above may be stated in vector notation as
\begin{equation}
\boldsymbol{\epsilon} \sim N(\mathbf{0}_{n\times 1}, \sigma^2 I_n), (\#eq:error-assumptions)
\end{equation}
where $\mathbf{0}_{n\times 1}$ is the $n \times 1$ vector of zeros and $I_n$ is the $n\times n$ identity matrix.

Writing the error assumptions in the form of Equation \@ref(eq:error-assumptions) allows us to see other important properties of a linear model. Specifically, if we make the assumptions in Equation \@ref(eq:error-assumptions), then

\begin{equation}
\mathbf{y} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2 I_n).
(\#eq:dist-properties-y)
\end{equation}

How do we know that Equation \@ref(eq:dist-properties-y) is true? Show that:

* $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$ and
* $\mathrm{var}(\mathbf{y}) = \sigma^2 I_n$.
* Note $\mathbf{y}$ is a linear function of the multivariate normal vector $\boldsymbol{\epsilon}$, so $\mathbf{y}$ must also have a multivariate normal distribution.

## Mathematical interpretation of coefficients

The regression coefficients have simple mathematical interpretations in basic settings.

### Coefficient interpretation in simple linear regression
Suppose we have a simple linear regression model, so that $E(Y|X)=\beta_0 + \beta_1 X.$ The interpretations of the coefficients are:

* $\beta_0$ is the expected response when the predictor is 0, i.e., $\beta_0=E(Y|X=0)$.
* $\beta_1$ is the expected change in the response when the predictor increases 1 unit, i.e., $\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)$.

### Coefficient interpretation in multiple linear regression
Suppose we have a multiple linear regression model, so that $E(Y|X_1,\ldots,X_{p-1})=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1}.$ Let $\mathbb{X} = \{X_1,\ldots,X_{p-1}\}$ be the set of predictors and $\mathbb{X}_{-j} = \mathbb{X}\setminus\{X_j\}$, i.e., the set of predictors without $X_j$.

The interpretations of the coefficients are:

* $\beta_0$ is the expected response when all predictors are 0, i.e., $\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)$.
* $\beta_j$ is the expected change in the response when predictor $j$ increases 1 unit and the other predictors stay the same, i.e., $\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})$ where $\mathbf{x}^*\in \mathbb{R}^{p-2}$ is a fixed vector of length $p-2$ (the number of predictors excluding $X_j$).

## Exercises

1. If given a set of data with several variables, how would you decide what the response variable and the predictor variables would be?
1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are considered random? Which are considered fixed?
1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are observable? Which are not observable?
1. What are the typical goals of a regression analysis?
1. List the typical assumptions made for the errors in a linear model.
1. Without using a formula, what is the basic difference between a linear model and a non-linear model?
1. Assuming that $\boldsymbol{\epsilon} ~ N(\mathbf{0}_{n\times 1}, \sigma^2 I_n)$ and $\mathbf{y}  = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, show that:
    a. $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$.
    a. $\mathrm{var}(\mathbf{y}) = \sigma^2 I_n$.
1. In the context of simple linear regression under the standard assumptions, show that:
    a. $\beta_0=E(Y|X=0)$.
    a. $\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)$.
1. In the context of multiple linear regression under the standard assumptions, show that:
    a. $\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)$.
    b. For $j=1,2,\ldots,p-1$, $\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})$ where $\mathbf{x}^*$ is a fixed vector of the appropriate size.

<!-- ## Regression for Pearson's height data -->

<!-- @wachsmuth_et_al2003 compiled child and parent height data from English familes tabulated by @pearson1897 and @pearson_lee1903. The data are available in the `PearsonLee` data set in the **HistData** package [@R-HistData]. The `PearsonLee` data frame includes the variables: -->

<!-- * `child`: child height (inches). -->
<!-- * `parent`: parent height (inches). -->
<!-- * `gp`: a factor with levels `fd` (father/daughter), `fs` (father/son), `md` (mother/daughter), `ms` (mother/son) indicating the parent/child relationship. -->
<!-- * `par`: a factor with levels `Father`, `Mother` indicating the parent measured. -->
<!-- * `chl`: a factor with levels `Daughter`, `Son` indicating the child's relationship to the parent. -->

<!-- It is natural to wonder whether the height of a parent could explain the height of their child. We can consider a regression analysis that regresses child's height (the response variable) on parent's height (the predictor variable). The additional variables `gp`, `par`, and `chl` could also be used as predictor variables in our analysis. We perform an informal (linear) regression analysis visually  using **ggplot2** [@R-ggplot2].  -->

<!-- Consider a plot of child's height versus parent's height. -->

<!-- ```{r} -->
<!-- data(PearsonLee, package = "HistData") # load data -->
<!-- library(ggplot2) # load ggplot2 package -->
<!-- # create ggplot object for repeated use -->
<!-- # we'll be using common aesthetics across multiple geometries -->
<!-- # so we put them in the ggplot function -->
<!-- # also improve the x, y labels -->
<!-- ggheight <- ggplot(data = PearsonLee,  -->
<!--                     mapping = aes(x = parent, y = child)) +  -->
<!--              xlab("parent height (in)") + ylab("child height (in)") -->
<!-- ggheight + geom_point() # scatter plot of child vs parent height -->
<!-- ``` -->

<!-- We see a positive linear association between parent height and child height: as the height of the parent increases, the height of the child also tends to increase. -->

<!-- A simple linear regression model describes the relationship between a response and a predictor variable using the "best fitting" straight line (we'll formalize what best means later). We add the estimated simple linear regression model to our previous plot below using the `geom_smooth` function. The line fits reasonably well. -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add estimated line -->
<!-- ``` -->

<!-- We may also wonder whether the type of parent (father/mother) or child (daughter/son) affects the relationship. We facet our scatter plots based on the `par` and `chl` variables below. While the overall patterns are similar, we notice that Father heights tend to be larger than Mother heights and Son heights tend to be larger than Daughter heights.  -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   facet_grid(par ~ chl) # facet the data by parent/child type -->
<!-- ``` -->

<!-- Having seen the previous graphic, we may wonder whether we can better model the relationship between parent and child height by accounting for which parent and child were measured. An interaction model assumes that the intercept and slope of each combination of parent/child is the different. We fit and plot an interaction model below. -->

<!-- ```{r} -->
<!-- ggheight + geom_point() + facet_grid(par ~ chl) +   -->
<!--        geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add interaction  model to data -->
<!-- ``` -->

<!-- Other questions we could explore are whether the slopes across the different parent/child combinations are the same, whether the variability of the data is constant as parent height changes, predicting heights outside the range of the observed data, the precision of our estimated model, etc. -->

<!-- Regression analysis will generally be much more complex that was is presented above, but this example hopefully gives you an idea of the kinds of questions regression analysis can help you answer.  -->


<!-- ## Scatter plots and linear regression -->

  <!-- Scatter plots are a convenient way to study the potential relationship between a single response and a single predictor variable. -->

  <!-- ### Height inheritability -->

  <!-- Karl Pearson (1857-1936) organized the collection of $n=1375$ heights of mothers in the United Kingdom under the age of 65 and one of their adult daughters over the age of 18.  These data are available in the `Heights` data set in the **alr4** package. We are interested in the inheritance from the mother to the daughter, so the mother's height (`mheight`) is used as the predictor variable and the daughter's height (`dheight`) is used as the response variable. -->

  <!-- Questions of interest: -->

  <!-- * Do taller mothers tend to have taller daughters -->
  <!-- * Do shorter mothers tend to have shorter daughters? -->

  <!-- ```{r} -->
  <!-- data(Heights, package = "alr4") -->
  <!-- str(Heights) -->
  <!-- plot(dheight ~ mheight, data = Heights, -->
              <!--      xlab = "mother's height (in)", -->
              <!--      ylab = "daughter's height (in)", -->
              <!--      xlim = c(55, 75), ylim = c(55, 75)) -->
  <!-- ``` -->
  <!-- There seems to be a clear trend between mother's heights and daughter's heights. The taller the mother, the taller the daughter *tends* to be. -->

  <!-- ### Predicting snowfall -->

  <!-- The `ftcollinssnow` data set in the **alr4** package measures late (September 1st until December 31st) and early (January 1st to June 30th) season snowfall for Fort Collins, CO between Late 1900 and Early 1993. -->

  <!-- Question of interest: Can late season snowfall predict snowfall in the early part of the next year? -->


  <!-- ```{r} -->
  <!-- data("ftcollinssnow", package = "alr4") # load data -->
<!-- str(ftcollinssnow) # examine structure -->
<!-- plot(Late ~ Early, data = ftcollinssnow) # plot data -->
<!-- # add "line of best fit" -->
  <!-- abline(lm(Late ~ Early, data = ftcollinssnow), lty = 2) -->
  <!-- # sample mean line -->
  <!-- abline(mean(ftcollinssnow$Late), 0) -->
  <!-- ``` -->

  <!-- A plot of the snowfall data for the two time periods suggest that this relationship is weak or they may be uncorrelated. -->
  <!-- * The dashed line indicates the "linear of best fit" , while the solid line indicates the average of the Late snowfall. -->

  <!-- ### Turkey growth -->
  <!-- Pens of turkeys were fed the same diet, except that each pen was supplemented with a `Dose` of amino acid methionine as a percentage of the total diet of the birds. The amino acid methionine was provided using three different `Source`s (one standard and two experimental). The `Weight` gain (g) of the turkeys was measured. These data are available in the `turkey` data in the **alr4** package. -->

  <!-- Questions of interest: -->

  <!-- * Is there a relationship between weight gain of the turkeys and the dose amount? If so, is the relationship linear? -->
  <!-- * Does the source of the methionine impact the weight gain of the turkeys? -->

  <!-- Consider a plot of the average `Weight` gain (g) of the turkeys as a function of the `Dose` amount (% of diet), separating the groups by the `Source` of the methionine. -->

  <!-- ```{r} -->
  <!-- data(turkey, package = "alr4") -->
  <!-- str(turkey) -->
  <!-- summary(turkey) # the source factor (S) is not a factor -->
<!-- turkey$S = factor(turkey$S) -->
  <!-- levels(turkey$S) <- c("control", "new source a", "new source b") -->
  <!-- names(turkey) <- c("Dose", "Gain", "Source", "Replications", "SD") # rename variables -->
<!-- # create turkey data ggplot -->
  <!-- library(ggplot2) # load ggplot2 package -->
<!-- gg_turkey <- ggplot(turkey, -->
                           <!--                     mapping = aes(x = Dose, y = Gain, -->
                                                                    <!--                                   color = Source, shape = Source)) -->
  <!-- gg_turkey + geom_point() + geom_line() -->
  <!-- ``` -->

  <!-- Weight gain increases with dose amount, but doesn't appear to be linear. -->

<!-- The amino acid source may slightly affect the growth trajectory of the turkeys. -->

<!-- An alternative version of the previous plot using the **lattice** package -->

<!-- ```{r} -->
<!-- library(lattice) # load lattice package -->
<!-- xyplot(Gain ~ Dose, data = turkey, groups = Source, -->
<!--        auto.key = TRUE, type = "b") -->
<!-- ``` -->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## Parameter estimation for linear models

In this chapter we focus on estimating the parameters of a linear
regression model. We also discuss important properties of the parameter
estimators.

To make the discussion easier to follow, we start by describing the
*simple* linear regression model, which is a linear model with only a
single predictor.

<!-- Fitting a regression model is the same thing as estimating the parameters of a simple linear regression model.  -->

There are many different methods of parameter estimation in statistics:
method-of-moments, maximum likelihood, Bayesian, etc. The most common
parameter estimation method for linear models is **least squares
method**, which is perhaps comonly called **Ordinary Least Squares
(OLS)** estimation. OLS estimation estimates the regression coefficients
with the values that minimize the residuals sum of squares (RSS), which
we will define shortly.

## OLS estimation of the simple linear regression model

In a simple linear regression context, we have $n$ observed responses
$Y_1,Y_2,\ldots,Y_n$ and $n$ predictor values $x_1,x_2,\ldots,x_n$.

Recall that for a simple linear regression model
$$Y = \beta_0 + \beta_1 X + \epsilon = E(Y|X) + \epsilon$$ with
$$E(Y|X) = \beta_0 + \beta_1 X.$$ We need to define some new notation
and objects to define the RSS.

Let $\hat{\beta}_j$ denote the estimated value of $\beta_j$ and the
estimated mean response as a function of the predictor $X$ is
$$\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X.$$

The $i$th fitted value is defined as
$$\hat{Y}_i = \hat{E}(Y|X = x_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i.$$
Thus, the $i$th fitted value is the estimated mean of $Y$ when the
predictor $X=x_i$. More specifically, the $i$th fitted value is the
estimated mean response of the $i$th observation.

The $i$th residual is defined as $$\hat{\epsilon}_i = Y_i - \hat{Y}_i.$$
The $i$th residual is the difference between the response and estimated
mean response of observation $i$.

**The RSS of a regression model is the sum of its squared residuals**.

The RSS for a simple linear regression model, as a function of the
estimated regression coefficients, is \begin{align*}
RSS(\hat{\beta}_0, \hat{\beta}_1) &= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 \\
&= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
 &= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{align*}

The **fitted model** is the estimated model that minimizes the RSS. In a
simple linear regression context, the fitted model is known as the
**line of best fit**.

In Figure \@ref(fig:rss-viz), we attempt to visualize the response
values, fitted values, residuals, and line of best fit in a simple
linear regression context. Notice that:

-   The fitted values are the value returned by the line of best fit
    when it is evaluated at the observed predictor values.
    Alternatively, the fitted value for each observation is the y-value
    obtained when intersecting the line of best fit with a vertical line
    drawn from each observed predictor value.
-   The residual is the vertical distance between each response value
    and the fitted value.
-   The RSS seeks to minimize the sum of the squared vertical distances
    between the response and fitted values.

```{r rss-viz, fig.cap = "Visualization of the response values, fitted values, residuals, and line of best fit.", echo=FALSE}
set.seed(2)
x <- c(3, 5, 7, 8, 9)
y <- 2 + 2 * x + rnorm(5, sd = 4)
plot(y ~ x, pch = 19)
lmod <- lm(y ~ x)
yhat <- fitted(lmod)
abline(lmod, col = "grey")
points(yhat ~ x, pch = 4, col = "blue")
segments(x, y, x, yhat, col = "orange")
legend("topleft",
       legend = c("observed response", "fitted value", "residual", "line of best fit"),
       pch = c(19, 4, NA, NA),
       col = c("black", "blue", "orange", "grey"),
       lwd = c(NA, NA, 1, 1))
```

### Visualizing the RSS as a function of the estimated coefficients

As we have attempted to emphasize through its notation,
$RSS(\hat{\beta}_0, \hat{\beta}_1)$ is a function of $\hat{\beta}_0$ and
$\hat{\beta}_1$. OLS estimation for the simple linear regression model
seeks to find the values of the estimated coefficients that minimize the
$RSS(\hat{\beta}_0, \hat{\beta}_1)$. In the example below, we visualize
this three-dimensional surface to see how difficult it would be to
optimize the RSS computationally .

Consider the Pearson and Lee's height data (`PearsonLee` in the
**HistData** package) previously discussed. For that data set, we tried
to model the child's height (`child`) based on the height of the child's
parents (`parent`). Thus, our response variable is `child` and our
predictor variable is `parent`. We seek to estimate the regression
equation
$$E(\mathtt{child} \mid \mathtt{parent}) = \beta_0 + \beta_1 \mathtt{parent}$$
with the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the
associated RSS.

We first load the height data, extract the response and predictor and
assign them the names `y` and `x`.

```{r}
# load height data
data(PearsonLee, package = "HistData")
# extract response and predictor variables from data set
y <- PearsonLee$child
x <- PearsonLee$parent
```

We now create a function that computes the RSS as a function of
$\hat{\beta}_0$ and $\hat{\beta}_1$ (called `b0` and `b1`, respectively
in the code below). The function takes the vector `b = c(b0, b1)`,
extracts `b0` and `b1` from this vector, computes the fitted values
(`yhat`) for the provided `b0` and `b1`, computes the corresponding
residuals (`ehat`), and the returns the sum of the squared residuals,
i.e., the RSS.

```{r}
# function to compute the RSS
# b = c(b0, b1)
compute_rss <- function(b) {
  b0 = b[1] # extract b0 from b
  b1 = b[2] # extract b1 from b
  yhat <- b0 + b1 * x # compute fitted values
  ehat <- y - yhat # compute residuals
  return(sum(ehat^2)) # return RSS
}
```

Next, we specify sequences of `b0` and `b1` values to consider for
optimizing the RSS. We create a matrix, `rss_mat` to store the computed
RSS for each combination of `b0` and `b1`. We then use a double `for`
loop to evaluate the RSS for each combination of `b0` and `b1` in our
sequences.

```{r}
# sequences of candidate b0 and b1 values
b0_seq <- seq(41.06, 41.08, len = 101)
b1_seq <- seq(0.383, 0.385, len = 101)
# matrix to store rss values
rss_mat <- matrix(nrow = length(b0_seq), ncol = length(b1_seq))
# use double loop to compute RSS for all combinations of b0_seq and b1_seq
# seq_along(b0_seq) returns the vector 1:length(b0_seq), but is safer 
for (i in seq_along(b0_seq)) {
  for (j in seq_along(b1_seq)) {
    rss_mat[i, j] <- compute_rss(c(b0_seq[i], b1_seq[j]))
  }
}
```

We draw a contour plot of the RSS surface using the `contour` function.

```{r}
# draw a contour plot of the RSS surface
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = "b0", ylab = "b1")
title("RSS surface of Pearson and Lee height data")
```

A contour plot uses contour lines to describe the height of the $z$
dimension of a 3-dimensional $(x, y, z)$ surface. Each line/contour
indicates the height of the surface along that line. Note that in the
graphic above, the contours are basically straight lines. There's no
easily identifiable combinations of `b0` and `b1` the produce the
minimum RSS.

We can approximate the optimal values of `b0` and `b1` that minimize the
RSS through the `optim` function. The optim function takes two main
arguments:

-   `par`: a vector of starting values for the optimization algorithm.
    In our case, this will be the starting values for `b0` and `b1`.
-   `fn`: a function of `par` to minimize.

The `optim` function will return a list with several pieces of
information (see `?stats::optim`) for details. We want the `par`
component of the returned list, which is the `par` vector that
(approximately) minimizes `fn`. We then use the `points` function to
plot the "optimal" values of `b0` and `b1` that minimize the RSS.

```{r}
# use the optim function to find the values of b0 and b1 that minimize the RSS
# par is the vector of initial values
# fn is the function to minimize
# $par extracts the values found by optim to minimize fn
optimal_b <- optim(par = c(41, 0.4), fn = compute_rss)$par
# print the optimal values of b
optimal_b
# plot optimal value as an X on the contour plot
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = "b0", ylab = "b1")
title("RSS surface of Pearson and Lee height data")
points(x = optimal_b[1], y = optimal_b[2], pch = 4)
```

What is our takeaway from this example? It's probably not ideal to
numerically search for the values of $\hat{\beta}_0$ and $\hat{\beta}_1$
that minimize $RSS(\hat{\beta}_0$, $\hat{\beta}_1)$. Instead, we should
seek an exact solution using mathematics.

### OLS estimators of the simple linear regression parameters

Define $\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i$ and
$\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$.

The OLS estimators of the regression coefficients for a simple linear
regression coefficients are

\begin{align*}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n x_i Y_i - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)\biggl(\sum_{i=1}^n Y_i\biggr)}{\sum_{i=1}^n x_i^2 - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&= \frac{\sum_{i=1}^n (x_i - \bar{x})Y_i}{\sum_{i=1}^n (x_i - \bar{x})x_i}
\end{align*} and \begin{equation}
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}.
\end{equation}

Thought it's already been said, we state once again that the OLS
estimators of $\beta_0$ and $\beta_1$ shown above are the estimators
that minimize the RSS.

The other parameter we've discussed is the error variance, $\sigma^2$.
The most common estimator of the error variance is \begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-p}, (\#eq:sigmasq-hat)
\end{equation} where $p$ is the number of regression coefficients. In
general, $n-p$ is the degrees of freedom of the RSS. In a simple linear
regression context, the denominator of \@ref(eq:sigmasq-hat) is $n-2$.

## Penguins simple linear regression example

We will use the `penguins` data set in the **palmerpenguins** package
[@R-palmerpenguins] to illustrate a very basic simple linear regression
analysis.

The `penguins` data set provides data related to various penguin species
measured in the Palmer Archipelago (Antarctica), originally provided by
@GormanEtAl2014. We start by loading the data into memory.

```{r}
data(penguins, package = "palmerpenguins")
```

The data set includes `r nrow(penguins)` observations of
`r ncol(penguins)` variables. The variables are:

-   `species`: a `factor` indicating the penguin species
-   `island`: a `factor` indicating the island the penguin was observed
-   `bill_length_mm`: a `numeric` variable indicating the bill length in
    millimeters
-   `bill_depth_mm`: a `numeric` variable indicating the bill depth in
    millimeters
-   `flipper_length_mm`: an `integer` variable indicating the flipper
    length in millimeters
-   `body_mass_g`: an `integer` variable indicating the body mass in
    grams
-   `sex`: a `factor` indicating the penguin sex (`female`, `male`)
-   `year`: an integer denoting the study year the penguin was observed
    (`2007`, `2008`, or `2009`)

We begin by creating a scatter plot of `bill_length_mm` versus
`body_mass_g` (y-axis versus x-axis) in Figure \@ref(fig:penguin-plot).
We see a clear positive association between body mass and bill length:
as the body mass increases, the bill length tends to increase. The
pattern is linear, i.e., roughly a straight line.

```{r penguin-plot, fig.cap = "A scatter plot of penguin bill length (mm) versus body mass (g)"}
plot(bill_length_mm ~ body_mass_g, data = penguins,
     ylab = "bill length (mm)", xlab = "body mass (g)",
     main = "Penguin size measurements")
```

We first perform a single linear regression analysis manually using the
equations previously provided by regressing `bill_length_mm` on
`body_mass_g`.

Using the `summary` function on the `penguins` data frame, we see that
both `bill_length_mm` and `body_mass_g` have `NA` values.

```{r}
summary(penguins)
```

We want to remove the rows of `penguins` where either `body_mass_g` or
`bill_length_mm` have `NA` values. We do that below using the `na.omit`
function (selecting only the relevant variables) and assign the cleaned
object the name `penguins_clean`.

```{r}
# remove rows of penguins where bill_length_mm or body_mass_g have NA values 
penguins_clean <- na.omit(penguins[,c("bill_length_mm", "body_mass_g")])
```

We extract the `bill_length_mm` variable from the `penguins` data frame
and assign it the name `y` since it will be the response variable. We
extract the `body_mass_g` variable from the `penguins` data frame and
assign it the name `y` since it will be the predictor variable. We also
determine the number of observations and assign that value the name `n`.

```{r}
# extract response and predictor from penguins_clean
y <- penguins_clean$bill_length_mm
x <- penguins_clean$body_mass_g
# determine number of observations
n <- length(y)
```

We now compute $\hat{\beta}_1$ and $\hat{\beta}_0$. Note that placing
`()` around the assignment operations will both perform the assign and
print the results.

```{r}
# compute OLS estimates of beta1 and beta0
(b1 <- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n))
(b0 <- mean(y) - b1 * mean(x))        
```

The estimated value of $\beta_0$ is $\hat{\beta}_0=26.90$ and the
estimated value of $\beta_1$ is $\hat{\beta}_1=0.004$. The basic
mathematical interpretation of our results is that:

-   ($\hat{\beta}_1$): If a penguin has a body mass 1 gram larger than
    another penguin, we expect the larger penguins bill length to be
    0.004 millimeters longer.
-   ($\hat{\beta}_0$):A penguin with a body mass of 0 grams is expected
    to have a bill length of 26.9 millimeters.

The latter interpretation is clearly non-sensical and is caused by the
fact that we are extrapolating far outside the observed body mass
values. The relationship between body mass and bill length is different
for penguin chicks versus adults.

We can use the `abline` function to overlay the fitted model on the
observed data. Note that in simple linear regression, $\hat{\beta}_1$
corresponds to the slope of the fitted line and $\hat{\beta}_0$ will be
the intercept.

```{r}
plot(bill_length_mm ~ body_mass_g, data = penguins,
     ylab = "bill length (mm)", xlab = "body mass (g)",
     main = "Penguin size measurements")
# a is the intercept and b is the slope
abline(a = b0, b = b1)
```

The fit of the model to our observed data seems reasonable.

We can also compute the residuals,
$\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n$, the fitted values
$\hat{y}_1,\ldots,\hat{y}_n$, and the associated RSS,
$RSS=\sum_{i=1}^n \hat{\epsilon}_i^2$.

```{r}
yhat <- b0 + b1 * x # compute fitted values
ehat <- y - yhat # compute residuals
(rss <- sum(ehat^2)) # sum of the squared residuals
(sigmasqhat <- rss/(n-2)) # estimated error variance
```

## Fitting a linear model using R

We now describe how to use R to fit a linear model to data.

The `lm` function uses OLS to fit a linear model to data. The function
has two major arguments:

-   `data`: the data frame in which the model variables are stored. This
    can be omitted if the variables are already stored in memory.

-   `formula`: a @wilkinsonrogers1973 style formula describing the
    linear regression model. Assuming the `y` is the response, `x`,
    `x1`, `x2`, `x3` are available numeric predictors:

    -   `y ~ x` describes a simple linear regression model based on
        $E(Y|X)=\beta_0+\beta_1 X$.
    -   `y ~ x1 + x2` describes a multiple linear regression model based
        on $E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2$.
    -   `y ~ x1 + x2 + x1:x2` and `y ~ x1 * x2` describe a multiple
        linear regression model based on
        $E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2$.
    -   `y ~ -1 + x1 + x2` describes a multiple linear regression model
        without an intercept, in this case,
        $E(Y|X_1, X_2)=\beta_1 X_1 + \beta_2 X_2$.
    -   `y ~ x + I(x^2)` describe a multiple linear regression model
        based on $E(Y|X)=\beta_0+\beta_1 X + \beta_2 X^2$.

We fit a linear model regressing `body_mass_g` on `bill_length_mm` using
the `penguins` data frame and store it in the object `lmod`. `lmod` is
an object of class `lm`.

```{r}
lmod <- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model
class(lmod) # class of lmod
```

There are a number of methods (generic function that do something
specific when applied to a certain type of object). Commonly used ones
include:

-   `residuals`: extracts $\hat{\boldsymbol{\epsilon}}$ from an `lm`
    object.

-   `fitted`: extracts $\hat{\mathbf{y}}$ from an `lm` object.

-   `coef` or `coefficients`: extracts $\hat{\boldsymbol{\beta}}$ from
    an `lm` object.

-   `deviance`: extracts the RSS from an `lm` object.

-   `sigma`: extracts $\hat{\sigma}$ from an `lm` object.

-   `df.residual`: extracts $n-p$, the degrees of freedom for the RSS,
    from an `lm` object.

-   `summary`: provides:

    -   A 5-number summary of the $\hat{\boldsymbol{\epsilon}}$
    -   A table that lists the predictors, the `Estimate` of the
        associated coefficients, the **estimated** standard error of the
        estimates (`Std.Error`), the computed test statistic associated
        with testing $H_0: \beta_j = 0$ versus $H_a: \beta_j \neq 0$ for
        $j=0,1,\ldots,p-1$ (`t value`), and the associated p-value of
        each test `Pr(>|t|)`.

We now use some of the methods to extract important characteristics of
our fitted model. We then check whether the values obtained from these
methods match our manual calculations.

```{r}
(coeffs2 <- coefficients(lmod)) # extract, assign, and print coefficients
ehat2 <- residuals(lmod) # extract and assign residuals
yhat2 <- fitted(lmod) # extract and assign fitted values
rss2 <- deviance(lmod) # extract and assign rss
sigmasqhat2 <- rss2/df.residual(lmod) # estimated error variance
# compare to manually computed values
all.equal(c(b0, b1), coeffs2, check.attributes = FALSE)
all.equal(ehat, ehat2, check.attributes = FALSE)
all.equal(rss, rss2)
all.equal(sigmasqhat, sigmasqhat2)
# methods(class="lm")
```

### Derivation of OLS simple linear regression estimators

Use calculus to derive the OLS estimator of the regression coefficients.
Take the partial derivatives of $RSS(\hat{\beta}_0, \hat{\beta}_1)$ with
respect to $\hat{\beta}_0$ and $\hat{\beta}_1$, set the derivatives
equal to zero, and solve for $\hat{\beta}_0$ and $\hat{\beta}_1$ to find
the critical points of $RSS(\hat{\beta}_0, \hat{\beta}_1)$. Technically,
you must show that the Hessian matrix of
$RSS(\hat{\beta}_0, \hat{\beta}_1)$ is positive definite to verify that
our solution minimizes the RSS, but we won't do that here.

$$\\[4in]$$
