---
title: "Unusual Observations Part 1"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
---

```{r, include=FALSE}
library(faraway)
library(car)
```

# Introduction

An implicit assumption made when fitting a regression model is that **all observations should be equally reliable and have approximately equal role** in determining the regression results and in influencing conclusions.

- A **leverage point** is an observation that is unusual in the predictor space.
- An **outlier** is an observation whose response does not match the pattern of the fitted model.
- An **influential observation** is one that causes a substantial change in the fitted model based on its inclusion or deletion from the model.
- An influential observation is usually either a leverage point, an outlier, or a combination of the two.  

# Identifying Leverage Points

The leverage values are the diagonal elements of the hat matrix $H=X(X^T X)^{-1} X^T$. 

The i^th^ leverage value is given by $h_i=H_{ii}$, the i^th^ diagonal position of the hat matrix.


## Computing the Hat Matrix and Leverage Values for Savings Model

```{r}
data(savings, package = "faraway") # load data
lmod <- lm(sr ~ ., data = savings) #fit full model
#sumary(lmod)
```

Here we construct the hat matrix $H=X(X^T X)^{-1} X^T$

```{r}
X <- model.matrix(lmod)
HatMat <- X %*% solve(t(X) %*% X) %*% t(X)
head(HatMat[1:5,1:5])
```

There is a built-in function `hatvalues` which does this as well.

```{r}
h <- hatvalues(lmod)
h[1:5]
```

## Half-Normal Plots

A **half-normal plot** of the leverage values can be used to identify observations with unusually high leverage.

- A half-normal plot compares the sorted data against the positive normal quantiles.

The steps are to producing a half-normal plot for $x_1, \ldots , x_n$ are:

1. Sort the data:  $x_{\lbrack 1 \rbrack} \leq x_{\lbrack 2 \rbrack} \leq \ldots \leq x_{\lbrack n \rbrack}$.
2. Compute $u_i=\phi^{-1} \left(\frac{n+i}{2n+1} \right)$.
3. Plot $x_{\lbrack i \rbrack}$  versus $u_i$.

### Creating a Half-Normal Plot

## Question 1: Complete the code cell below to create a half-normal plot

```{r, eval = FALSE}
n <- ?? # number of obs
h.sort <- ?? # sort hatvalues
u <- numeric(n)   # vector where we'll store half normal quantiles
# compute half normal quantiles
for (i in 1:n){
  u[i] = ??
}
plot(h.sort ~ u)  # half-normal plot
```

The leverage points are the points in the plot that diverge substantially from the rest of the data.

- If the half-normal plot is approximately a straight line of points, then there are no leverage points.
- **If the half-normal plot looks like a hockey stick, then the points on the blade are leverage points.**

The `faraway::halfnorm` function can be used to generate a half-normal plot.

```{r}
# get country name for each observation
# useful for labeling leverage points
countries <- row.names(savings)
halfnorm(h, nlab = 2, labs = countries, ylab = "leverage")
```

### Example With No Leverage Points

```{r}
data(state)
statedata <- data.frame(state.x77, row.names = state.abb)
names.state <- row.names(statedata)
lmod2 <- lm(Employed ~ ., data = longley)
h2 <- hatvalues(lmod2)
halfnorm(h2, nlab = 2, labs = names.state, ylab = "leverage")
```

## Index Plots

An index plot of the leverage values can also be used to identify leverage points.  

- An index plot plots the statistic of an observation versus its observation number.
- You want to focus on observations where the statistics are large or small relative to the other values.

The `car::infIndexPlot` function can be used to generate index plots related to many influence-related statistics (such as leverage value).

```{r}
infIndexPlot(lmod, vars = "hat")
```


# Identifying outliers

An **outlier** is a point that does not fit the current model.

- An outlier is context specific!  An outlier for one model may not be an outlier for a different model.

## Examples of Outliers 

## Question 2: Each of the three plots below have added a different outlier to the same original model. Which outliers seem to have large leverage? Which outliers have large influence? Explain how you determined your answers.

```{r}
set.seed(123)
testdata <- data.frame(x = 1:10, y = 1:10 + rnorm(10))
testmod <- lm(y ~ x, data = testdata)
p1 <- c(5.5, 12)
testmod1 <- lm(y ~ x, data = rbind(testdata, p1))
plot(y ~ x, data = rbind(testdata, p1))
points(5.5, 12, pch = 4, cex = 2, col = "red")
abline(testmod)
abline(testmod1, lty = 2)
```


```{r}
p2 <- c(15, 15.1)
testmod2 <- lm(y ~ x, data = rbind(testdata, p2))
plot(y ~ x, data = rbind(testdata, p2))
points(15, 15.1, pch = 4, cex = 2, col = "red")
abline(testmod)
abline(testmod2, lty = 2)
```

```{r}
p3 <- c(15, 5.1)
testmod3 <- lm(y ~ x, data = rbind(testdata, p3))
plot(y ~ x, data = rbind(testdata, p3))
points(15, 5.1, pch = 4, cex = 2, col = "red")
abline(testmod)
abline(testmod3, lty = 2)
```

## Leave-One-Out Statistics

Leave-one-out statistics are statistics computed from the model fitted without the i^th^ observation.

- $\widehat{\boldsymbol\beta}_{(i)}$ is the vector of leave-one-out estimated coefficients.
- $\widehat{\sigma}_{(i)}$ is the leave-one-out estimate of the error standard deviation.  
- $\widehat{y}_{(i)}$ is the leave-one-out fitted value for the i^th^ observation.
- The subscript $(i)$ means that these statistics were estimated for the model fitted without the i^th^ observation.


```{r}
# Zambia is observation 46
newdata <- savings[-46,]
newmod <- lm(sr ~ ., data = newdata)
```


If the **leave-one-out residual** (deleted residual) $y_i-\widehat{y}_{(i)}$ is large, then observation $i$ is an outlier.

```{r}
residuals(lmod)[46] # original residual
(res.zambia <- savings[46,1] - predict(newmod, new = as.data.frame(savings[46,]))) #leave-one-out residual
```

- The OLS residuals may not be suitable for identifying outliers since truly influential observations will pull the fitted model close to themselves, making the residual smaller.

## Studentized Residuals

When the model is correct and $\boldsymbol\epsilon \sim N(0, \sigma^2 I_n)$ the scaled **studentized residual** (also called jackknife or crossvalidated residual) is a better judge of the potential size of an outlier:

$$t_i= \frac{y_i-\widehat{y}_{(i)}}{\sigma_{(i)} \sqrt{1+x_i^T(X^T_{(i)}X_{(i)})^{-1}x_i}} = \frac{\widehat{\epsilon}_i}{\widehat{\sigma}_{(i)} \sqrt{1-h_i}}  \sim t_{n-p-1}$$

```{r}
sig <- summary(newmod)$sigma
residuals(lmod)[46]/(sig * sqrt( 1 - hatvalues(lmod)[46]))
```

### Bonferonni correction

We can calculate a $p$-value to assess whether observation $i$ is an outlier.
If performing multiple hypothesis tests at level $\alpha$: 

- The probability of making at least one type I error will be more than $\alpha$.
- We must adjust the level of each test so that overall (familywise) type I error rate is satisfied.  

Suppose we want a level $\alpha$ test for n tests, i.e., we want P(no type I errors in n tests)$=1-\alpha$.  


$$\begin{aligned}
\mbox{P(no type I errors in $n$ tests)} &= P \left( \cap_{i=1}^n(\mbox{no type I error in test $i$}) \right)\\
&= 1 - P \left( \cup_{i=1}^n(\mbox{there IS type I error in test $i$}) \right)\\
& \geq 1 - \sum_{i=1}^n P(\mbox{there IS type I error in test $i$})\\
&= 1 - n\alpha
\end{aligned}$$

- To get an overall level $\alpha$ test, we should use the level $\alpha/n$ in each of the individual tests.
- This approach is known as the **Bonferonni correction**, and is used in many contexts to make proper simultaneous inference (not just for outliers or regression).
- The Bonferonni correction is a very conservative method.
  - It doesn’t reject H~0~ as often as it should.
  - It gets more conservative as $n$ gets larger.

A observation is considered an outlier if $|t_i| \geq t_{n-p-1}^{\frac{\alpha}{2n}}$.

## Question 3: Why do we divide by $2n$ and not just $n$?

### Computing Studentized Residuals

```{r}
# obtain studentized residuals
stud <- rstudent(lmod)

# largest magnitude studentized residual
max(abs(stud))
```


### Computing Bonferonni correction P-value

```{r}
# since we are doing a two-sided test, we need the
# 1 - alpha/2n quantile not 1-alpha/n.
# df = 50 - 5 - 1
qt(1 - .05/(50*2), df = 44)
```

## Question 4: Based on the output above, does this model have any outliers?

### Using the outlierTest Function

```{r}
# perform outlier check using Bonferroni correction
outlierTest(lmod)
```


## Index Plots of Studentized Residuals and Bonferroni p-values

```{r}
infIndexPlot(lmod, vars = c("Studentized", "Bonf"))
```

Notes:

- Two or more outliers next to each other can “hide” each other.
- If we fit a new model, we may get different or no outliers.
- If the error distribution is nonnormal, it is very reasonable to get large residuals.
- Individual outliers are less of a problem in larger datasets because they are not likely to have a large leverage.
  - It is still good to identify the outliers.
  - They probably won’t be an issue unless they occur in clusters.

