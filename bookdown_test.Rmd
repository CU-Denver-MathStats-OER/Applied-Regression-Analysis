--- 
title: "Data Analysis with Linear Regression"
author: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "A collection of R notebooks demonstrating how to perform data analysis with linear regression."
---

# Preliminaries {-}

I recommend you execute the following commands install packages we may use in this course.

```{r, eval = FALSE}
# packages related to books
books = c("faraway", "alr4", "car", "rms")
install.packages(books)
# packages related to tidy/tidying data
tidy = c("broom", "tidyr", "dplyr")
install.packages(tidy)
# packages related to plotting
moreplots = c("ggplot2", "ggthemes", "lattice", "HH")
install.packages(moreplots)
# packages related to model diagnostics
diag = c("leaps", "lmtest", "gvlma", "caret", "perturb")
install.packages(diag)
```

*Acknowledgments*

The **bookdown** package [@R-bookdown] was used to generate this book.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document: default
  html_notebook: default
---

```{r, include=FALSE}
# change Console output behavior
knitr::opts_chunk$set(comment = "#>", collapse = TRUE)
```

# R Foundations

Meaningful data analysis requires the use of computer software. In this course, we will utilize R.

In what follows, I will attempt to lay a foundation of key components of R that you will need for data analysis. I make no attempt to be exhaustive, and many other important components of R (like plotting) will be discussed later, as needed.

## What is R?

* R is programming language and environment designed for statistical computing.
  * It was introduced by Robert Gentleman and Robert Ihaka in 1993.
  * It is modeled after the _S_ programming language.
* R is free, open source, and runs on Windows, Macs, Linux, and other types of computers.
* R is an interactive programming language
  * You type and execute a command in the Console for immediate feedback in contrast to a compiled programming language, which compiles a program that is then executed.
* R is highly extendable.
  * Many user-created packages are available to extend the functionality beyond what is installed by default.
  * Users can write their own functions and easily add software libraries to R.

## Where to get R (and R Studio Desktop)

R may be downloaded from the R Project's [website](https://www.r-project.org/). This [link](https://cloud.r-project.org/) *should* bring you to the relevant page for downloading the software.

R Studio Desktop is a free "front end" for R provided by [R Studio](https://rstudio.com/). R Studio Desktop makes doing data science with R much easier by adding an Integrated Development Environment (IDE) and providing many other features. Currently, you may download R Studio at this [link](https://rstudio.com/products/rstudio/download/). You may need to navigate the R Studio website directly if this link no longer functions.

Install R and R Studio Desktop before continuing. Then open R Studio Desktop as you continue to learn about R.

## R Studio Layout

R Studio Desktop has four panes:

1. Console: the pane where the code is executed.
2. Source: the pane where you prepare commands to be executed.
3. Environment/History: the pane where you can see all the objects in your workspace, your command history, and other things.
4. The Files/Plot/Packages/Help: the pane where you navigate between directories, where plots can be viewed, where you can see the packages available to be loaded, and where you can get help.

<!-- ![RStudio panes](pictures/rstudio_panes.png) -->
To see all R Studio, press the keys `Shift + Ctrl + Alt + 0`

## Running code, scripts, and comments
Code is executed in R by typing it in the Console and hitting enter.

Instead of typing all of your code in the Console and hitting enter, it's better to write your code in a Script and execute the code separately.

A new script can be obtained by executing File -> New File -> R Script or pressing "Ctrl + Shift + n" (on a PC) or "Cmd + Shift + n" on a Mac.

There are various ways to run code from a Script file. The most common ones are:

1. Highlight the code you want to run and hit the Run button at the top of the Script pane.
2. Highlight the code you want to run and press "Ctrl + Enter" on your keyboard. If you don't highlight anything, by default, R Studio runs the command the cursor currently lies on.

To save a script, click File -> Save or press "Ctrl + s" (on a PC) or "Cmd + s" (on a Mac).

A comment is a set of text ignored by R when submitted to the Console.

A comment is indicated by the `#` symbol. Nothing to the right of the `#` is executed in the Console.

To comment (or uncomment) multiple lines in R, highlight the code you want to comment and press "Ctrl + Shift + c" on a PC or "Cmd + Shift + c" on a Mac.

### Example {.example}
Perform the following tasks:

1. Type `1+1` in the Console and hit enter.
2. Open a new Script in R Studio.
3. `mean(1:3)` in your Script file.
4. Type `# mean(1:3)` in your Script file.
5. Run the commands from the Script using an approach mentioned above.

## Packages
Packages are collections of functions, data, and other objects that extend the functionality installed by default in R. 

R packages can be installed using the `install.packages` function and loaded using the `library` function.

### Example {.example}
Practice installing and loading a package by doing the following:

1. Install the set of **faraway** package by executing the command
`install.packages("faraway")`.
2. Load the **faraway** package by executing the command `library(faraway)`. 

## Getting help

There are a number of helps to get help in R.

If you know the command for which you want help, then exectue `?command` in the Console.
  * e.g., `?lm`
  * This also may work with data sets, package names, object classes, etc.

If you want to search the documentation for a certain *topic*, then execute `??topic` in the Console.
* If you need help deciphering an error, identifying packages to perform a certain analysis, how to do something better, then a web search is likely to help.

### Example {.example}

Do the following:
1. Execute `?lm` in the Console to get help on the `lm` function, which is one of the main functions used for fitting linear models.
2. Execute `??logarithms` in the Console to search the R documentation for information about logarithms.
3. Do a web search for something along the lines of "How do I change the size of the axis labels in an R plot?".

## Data types and structures

### Basic data types
R has 6 basic ("atomic") [vector types](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types):

1. character - collections of characters. E.g., `"a"`, "hello world!"
2. double - decimal numbers. e.g., `1.2`, `1.0`
3. integer - whole numbers. In R, you must add `L` to the end of a number to specify it as an integer. E.g., `1L` is an integer but `1` is a double.
4. logical - boolean values, `TRUE` and `FALSE`
5. complex - complex numbers. E.g., `1+3i`
6. raw - a type to hold raw bytes.

The `typeof` function returns the R internal type or storage mode of any object.

Consider the following commands and output:
```{r}
typeof(1)
typeof(1L)
typeof("hello world!")
```

### Other important object types

There are other important types of objects in R that are not basic. We will discuss a few. The [R Project manual](https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Basic-types) provides additional information about available types.

#### Numeric
An object is `numeric` if it is of type `integer` or `double`. In that case, it's `mode` is said to be `numeric`.

The `is.numeric` function tests whether an object can be interpreted as numbers. We can use it to determine whether an object is `numeric`.

Some examples:

```{r}
is.numeric("hello world!")
is.numeric(1)
is.numeric(1L)
```
#### NULL
`NULL` is a special object to indicate the object is absent. An object having a length of zero is not the same thing as an object being absent.

#### NA
A "missing value" occurs when the value of something isn't known. R uses the special object `NA` to represent missing value.

If you have a missing value, you should represent that value as `NA`. Note: `"NA"` is not the same thing as `NA`.

#### Functions
A function is an object the performs a certain action or set of actions based on objects it receives from its arguments.

### Data structures

R operates on data structures.  A data structure is simply some sort of "container" that holds certain kinds of information

R has 5 basic data structures:

* vector
* matrix
* array
* data frame
* list

Vectors, matrices, and arrays are homogeneous objects that can only store a single data type at a time.

Data frames and lists can store multiple data types.

Vectors and lists are considered one-dimensional objects. A list is technically a vector. Vectors of a single type are atomic vectors. (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#List-objects)

Matrices and data frames are considered two-dimensional objects.

Arrays can be n-dimensional objects.

This is summarized in the table below, which is based on a [table](http://adv-r.had.co.nz/Data-structures.html#data-structure) in the first edition of Hadley Wickham's *Advanced R*.

```{r, echo=FALSE}
knitr::kable(data.frame(dimensionality = c("1d", "2d", "nd"),
                        homogeneous = c("vector", "matrix", "array"),
                        heterogeneous = c("list", "data frame", "")))
```

## Assignment

To store a data structure in the computer's memory we must assign it a name.

Data structures can be stored using the assignment operator `<-` or `=`.

Some comments:
  
* In general, both `<-` and `=` can be used for assignment.
* Pressing the "Alt" and "-" keys simultaneously on a PC or Linux machine (Option and - on a Mac) will insert `<-` into the R console and script files (but not in R Markdown for some reason).
* `<-` and `=` are NOT synonyms, but can be used identically most of the time. It's safest to use `<-` for assignment.

Once an object has been assigned a name, it can be printed by executing the name of the object or using the `print` function.

### Example
In the following code, we compute the mean of a vector and print the result.
```{r}
# compute the mean of 1, 2, ..., 10 and assign the name m
m <- mean(1:10) 
m # print m
print(m) # print m a different way
```

## Vectors
A *vector* is a single-dimensional set of data of the same type.

### Creation
The most basic way to create a vector is the `c` function.

The `c` function combines values into a vector or list.

e.g., the following commands create vectors of type numeric, character, and logical, respectively.

* `c(1, 2, 5.3, 6, -2, 4)`
* `c("one", "two", "three")`
* `c(TRUE, TRUE, FALSE, TRUE)`

### Creating patterned vectors

R provides a number of functions for creating vectors following certain consistent patterns.

The `seq` (sequence) function is used to create an equidistant series of numeric values.

Some examples:
  
* `seq(1, 10)`: A sequence of numbers from 1 to 10 in increments of 1.
* `1:10`: A sequence of numbers from 1 to 10 in increments of 1.
* `seq(1, 20, by = 2)`: A sequence of numbers from 1 to 20 in increments of 2.
* `seq(10, 20, len = 100)`: A sequence of numbers from 10 to 20 of length 100.

The `rep` (replicate) function can be used to create a vector by replicating values.

Some examples:
  
  * `rep(1:3, times = 3)`: Repeat the sequence 1, 2, 3 three times in a row.
* `rep(c("trt1", "trt2", "trt3"), times = 1:3)`: Repeat "trt1" once, "trt2" twice, and "trt3" three times.
* `rep(1:3, each = 3)`: Repeat each element of the sequence 1, 2, 3 three times.

### Example {.example}
Execute the following commands above in the Console to see what you get.

```{r, eval=FALSE}
# vector creation
c(1, 2, 5.3, 6, -2, 4)
c("one", "two", "three")
c(TRUE, TRUE, FALSE, TRUE)
# sequences of values
seq(1, 10)
1:10
seq(1, 20, by = 2)
seq(10, 20, len = 100)
# replicated values
rep(1:3, times = 3)
rep(c("trt1", "trt2", "trt3"), times = 1:3)
rep(1:3, each = 3)
```

Vectors can be combined into a new object using the `c`.

### Example {.example}
Execute the following commands in the Console

```{r}
v1 <- 1:5 # create a vector
v1 # print the vector
print(v1)
v2 <- c(1, 10, 11) # create a new vector
new <- c(v1, v2) # combine and assign the combined vectors
new # print the combined vector
```

### Categorical vectors

Categorical data should be stored as a `factor` in R.

The `factor` function takes values that can be coerced to a character and converts them to an object of class `factor`.

Some examples:
```{r}
f1 <- factor(rep(1:6, times = 3))
f1
f2 <- factor(c("a", 7, "blue", "blue", FALSE))
f2
```

### Example {.example}

Create a vector named `grp` that has two levels: `a` and `b`, where the first 7 values are `a` and the second 4 values are `b`.

### Extracting parts of a vector
Subsets of the elements of a vector can be extracted by appending an index vector in square brackets `[]` to the name of the vector .

Let's create the numeric vector 2, 4, 6, 8, 10, 12, 14, 16.

```{r}
a <- seq(2, 16, by = 2)
a
``` 

Let's access the 2nd, 4th, and 6th elements of `a`.
```{r}
a[c(2, 4, 6)]
```

Let's access all elements in `a` EXCEPT the 2nd, 4th, and 6th using the minus (`-`) sign in front of the index vector.
```{r}
a[-c(2, 4, 6)]
```

Let's access all elements in `a` except elements 3 through 6.
```{r}
a[-(3:6)]
```

## Helpful functions

### General functions

Some general functions commonly used to describe data objects:

* `length(x)`: length of `x`
* `sum(x)`: sum elements in `x`
* `mean(x)`: sample mean of elements in `x`
* `var(x)`: sample variance of elements in `x`
* `sd(x)`: sample standard deviation of elements in `x`
* `range(x)`: range (minimum and maximum) of elements in `x`
* `log(x)`: (natural) logarithm of elements in `x`
* `summary(x)`: a summary of `x`. Output changes depending on the class of `x`.
* `str(x)`: provides information about the structure of `x`. Usually, the class of the object and some information about its size.

### Example {.example}

Run the following commands in the Console:

```{r, eval=FALSE}
x <- rexp(100) # sample 100 iid values from an Exponential(1) distribution
length(x) # length of x
sum(x) # sum of x
mean(x) # sample mean of x
var(x) # sample variance of x
sd(x) # sample standard deviation of x
range(x) # range of x
log(x) # logarithm of x
summary(x) # summary of x
str(x) # structure of x
```

### Functions related to statistical distributions

Suppose that a random variable $X$ has the `dist` distribution:
  
*	`p[dist](q, ...)`: returns the cdf of $X$ evaluated at `q`, i.e., $p=P(X\leq q)$.
*	`q[dist](p, ...)`: returns the inverse cdf (or quantile function) of $X$ evaluated at $p$, i.e., $q = \inf\{x: P(X\leq x) \geq p\}$.
* `d[dist](x, ...)`: returns the mass or density of $X$  evaluated at $x$ (depending on whether it's discrete or continuous).
* `r[dist](n, ...)`: returns an i.i.d. random sample of size `n` having the same distribution as $X$.
* The `...` indicates that additional arguments describing the parameters of the distribution may be required.

### Example {.example}

Execute the following commands in R to see the output. What is each command doing?

```{r, eval=FALSE}
pnorm(1.96, mean = 0, sd = 1)
qunif(0.6, min = 0, max = 1)
dbinom(2, size = 20, prob = .2)
dexp(1, rate = 2)
rchisq(100, df = 5)
```

* `pnorm(1.96, mean = 0, sd = 1)` returns the probability that a standard normal random variable is less than or equal to 1.96, i.e., $P(X \leq 1.96)$.
* `qunif(0.6, min = 0, max = 1)` returns the value $x$ such that $P(X\leq x) = 0.6$ for a uniform random variable on the interval $[0, 1]$.
* `dbinom(2, size = 20, prob = .2)` returns the probability that $P(X=2)$ for $X~\textrm{Binom}(n=20,\pi=0.2)$.
* `dexp(1, rate = 2)` evaluates the density of an exponential random variable with mean = 1/2 at $x=1$.
* `rchisq(100, df = 5)` returns a sample of 100 observations from a chi-squared random variable with 5 degrees of freedom.

## Data Frames

Data frames are two-dimensional data objects. Each column of a data frame is a vector (or variable) of possibly different data types. This is a *fundamental* data structure used by most of R's modeling software. 

In general, I recommend *tidy data*, which means that each variable forms a column of the data frame, and each observation forms a row.

### Creation
Data frames are created by passing vectors into the `data.frame` function.

The names of the columns in the data frame are the names of the vectors you give the `data.frame` function.

Consider the following simple example.

```{r, paged.print=FALSE}
d <- c(1, 2, 3, 4)
e <- c("red", "white", "blue", NA)
f <- c(TRUE, TRUE, TRUE, FALSE)
df <- data.frame(d,e,f)
df
```

The columns of a data frame can be renamed using the `names` function on the data frame.

```{r, paged.print=FALSE}
names(df) <- c("ID", "Color", "Passed")
df
```

The columns of a data frame can be named when you are first creating the data frame by using `name =` for each vector of data.
```{r, paged.print=FALSE}
df2 <- data.frame(ID = d, Color = e, Passed = f)
df2
```

### Extracting parts of a data frame
The column vectors of a data frame may be extracted using `$` and specifying the name of the desired vector.

* `df$Color` would access the `Color` column of data frame `df`.

Part of a data frame can also be extracted by thinking of at as a general matrix and specifying the desired rows or columns in square brackets after the object name. For example, if we had a data frame named `df`:

* `df[1,]` would access the first row of `df`.
* `df[1:2,]` would access the first two rows of `df`.
* `df[,2]` would access the second column of `df`.
* `df[1:2, 2:3]` would access the information in rows 1 and 2 of columns 2 and 3 of `df`.

If you need to select multiple columns of a data frame by name, you can pass a character vector with column names in the column position of `[]`.

* `df[, c("Color", "Passed")]` would extract the `Color` and `Passed` columns of df.
### Example {.example}

Execute the following commands in the Console:
```{r, eval=FALSE, paged.print=FALSE}
df3 <- data.frame(numbers = 1:5,
                  characters = letters[1:5],
                  logicals = c(TRUE, TRUE, FALSE, TRUE, FALSE))
df3 # print df
df3$logicals # access the logicals vector of df3
df3[1, ] # access the first column of df3
df3[, 3] # access the third column of df3
df3[, 2:3] # access the column 2 and 3 of df3
df3[, c("numbers", "logicals")] # access the numbers and logical columns of df3
```

Students often can work more conveniently with vectors, so it is sometimes desirable to access a part of a data frame and assign it a new name for later use. For example, to access the `ID` column of `df2` and assign it the name `newID`, we could execute `newID <- df2$ID`.

### Importing Data

The `read.table` function imports data from file into R as a data frame.

Usage: `read.table(file, header = TRUE, sep = ",")`

* `file` is the file path and name of the file you want to import into R.
  * If you don't know the file path, set `file = file.choose()` will bring up a dialog box asking you to locate the file you want to import.
* `header` specifies whether the data file has a header (variable labels for each column of data in the first row of the data file).
  * If you don't specify this option in R or use `header = FALSE`, then R will assume the file doesn't have any headings.
  * `header = TRUE` tells R to read in the data as a data frame with column names taken from the first row of the data file.
* `sep` specifies the delimiter separating elements in the file.
  * If each column of data in the file is separated by a space, then use `sep = " "`
  * If each column of data in the file is separated by a comma, then use `sep = ","`
  * If each column of data in the file is separated by a tab, then use `sep = "\t"`.

Here is an example reading a csv (comma separated file) with a header:
  
```{r}
dtf <- read.table(file = "https://raw.githubusercontent.com/jfrench/DataWrangleViz/master/data/covid_dec4.csv",
                  header = TRUE,
                  sep = ",")
str(dtf)
```

Note that the `read_table` function in the **readr** package and the `fread` function in the **data.table** package are perhaps better ways of reading in tabular data and use similar syntax.

## Logical statements

### Basic comparisons
Sometimes we need to know if the elements of an object satisfy certain conditions.  This can be determined using the logical operators `<`, `<=`, `>`, `>=`, `==`, `!=`.

* `==` means equal to.
* `!=` means NOT equal to.

### Example {.example}

Execute the following commands in R and see what you get. What is each statement performing?

```{r, eval=FALSE}
# a <- seq(2, 16, by = 2) # creating the vector a
a
a > 10
a <= 4
a == 10
a != 10
```

### And and Or statements
More complicated logical statements can be made using `&` and `|`.

* `&` means "and"
  * Only `TRUE & TRUE` returns `TRUE`. Otherwise the `&` operator returns `FALSE`.
* `|` means "or"
  * Only a single value in an "|" statements needs to be true for `TRUE` to be returned.

Note that:

* `TRUE & TRUE` returns `TRUE`
* `FALSE & TRUE` returns `FALSE`
* `FALSE & FALSE` returns `FALSE`
* `TRUE | TRUE` returns `TRUE`
* `FALSE | TRUE` returns `TRUE`
* `FALSE | FALSE` returns `FALSE`

```{r}
TRUE & TRUE
FALSE & TRUE
FALSE & FALSE
TRUE | TRUE
FALSE | TRUE
FALSE | FALSE
```



### Example
Execute the following commands in R and see what you get.

```{r, eval=FALSE}
(a > 6) & (a <= 10)
(a <= 4) | (a >= 12)
```

## Subsetting with logical statements

Logical statements can be used to return parts of an object satisfying the appropriate criteria. Specifically, we pass logical statements within the square brackets used to access part of a data structure.

### Example

Execute the following code:
  
```{r, eval = FALSE}
# accessing parts of a vector
a
a < 6
a[a < 6]
a == 10
a[a == 10]
(a < 6) | ( a == 10)
a[(a < 6) | ( a == 10)]
# accessing parts of a data frame
# create a logical vector based on whether
# a state_abb in dtf is "CA" or "CO"
ca_or_co <- is.element(dtf$state_abb, c("CA", "CO"))
ca_or_co
# access the CA and CA rows of dtf
dtf[ca_or_co,]
```

## Ecosystem debate

We will typically work with **base** R, which are commands and functions R offers by default.

The **tidyverse** ([https://www.tidyverse.org](https://www.tidyverse.org)) is a collection of R packages that provides a unified framework for data manipulation and visualization.

Since this course focuses more on modeling than data manipulation, I will typically focus on approaches in **base** R. I will use functions from the **tidyverse** when it greatly simplifies analysis, data manipulation, or visualization.

<!--chapter:end:01-r-foundations.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  html_notebook: default
---

# Data exploration

Based on Chapter 1 of LMWR2, Chapter 1 of ALR4

## Data analysis process

1. Define a question of interest.
2. Collect relevant data.
3. Analyze the data.
4. Interpret your analysis.
5. Make a decision.

"The formulation of a problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill" - Albert Einstein

### Problem Formulation

* Understand the physical background.
  * Statisticians often work in collaboration with others and need to understand something about the subject area.
* Understand the objective.
  * What are your goals?
  * Make sure you know what the client wants.
* Put the problem into statistical terms.
  * This is often the most challenging step and where irreparable errors are sometimes made.
  * That a statistical method can read in and process the data is not enough. The results of an inapt analysis may be meaningless.

### Data collection

Data collection:

* How the data were collected has a crucial impact on what conclusions can be made.
  * Are the data observational or experimental?
  * Are the data a sample of convenience or were they obtained via a designed sample survey?
* Is there nonresponse bias?
  * The data you do not see may be just as important as the data you do see.
* Are there missing values?
  * This is a common problem that is troublesome and time consuming to handle.
  * How are the data coded? How are the qualitative variables represented?
* What are the units of measurement?
* Beware of data entry errors and other corruption of the data.
  * Perform some data sanity checks.

## Data exploration

An initial exploration of the data should be performed prior to any formal analysis or modeling.

Initial data analysis should consist of numerical summaries and appropriate plots.

### Numerical summaries of data

Statistics can be used to numerically summarize aspects of the data:

* mean
* standard deviation (SD)
* maximum and minimum
* correlation
* other measures, as appropriate

### Visual summaries of data

Plots can provide a useful visual summary of the data.

* For one numerical variable: boxplots, histograms, density plots, etc.
* For two numerical variables: scatterplots.
* For three or more variables, construct interactive and dynamic graphics.
* For one categorial variable: bar charts

Good graphics are essential in data analysis.

* They help us avoid mistakes.
* They help us decide on a model.
* They help communicate the results of our analysis.
* Graphics can be more convincing than text at times.

### What to look for
When summarizing the data, look for:

* outliers
* data-entry errors
* skewness
* unusual distributions
* patterns or structure

## Kidney Example

The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The following variables were recorded:

* `pregnant` - number of times pregnant
* `glucose` - plasma glucose concentration at 2 hours in an oral glucose tolerance test
* `diastolic` - diastolic blood pressure (mm Hg)
* `triceps` - triceps skin fold thickness (mm)
* `insulin` - 2-hour serum insulin (mu U/ml)
* `bmi` - body mass index (weight in kg/(height in m2))
* `diabetes` - diabetes pedigree function
* `age` - age (years)
* `test` - test whether the patient showed signs of diabetes (coded zero if negative, one if positive).

The data may be obtained from the UCI Repository of machine learning databases at https://archive.ics.uci.edu/ml.

Let's load and examine the structure of the data
```{r}
data(pima, package = "faraway")
str(pima) # structure
head(pima) # first six rows
tail(pima) # last six rows
```
### Numerically summarizing the data
The `summary` command is a useful way to numerically summarize a data frame.

The `summary` function will compute the minimum, 0.25 quantile, mean, median, 0.75 quantile, and maximum of a `numeric` variable.

The `summary` function will count the number of values of each level of a `factor` variable.

Let's summarize the `pima` data frame.

```{r}
summary(pima)
```
### Cleaning the data

Cleaning data involves finding and correcting data quality issues.

Some odd things about the `pima` data

* The minimum `diastolic` blood pressure is zero.
  * That's generally an indication of a health problem.
* The `test` variable appears to be `numeric` but should be a `factor` (categorical) variable.
* Many other variables have unusual zeros.
  * Look for anything unusual or unexpected, perhaps indicating a data-entry error.

Let's look at the sorted `diastolic` values.
```{r, echo=T}
sort(pima$diastolic)
```

The first 35 values of `diastolic` are zero. That's a problem.

* It seems that 0 was used in place of a missing value.
* This is very bad since 0 is a real number and this problem may be overlooked, which can lead to faulty analysis!
* This is why we must check our data carefully for things that don't make sense.

The value for missing data in R is `NA`.

Several variables share this problem.  Let's set the 0s that should be missing values to `NA`.

```{r, echo=T}
pima$diastolic[pima$diastolic == 0]  <- NA
pima$glucose[pima$glucose == 0] <- NA
pima$triceps[pima$triceps == 0]  <- NA
pima$insulin[pima$insulin == 0] <- NA
pima$bmi[pima$bmi == 0] <- NA
```

The `test` variable is a categorical variable, not numerical.  

* R thinks the `test` variable is `numeric`.
* In R, a categorical variable is a `factor`.
* We need to convert the `test` variable to a `factor`.

Let's convert `test` to a factor.
```{r, echo=T}
 pima$test <- factor(pima$test)
 summary(pima$test)
```

500 of the cases were negative and 268 were positive.  We can provide more descriptive labels using the `levels` function.

We change the `0` and `1` levels to `negative` and `positive` to make the data more descriptive. A `summary` of the updates `test` variable shows why this is useful. 

```{r, echo=T}
levels(pima$test) <- c("negative", "positive")
summary(pima$test)
```

## Visualizing data with **base** graphics

### Histograms
The `hist` command can be used create a histogram of a numerical vector.

* The labels of the plot can be customized using the `xlab` and `ylab` arguments.
* The main title of the plot can be customized using the `main` argument.
  
  
Here is a slightly customized histogram of diastolic blood pressure.
```{r}
hist(pima$dias, xlab = "diastolic blood pressure" , main="")
```

The histogram is approximately bell-shaped and centered around 70.

We can change the number of breaks in the histogram by specifying the `breaks` argument of the `hist` function.

Consider how the plot changes below.

```{r}
hist(pima$dias, xlab = "diastolic blood pressure", main = "", breaks = 20)
```

### Density plots

Many people prefer the density plot over the histogram because the histogram is more sensitive to its options.  

A density plot is essentially a smoothed version of a histogram.

  * It isn't as blocky.
  * It sometimes has weird things happen at the boundaries.


The `plot` and `density` function can be combined to construct a density plot.
```{r}
plot(density(pima$diastolic, na.rm=TRUE), main = "")
```

In the example above, we have to specify `na.rm = TRUE` so that the density is only computed using the non-missing values.

### Index plots

An index plot is a scatter plot of a numerical variable versus the index of each value (i.e., the position it occurs in the vector).

* This is most useful for sorted vectors.

A scatter plot of the sorted numerical values versus their index can be used to identify outliers and see whether the data has many repeated values.

```{r}
plot(sort(pima$diastolic), ylab = "sorted diastolic bp")
```

The flat spots in the plot above show that the `diastolic` variable has mean repeated values.

### Bivariate scatter plots

Bivariate scatter plots can be used to identify the relationship between two numerical variables.

A scatter plot of diabetes vs diastolic blood pressure.
```{r}
plot(diabetes ~ diastolic, data = pima)
```

There is no clear pattern in the points, so it's difficult to claim a relationship between the two variables.

### Bivariate boxplots

A parallel boxplot of `diabetes` score versus `test` result.
```{r}
plot(diabetes ~ test, data = pima)
```

The median `diabetes` score seems to be a bit higher for positive tests in comparison to the negative tests.

### Multiple plots in one figure

The `par` function can be used to construct multiple plots in one figure. 

* The `mfrow` argument can be used to specify the number of rows and columns of plots you need.

A 1 by 2 set of plots is shown below.
```{r}
par(mfrow = c(1, 2))
plot(diabetes ~ diastolic, data = pima)
plot(diabetes ~ test, data = pima)
par(mfrow = c(1, 1)) # reset to a single plot
```

## Visualizing data with **ggplot2**

The plots we have just created are using the **base** graphics system in R.

* These are very fast, simple, and professional.

A fancier alternative is to construct plots using the **ggplot2** package.

In its simplest form, to construct a (useful) plot in **ggplot2**, you need to provide:

* A `ggplot` object.
  * This is usually the object that holds your data frame.
  * The data frame is passed to `ggplot` via the `data` argument.
* A geometry object  
  * Roughly speaking, this is the *kind* of plot you want.
  * e.g., `geom_hist` for a histogram, `geom_point` for a scatter plot, `geom_density` for a density plot.
* An aesthetic mapping
  * Aesthetic mappings describe how variables in the data are mapped to visual properties of a geometry.
  * This is where you specify which variable with be the `x` variable, the `y` variable, which variable will control plots in the color, etc.
  
### A **ggplot2** histogram
```{r}
library(ggplot2)
ggpima <-  ggplot(pima)
ggpima + geom_histogram(aes(x=diastolic))
```

### A **ggplot2** density plot
```{r}
ggpima + geom_density(aes(x = diastolic))
```

### A **ggplot2** scatter plot
```{r}
ggpima + geom_point(aes(x = diastolic, y = diabetes))
```

### Scaling **ggplot2** plots

In general, *scaling* is the process by which **ggplot2** maps variables to unique values. When this is done for discrete variables, **ggplot2** will often scale the variable to distinct colors, symbols, or sizes, depending on the aesthetic mapped.

In the example below, we map the `test` variable to the `shape` aesthetic, which is then scaled to different shapes for the different `test` levels.

```{r}
ggpima + 
  geom_point(aes(x = diastolic, y = diabetes, shape = test))
```

Alternatively, we can map the `test` variable to the `color` aesthetic, which creates a plot with different colors for each observation based on the `test` level.

```{r}
ggpima + 
  geom_point(aes(x = diastolic, y = diabetes, color = test))
```

We can even combine these two aesthetic mappings in a single plot to get different colors and symbols for each level of `test`.

```{r}
ggpima + 
  geom_point(aes(x = diastolic, y = diabetes, shape = test, color = test))
```

### Facetting in `ggplot2` 

Facetting creates separate panels (facets) of a data frame based on one or more facetting variables.

Below, we facet the data by the `test` result.

```{r}
ggpima +
  geom_point(aes(x = diastolic, y = diabetes)) +
  facet_grid(~ test)
```

### Summary of **ggplot2**

To create a **ggplot2** plot:

* Create a`ggplot` object using the `ggplot` function.
  * Specify the data frame the data is contained in (e.g., the data frame is `pima`). 
* Specify the geometry for the plot (the kind of plot you want to produce)
* Specify the aesthetics using `aes`.
  * The aesthetic specifies what you see, such as position in the $x$ or $y$ direction or aspects such as shape or color.
  * The aesthetic can be specifed in the geometry, or if you have consistent aesthetics across multiple geometries, in the `ggplot` statement.

The advantage of **ggplot2** is more apparent in producing complex plots involving more than two variables.

   * **ggplot2** makes it easy to plot the data for each group with different colors, symbols, line types, etc.
   * **ggplot2** will automatically provide a legend mapping the attributes to the different groups.
    * You can easily facet the data into multiple panels. 

## Summary of data exploration

You should use both numerical and graphical summaries of data **prior** to modeling data.

Data exploration helps us to:

* Gain understanding about our data
* Identify problems or unusual features of our data
* Identify patterns in our data
* Decide on a modeling approach for the data
* etc.


<!--chapter:end:02-data-exploration.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Review of probability, random variables, and random vectors

## Probability Basics


Points $\omega$ in $\Omega$ are called **sample outcomes**, **realizations**, or **elements**.

A **set** is a (possibly empty) collection of elements.

* Sets are denoted as a set of elements between curly braces, i.e., $\{\omega_1, \omega_2, \ldots\}$, where the $\omega_i$ are elements of $\Omega$.

Set $A$ is a subset of set $B$ if every element of $A$ is an element of $B$.

* This is denoted as $A \subseteq B$, meaning that $A$ is a subset of $B$. 
* Subsets of $\Omega$ are **events**.

The **null set** or **empty set**, $\emptyset$, is the set with no elements, i.e., $\{\}$.

* The empty set is a subset of any other set.

A function $P$ that assigns a real number $P(A)$ to every event $A$ is a probability distribution if it satisfies three properties:

1. $P(A)\geq 0$ for all $A\in \Omega$
2. $P(\Omega)=P(\omega \in \Omega) = 1$
3. If $A_1, A_2, \ldots$ are disjoint, then $P\left(\bigcup_{i=1}^\infty A_i \right)=\sum_{i=1}^\infty P(A_i)$.

A set of events $\{A_i:i\in I\}$ are **independent** if 
$$P\left(\cap_{i\in J} A_i \right)=\prod_{i\in J} P(A_i ) $$
for every finite subset $J\subseteq I$.


## Random Variables

A **random variable** $Y$ is a mapping/function
$$Y:\Omega\to\mathbb{R}$$
that assigns a real number $Y(\omega)$ to each outcome $\omega$.

* We typically drop the $(\omega)$ part for simplicity.

The **cumulative distribution function (CDF)** of $Y$, $F_Y$, is a function $F_Y:\mathbb{R}\to [0,1]$ defined by $$F_Y (y)=P(Y \leq y).$$

* The subscript of $F$ indicates the random variable the CDF describes.
* E.g., $F_X$ denotes the CDF of the random variable $X$ and $F_Y$ denotes the CDF of the random variable $Y$.
* The subscript can be dropped when the context makes it clear what random variable the CDF describes.

The support of $Y$, $\mathcal{S}$, is the smallest set such that $P(Y\in \mathcal{S})=1$.

### Discrete random variables

$Y$ is a **discrete** random variable if it takes countably many values $\{y_1, y_2, \dots \} = \mathcal{S}$.  

The **probability mass function (pmf)** for $Y$ is $f_Y (y)=P(Y=y)$, where $y\in \mathbb{R}$, and must have the following properties:

1. $0 \leq f_Y(y) \leq 1$.
2. $\sum_{y\in \mathcal{S}} f_Y(y) = 1$.

Additionally, the following statements are true:

* $F_Y(c) = P(Y \leq c) = \sum_{y\in \mathcal{S}:y \leq c} f_Y(y)$.
* $P(Y \in A) = \sum_{y \in A} f_Y(y)$ for some event $A$.
* $P(a \leq Y \leq b) = \sum_{y\in\mathcal{S}:a\leq y\leq b} f_Y(y)$.

The expected value, mean, or first moment of $Y$ is defined as 
$$E(Y) = \sum_{y\in \mathcal{S}} y f_Y(y),$$
assuming the sum is well-defined.

The **variance** of $Y$ is defined as 
$$var(Y)=E(Y-E(Y))^2== \sum_{y\in \mathcal{S}} (y - E(Y))^2 f_Y(y).$$
The **standard deviation** of Y is $$SD(Y)=\sqrt{var(Y)  }.$$

#### Example (Bernoulli)
A random variable $Y\sim \mathsf{Bernoulli}(\pi)$ if $\mathcal{S} = {0, 1}$ and $P(Y = 1) = \pi$, where $\pi\in (0,1)$.

The pmf of a Bernoulli random variable is $$f_Y(y) = \pi^y (1-\pi)^{(1-y)}.$$

Determine $E(Y)$ and $var(Y)$.

### Continuous random variables

$Y$ is a continuous random variable if there exists a function $f_Y (y)$ such that: 

1. $f_Y (y)\geq 0$ for all $y$,
2. $\int_{-\infty}^\infty f_Y (y)  dy = 1$,
3. $a\leq b$, $P(a<Y<b)=\int_a^b f_Y (y)  dy$.  

The function $f_Y$ is called the **probability density function (pdf)**.  

Additionally, $F_Y (y)=\int_{-\infty}^y f_Y (y)  dy$ and $f_Y (y)=F'_Y(y)$ for any point $y$ at which $F_Y$ is differentiable. 

The expected value of a continuous random variables $Y$ is defined as 
$$E(Y)= \int_{-\infty}^{\infty} y f_Y(y)  dy = \int_{y\in\mathcal{S}} y f_Y(y).$$
assuming the integral is well-defined.

The **variance** of a continuous random variable $Y$ is defined by 
$$var(Y)=E(Y-E(Y))^2=\int_{-\infty}^{\infty} (y - E(Y))^2 f_Y(y)  dy = \int_{y\in\mathcal{S}} (y - E(Y))^2 f_Y(y) dy.$$
The **standard deviation** of Y is $$SD(Y)=\sqrt{var(Y)  }.$$

### Useful facts for transformation of random variables

Let $Y$ be a random variable and $c\in\mathbb{R}$ be a constant. Then:

* $E(cY) = c E(Y)$
* $E(c + Y) = c + E(Y)$
* $var(cY) = c^2 var(Y)$
* $var(c + Y) = var(Y)$

## Multivariate distributions

### Basic properties

Let $Y_1,Y_2,\ldots,Y_n$ denote $n$ random variables with supports $\mathcal{S}_1,\mathcal{S}_2,\ldots,\mathcal{S}_n$, respectively.

If the random variables are jointly (all) discrete, then the joint pmf $f(y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$  satisfies the following properties:

1. $0\leq f(y_1,\ldots,y_n )\leq 1$,
2. $\sum_{y_1\in\mathcal{S}_1}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\sum_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n)$.

In this context,

$$E(Y_1 \cdots Y_n)=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n}y_1 \cdots y_n  f(y_1,\ldots,y_n).$$

In general,
$$E(g(Y_1,\ldots,Y_n))=\sum_{y_1\in\mathcal{S}_1} \cdots \sum_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n),$$
where $g$ is a function of the random variables.

If the random variables are jointly continuous, then $f(y_1,\ldots,y_n)=P(Y_1=y_1,\ldots,Y_n=y_n)$  is the joint pdf if it satisfies the following properties:

1. $f(y_1,\ldots,y_n ) \geq 0$,
2. $\int_{y_1\in\mathcal{S}_1}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n ) dy_n \cdots dy_1 = 1$,
3. $P((Y_1,\ldots,Y_n)\in A)=\int \cdots \int_{(y_1,\ldots,y_n) \in A} f(y_1,\ldots,y_n) dy_n\ldots dy_1$.

In this context,

$$E(Y_1 \cdots Y_n)=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} y_1 \cdots y_n  f(y_1,\ldots,y_n) dy_n \ldots dy_1.$$

In general,
$$E(g(Y_1,\ldots,Y_n))=\int_{y_1\in\mathcal{S}_1} \cdots \int_{y_n\in\mathcal{S}_n} g(y_1, \ldots, y_n) f(y_1,\ldots,y_n) dy_n \cdots dy_1,$$
where $g$ is a function of the random variables.

### Marginal distributions

If the random variables are jointly discrete, then the marginal pmf of $Y_1$ is $$f_{Y_1}(y_1)=\sum_{y_2\in\mathcal{S}_2}\cdots \sum_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n).$$

Similarly, if the  random variables are jointly continuous, then the marginal pdf of $Y_1$ is $$f_{Y_1}(y_1)=\int_{y_2\in\mathcal{S}_2}\cdots \int_{y_n\in\mathcal{S}_n} f(y_1,\ldots,y_n) dy_n \cdots dy_2.$$


### Independence of random variables
Random variables $X$ and $Y$ are independent if
$$F(x, y) = F_X(x) F_Y(y).$$

Alternatively, $X$ and $Y$ are independent if
$$f(x, y) = f_X(x)f_Y(y).$$

### Conditional distributions
Let $X$ and $Y$ be random variables. The conditional distribution of $X$ given $Y = y$, denoted $X|Y=y$ is
$$ f(x|y) = f(x, y)/f_{Y}(y).$$

### Covariance 

The covariance between random variables $X$ and $Y$ is 
$$cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).$$

### Useful facts for transformations of multiple random variables

Let $a$ and $b$ be scalar constants.  Then:

-	$E(aY)=aE(Y)$
-	$E(a+Y)=a+E(Y)$
-	$E(aY+bZ)=aE(Y)+bE(Z)$
-	$var(aY)=a^2 var(Y)$
-	$var(a+Y)=var(Y)$
-	$cov(aY,bZ)=ab cov(Y,Z).$
-	$var(Y+Z)=var(Y)+var(Z)+2cov(Y,Z).$

## Random vectors

### Definition
Let $\mathbf{y}=(Y_1,Y_2,\dots,Y_n )^T$ be an $n\times1$ vector of random variables.  $\mathbf{y}$ is a random vector.

* A vector is always defined to be a column vector, even if the notation is ambiguous.

### Mean, variance, and covariance

The mean of a random vector is
$$E(\mathbf{y})=\begin{pmatrix}E(Y_1)\\E(Y_2)\\\vdots\\E(Y_n)\end{pmatrix}.$$

The variance (covariance) of a random vector is
$$\begin{aligned}
var(\mathbf{y}) &= E(\mathbf{y}\mathbf{y}^T )-E(\mathbf{y})E(\mathbf{y})^T\\
&= \begin{pmatrix}var(Y_1) & cov(Y_1,Y_2) &\dots &cov(Y_1,Y_n)\\cov(Y_2,Y_1 )&var(Y_2)&\dots&cov(Y_2,Y_n)\\\vdots&\vdots&\vdots&\vdots\\
cov(Y_n,Y_1)&cov(Y_n,Y_2)&\dots&var(Y_n)\end{pmatrix}\end{aligned}.$$

Let $\mathbf{x} = (X_1, X_2, \ldots, X_n)^T$ and $\mathbf{y} = (Y_1, Y_2, \ldots, Y_n)^T$ be $n\times 1$ random vectors.

The covariance between two random vectors is 
$$cov(\mathbf{x}, \mathbf{y}) = E(\mathbf{x}, \mathbf{y}^T) - E(\mathbf{x}) E(\mathbf{y})^T.$$

## Properties of transformations of random vectors

Define:

* $\mathbf{a}$ to be  an $n\times 1$ vector of constants
* $A$ to be an $m\times n$ matrix of constants
* $\mathbf{x}=(X_1,X_2,\ldots,X_n)^T$ to be an $n\times 1$ random vector
* $\mathbf{y}=(Y_1,Y_2,\ldots,Y_n)^T$ to be an $n\times 1$ random vector
*	$\mathbf{z}=(Z_1,Z_2,\ldots,Z_n)^T$ to be an $n\times 1$ random vector
* $0_{n\times n}$ to be an $n\times n$ matrix of zeros.

Then:

*	$E(A\mathbf{y})=AE(\mathbf{y}), E(\mathbf{y}A^T )=E(\mathbf{y}) A^T.$
*	$E(\mathbf{x}+\mathbf{y})=E(\mathbf{x})+E(\mathbf{y})$
*	$var(A\mathbf{y})=A\ var(\mathbf{y}) A^T$
*	$cov(\mathbf{x}+\mathbf{y},\mathbf{z})=cov(\mathbf{x},\mathbf{z})+cov(\mathbf{y},\mathbf{z})$
*	$cov(\mathbf{x},\mathbf{y}+\mathbf{z})=cov(\mathbf{x},\mathbf{y})+cov(\mathbf{x},\mathbf{z})$
*	$cov(A\mathbf{x},\mathbf{y})=A\ cov(\mathbf{x},\mathbf{y})$
* $cov(\mathbf{x},A\mathbf{y})=cov(\mathbf{x},\mathbf{y}) A^T$
* $var(a)= 0_{n\times n}$
* $cov(\mathbf{a},\mathbf{y})=0_{n\times n}$
* $var(\mathbf{a}+\mathbf{y})=var(\mathbf{y})$

## Multivariate normal (Gaussian) distribution

### Definition
$\mathbf{y}=(Y_1,\dots,Y_n )^T$ has a multivariate normal distribution with mean $\mathbf{\mu}$ (an $n\times 1$ vector) and covariance $\mathbf{\Sigma}$ (an $n\times n$ matrix) if the joint pdf is

$$f(\mathbf{y})=\frac{1}{(2\pi)^{n/2} |\mathbf{\Sigma}|^{1/2} }  \exp\left(-\frac{1}{2} (\mathbf{y}-\mathbf{\mu})^T \Sigma^{-1} (\mathbf{y}-\mathbf{\mu})\right).$$
Note that $\mathbf{\Sigma}$ must be symmetric and positive definite.

We would denote this as $\mathbf{y}\sim N(\mathbf{\mu},\mathbf{\Sigma)}$.

### Useful facts

**Important fact**:  A linear function of a multivariate normal random vector (i.e., $\mathbf{a}+A\mathbf{y}$) is also multivariate normal (though it could collapse to a single random variable if $A$ is a $1\times n$ vector).  

**Application**:  Suppose that $\mathbf{y}\sim N(\mu,\Sigma)$. For an $m\times n$ matrix of constants $A$, $A\mathbf{y}\sim N(A\mu,A\Sigma A^T)$.

## Example

Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers.  Let $Y_1$ denote the proportion of the capacity of the bulk tank that is available after the tank is stocked at the beginning of the week.  Because of the limited supplies, $Y_1$ varies from week to week.  Let $Y_2$ denote the proportion of the capacity of the bulk tank that is sold during the week.  Because $Y_1$ and $Y_2$ are both proportions, both variables are between 0 and 1.  Further, the amount sold, $y_2$, cannot exceed the amount available, $y_1$.  Suppose the joint density function for $Y_1$ and $Y_2$ is given by 
$$f(y_1,y_2 )=3y_1;\ 0 \leq y_2\leq y_1\leq 1.$$

### Problem 1
Determine $P(0\leq Y_1\leq 0.5;\ 0.25\leq Y_2)$


### Problem 2
Determine $f_{Y_1 }$ and $f_{Y_2 }$

### Problem 3
Determine $E(Y_1)$ and $E(Y_2)$


### Problem 4
Determine $var(Y_1)$ and $var(Y_2)$

### Problem 5
Determine $E(Y_1 Y_2)$

### Problem 6
Determine $cov(Y_1,Y_2)$

### Problem 7
Determine the mean and variance of $\mathbf{a}^T \mathbf{y}$, where $\mathbf{a}=(1,-1)^T$ and $\mathbf{y}=(Y_1,Y_2 )^T$.  This is the expectation and variance of the difference between the amount of gas available and the amount of gas sold.

<!--chapter:end:03-probability-and-vectors.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_notebook
---

A **matrix** is a two-dimensional array of values, symbols, or other objects (depending on the context). We will assume that our matrices contain numbers or random variables. Context will make it clear which is being represented.

* Matrices are commonly denoted by bold capital letters like $\mathbf{A}$ or $\mathbf{B}$, but this will sometimes be simplified to capital letters like $A$ or $B$.

# Useful matrix facts

## Notation
A matrix $\mathbf{A}$ with $m$ rows and $n$ columns (an $m\times n$ matrix) will be denoted as 
$$\mathbf{A} = \begin{bmatrix}
\mathbf{A}_{1,1} & \mathbf{A}_{2,1} & \cdots & \mathbf{A}_{1,n} \\
\mathbf{A}_{2,1} & \mathbf{A}_{2,1} & \cdots & \mathbf{A}_{2,n} \\
\vdots & \vdots & \ddots & \vdots \\
\mathbf{A}_{m,1} & \mathbf{A}_{m,2} & \cdots & \mathbf{A}_{m,n} \\
\end{bmatrix},
$$

where $\mathbf{A}_{i,j}$ denotes the element in row $i$ and column $j$ of matrix $\mathbf{A}$.

A **column vector** is a matrix with a single column. A **row vector** is a matrix with a single row. 

* Vectors are commonly denoted with bold lowercase letters such as $\mathbf{a}$ or $\mathbf{b}$, but this may be simplified to lowercase letters such as $a$ or $b$.

A $p\times 1$ column vector $\mathbf{a}$ may constructed as 
$$\mathbf{a} = [a_1, a_2, \ldots, a_p]^T = 
\begin{bmatrix}
a_1 & a_2 & \cdots & a_p
\end{bmatrix}^T = \begin{bmatrix}
a_1 \\ a_2 \\ \vdots \\ a_p
\end{bmatrix}.$$

## Basic mathematical properties

### Addition and subtraction

Consider matrices $\mathbf{A}$ and $\mathbf{B}$ with identical sizes $m\times n$. 

We add $\mathbf{A}$ and $\mathbf{B}$ by adding the element in position $i,j$ of $\mathbf{B}$ with the element in position $i,j$ of $A$, i.e.,

$$(\mathbf{A} + \mathbf{B})_{i,j} = \mathbf{A}_{i,j} + \mathbf{B}_{i,j}.$$

Similarly, if we subtract $\mathbf{B}$ from matrix $\mathbf{A}$, then we subtract the element in position $i,j$ of $\mathbf{B}$ from the element in position $i,j$ of $\mathbf{A}$, i.e., 

$$(\mathbf{A} - \mathbf{B})_{i,j} = \mathbf{A}_{i,j} - \mathbf{B}_{i,j}.$$

Example:

$$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix} + 
\begin{bmatrix}
2 & 9 & 1 \\
1 & 3 & 1 \\
\end{bmatrix} = 
\begin{bmatrix}
3 & 11 & 4 \\
5 & 8 & 7 \\
\end{bmatrix}.$$

$$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix} - 
\begin{bmatrix}
2 & 9 & 1 \\
1 & 3 & 1 \\
\end{bmatrix} = 
\begin{bmatrix}
-1 & -7 & 2 \\
3 & 2 & 5 \\
\end{bmatrix}.$$

### Scalar multiplication

A matrix multiplied by a scalar value $c\in\mathbb{R}$ is the matrix obtained by multiplying each element of the matrix by $c$. If $\mathbf{A}$ is a matrix and $c\in \mathbb{R}$, then 
$$(c\mathbf{A})_{i,j} = c\mathbf{A}_{i,j}.$$
Example: $$3\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
\end{bmatrix}=
\begin{bmatrix}
3\cdot 1 & 3\cdot 2 & 3\cdot 3 \\
3\cdot 4 & 3\cdot 5 & 3\cdot 6 \\
\end{bmatrix}=
\begin{bmatrix}
3 & 6 & 9 \\
12 & 15 & 18 \\
\end{bmatrix}.$$

### Matrix multiplication

Consider two matrices $\mathbf{A}$ and $\mathbf{B}$. The matrix product $\mathbf{AB}$ is only defined if the number of columns in $\mathbf{A}$ matches the number of rows in $\mathbf{B}$. 

Assume $\mathbf{A}$ is an $m\times n$ matrix and $\mathbf{B}$ is an $n\times p$ matrix. $\mathbf{AB}$ will be an $m\times p$ matrix and $$(\mathbf{AB})_{i,j} = \sum_{k=1}^{n} \mathbf{A}_{i,k}\mathbf{B}_{k,j}.$$

Example: $$\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
\begin{bmatrix}
1 & 4\\
2 & 5\\
3 & 6
\end{bmatrix}=
\begin{bmatrix}
1\cdot 1 +  2 \cdot 2 + 3 \cdot 3 & 1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6\\
4\cdot 1 +  5 \cdot 2 + 6 \cdot 3 & 4 \cdot 4 + 5 \cdot 5 + 6 \cdot 6\\
\end{bmatrix}=
\begin{bmatrix}
14 & 32\\
32 & 77\\
\end{bmatrix}.$$

### Associative property

Addition and multiplication satisfy the associative property for matrices. Assuming that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to do the operations below, then

$$(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$$
and
$$(\mathbf{AB})\mathbf{C}=\mathbf{A}(\mathbf{BC}).$$

### Distributive property

Matrix operations satisfy the distributive property. Assuming that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to do the operations below, then

$$\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{AB} + \mathbf{AC}\quad\mathrm{and}\quad (\mathbf{A}+\mathbf{B})\mathbf{C} = \mathbf{AC} + \mathbf{BC}.$$

### No commutative property
In general, matrix multiplication does not satisfy the commutative property, i.e., 
$$AB \neq BA,$$ even when the matrix sizes allow the operation to be performed.

Example:

$$\begin{bmatrix}
1 & 2
\end{bmatrix}
\begin{bmatrix}
1\\
2
\end{bmatrix}
=
\begin{bmatrix}
5
\end{bmatrix}$$

$$
\begin{bmatrix}
1\\
2
\end{bmatrix}
\begin{bmatrix}
1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 2\\
2 & 4
\end{bmatrix}$$

## Transpose and related properties

### Definition
The **transpose** of a matrix, denoted $T$ as a superscript, exchanges the rows and columns of the matrix. More formally, the $i,j$ element of $\mathbf{A}^T$ is the $j,i$ element of $\mathbf{A}$, i.e., $(\mathbf{A}^T)_{i,j} = \mathbf{A}_{j,i}$.

Example:

$$\begin{bmatrix}
2 & 9 & 3 \\
4 & 5 & 6
\end{bmatrix}^T = 
\begin{bmatrix}
2 & 4\\
9 & 5\\
3 & 6
\end{bmatrix}.$$

### Transpose and mathematical operations

Assume that the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ have the sizes required to perform the operations below. Additionally, assume that $c\in \mathbb{R}$ is a scalar constant.

The following properties are true:

* $c^T = c$
* $(\mathbf{A} + \mathbf{B})^T = \mathbf{A}^T + \mathbf{B}^T$
* $(\mathbf{AB})^T = \mathbf{B}^T \mathbf{A}^T$, which can be extended to $(\mathbf{ABC})^T=\mathbf{C}^T \mathbf{B}^T \mathbf{A}^T$, etc.
* $(\mathbf{A}^T)^T=\mathbf{A}$

## Special matrices

### Square matrices

A matrix is **square** if the number of rows equals the number of columns. The **diagonal elements** of an $n\times n$ square matrix $\mathbf{A}$ are the elements $\mathbf{A}_{i,i}$ for $i = 1, 2, \ldots, n$. Any non-diagonal elements of $\mathbf{A}$ are called off-diagonal elements.

### Identity matrix
The $n\times n$ identity matrix $\mathbf{I}_{n\times n}$ is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so $\mathbf{I}_{n\times n}$ is often simplified to $\mathbf{I}$ or $I$.

Example:

$$\mathbf{I}_{3\times 3} = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

### Symmetric

A matrix $\mathbf{A}$ is **symmetric** if $\mathbf{A} = \mathbf{A}^T$, i.e., $\mathbf{A}_{i,j} = \mathbf{A}_{j,i}$ for all potential $i,j$.

* A symmetric matrix must be square.

### Idempotent

A matrix is **idempotent** if $\mathbf{AA} = \mathbf{A}$

* An idempotent matrix must be square.

## Matrix inverse

An $n\times n$ matrix $\mathbf{A}$ is invertible if there exists a matrix $\mathbf{B}$ such that $\mathbf{AB}=\mathbf{BA}=\mathbf{I}_{n\times n}$. The inverse of $\mathbf{A}$ is denoted $\mathbf{A}^{-1}$.

* Inverse matrices only exist for square matrices.

Some other properties related to the inverse operator:

* If $n\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ are invertible then $(\mathbf{AB})^{-1} = \mathbf{B}^{-1} \mathbf{A} ^{-1}$.
* If $\mathbf{A}$ is invertible then $(\mathbf{A}^{-1})^T = (\mathbf{A}^T)^{-1}$.

## Matrix derivatives

We start with some basic calculus results.

Let $f(b)$ be a function of a scalar value $b$ and $\frac{df(b)}{db}$ denote the derivative of the function with respect to $b$. Assume $x$ is a fixed value. Then the following is true:

$f(b)$ | $\frac{df(b)}{db}$
---|---
$bx$ | $x$
$b^2$ | $2b$
$x b^2$ | $2bx$

Now lets look at the deriviate of a scalar function with respect to a vector.

Let $f(\mathbf{b})$ be a function of a $p\times 1$ column vector $\mathbb{b}=[b_1, b_2, \ldots,  b_p]^T$. The derivative of $f(\mathbf{b})$ with respect to $\mathbf{b}$ is denoted $\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}}$ and 
$$\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}} = \begin{bmatrix}
\frac{\partial f(\mathbf{b})}{\partial b_1}\\
\frac{\partial f(\mathbf{b})}{\partial b_2}\\
\vdots \\
\frac{\partial f(\mathbf{b})}{\partial b_p}
\end{bmatrix}.$$

Assume $\mathbf{X}$ is a fixed matrix. The following is true:

$f(\mathbf{b})$ | $\frac{\partial f(\mathbf{b})}{\partial \mathbf{b}}$
---|---
$\mathbf{b}^T \mathbf{X}$ | $\mathbf{X}$
$\mathbf{b}^T \mathbf{b}$ | $2\mathbf{b}$
$\mathbf{b}^T \mathbf{X} \mathbf{b}$ | $2\mathbf{X}\mathbf{b}$


<!-- ## Matrix differentiation -->

<!-- Let $\mathbf{y} = (y_1, y_2, \ldots, y_n) -->

<!-- ## Matrix Differentiation 1 -->

<!-- Let $$\mathbf{y=Ax},$$ -->
<!-- where $\mathbf{y}$ is $m\times 1$, $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $m\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- $$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=A$$ -->

<!-- ## Matrix Differentiation 1 (Proof) -->

<!-- Since $i$th element of $\mathbf{y}$ is given by  -->
<!-- $$y_i=\sum\limits_{k=1}^{n}a_{ik}x_k,$$ -->
<!-- it follows that  -->
<!-- $$\frac{\partial y_i}{\partial x_j}=a_{ij}$$ -->
<!-- for all $i=1,\dots ,m,\quad j=1,\dots ,n$. Hence -->
<!-- $$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}=A$$ -->


<!-- ## Matrix Differentiation 2 -->

<!-- Let the scalar $\alpha$ be defined by $$\alpha =\mathbf{y}^T\mathbf{Ax},$$ -->
<!-- 	where $\mathbf{y}$ is $m\times 1$, $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $m\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$ and $\mathbf{y}$, then -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{x}}=\mathbf{y}^T\mathbf{A}$$ -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T$$ -->

<!-- ## Matrix Differentiation 2 (Proof)	 -->

<!-- Define $\mathbf{w}^T=\mathbf{y}^T\mathbf{A}$ -->
<!-- 	and note that $\alpha =\mathbf{w}^T\mathbf{x}$ -->

<!-- Hence,  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{w}^T=\mathbf{y}^T\mathbf{A}.$$ -->
<!-- 	Since $\alpha$ is a scalar we can write -->
<!-- 	$$\alpha =\alpha^T=\mathbf{x}^T\mathbf{A}^T\mathbf{y}$$ -->
<!-- 	hence,  -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{y}}=\mathbf{x}^T\mathbf{A}^T$$ -->

<!-- ## Matrix Differentiation 3 -->
<!-- For the special case in which the scalar $\alpha$ is given by the quadratic form$$\alpha=\mathbf{x}^T\mathbf{A}\mathbf{x}$$ -->
<!-- 	where  $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $n\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=\mathbf{x}^T(\mathbf{A}+\mathbf{A}^T)$$ -->

<!-- ## Matrix Differentiation 3 (Proof) -->
<!-- By definition,$$\alpha =\sum\limits_{j=1}^{n}\sum\limits_{i=1}^{n}a_{ij}x_ix_j$$ -->
<!-- 	Differentiating with respect to the $k$th element of $x$ we have -->
<!-- 	$$\frac{\partial \alpha}{\partial x_k}=\sum\limits_{j=1}^{n}a_{kj}x_j+\sum\limits_{i=1}^{n}a_{ik}x_i$$ -->
<!-- 	for all $k=1,\dots ,n$, and consequently, -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{x}}=\mathbf{x}^T\mathbf{A}^T+\mathbf{x}^T\mathbf{A}=\mathbf{x}^T(A^T+A)$$ -->

<!-- ## Matrix Differentiation 3.5 -->

<!-- For the special case where $\mathbb{A}$ is a symmetric matrix and  -->
<!-- 	$$\alpha=\mathbf{x}^T\mathbf{A}\mathbf{x}$$ -->
<!-- 	where  $\mathbf{x}$ is $n\times 1$ , $\mathbf{A}$ is $n\times n$, and $\mathbf{A}$ does not depend on $\mathbf{x}$, then  -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{x}}=2\mathbf{x}^T\mathbb{A}.$$ -->

<!-- ## Matrix Differentiation 4 -->

<!-- Let the scalar $\alpha$ be defined by$$\alpha =\mathbf{y}^T\mathbf{x}$$ -->
<!-- 	where $\mathbf{y}$ is $n\times 1$, $\mathbf{x}$ is $n\times 1$, and both $\mathbf{y}$ and $\mathbf{x}$ are functions of the vector $\mathbf{z}$. Then -->
<!-- 	$$\frac{\partial \alpha}{\partial \mathbf{z}}=\mathbf{x}^T\frac{\partial \mathbf{y}}{\partial \mathbf{z}}+\mathbf{y}^T\frac{\partial \mathbf{x}}{\partial \mathbf{z}}$$ -->

<!-- ## Matrix Differentiation 4.5 -->

<!-- Let the scalar $\alpha$ be defined by -->
<!-- 	$$\alpha =\mathbf{x}^T\mathbf{x}$$ -->
<!-- 	where $\mathbf{x}$ is $n\times 1$, and $\mathbf{x}$ is a function of the vector $\mathbf{z}$. Then -->
<!-- 	$$\frac{\partial \alpha }{\partial \mathbf{z}}=2\mathbf{x}^T\frac{\partial \mathbf{x}}{\partial \mathbf{z}}.$$ -->


<!--chapter:end:04-useful-matrix-facts.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    df_print: paged
  html_notebook: default
---

# Defining a linear model

Based on Chapter 2 of LMWR2, Chapter 2 and 3 of ALR4

## Background and terminology

Regression models are used to model the relationship between:

* one or more **response** variables and
* one or more **predictor** variables.

The distinction between these two variables is their purpose in the regression model.

* Predictor variables are used to predict the value of the response variable.

Response variables are also known as **outcome**, **output**, or **dependent** variables. 

* Response variables are modeled as random variables.

Predictor variables are also known as **explanatory**, **regressor**, **input**, **dependent**, or *feature* variables.

* Predictor variables are treated as known, fixed (non-random) variables.

Note:  Because the variables in our model are often interrelated, describing these variables as independent or dependent variables are vague and are best avoided.

A distinction is sometimes made between **regression models** and **classification models**. In that case:

* Regression models attempt to predict a numerical response.
* Classification models attempt to predict the category level a response will have.

A *linear model* is a regression model in which the regression coefficients (to be discussed later) enter the model linearly.

* A linear model is just a specific type of regression model.

## Types of regression

* *Simple* regression model: a regression model with one constant predictor and one non-constant predictor.
* *Multiple* regression model: a regression model with more than one non-constant predictor.
* *Multivariate* regression model: a regression model with more than one response variable.
* *Linear* regression model: a regression model in which the regression coefficients enter the model linearly.
* *Analysis of variance (ANOVA)* model: a linear regression model with one or more categorical predictors.
* *Analysis of covariance (ANCOVA)*: a linear regression model with a quantitative predictor and a categorical predictor.
* *Generalized linear model (GLM)*: a type of "generalized" regression model when the responses do not come from a normal distribution.

## Goals of regression

The basic goals of a regression model are to:

1. *Predict* future or unknown response values based on specified values of the predictors.
    * What will the selling price of a home be?
2. *Identify relationships* (associations) between predictor variables and the response.
    * What is the general relationship between the selling price of a home and the number of bedrooms the home has?

With our model, we also hope to be able to:

1. *Generalize* our results from the sample to the a larger population of interest.
    * E.g., we want to extend our results from a small set of college students to all college students.
2. *Infer causality* between our predictors and the response.

    * E.g., if we give a person a vaccine, then this causes the person's risk of catching the disease to decrease.

A note about models: 

There is no such thing as a "true model" for real data.

The general goal of a regression analysis is to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.)

## Definition of a linear model

Let $Y$ denote a response variable, $X_1, X_2, \ldots, X_{p-1}$ denote $p-1$ predictor variables, and $\epsilon$ denote a random variable we call **error**. 

Then a **linear model** for $Y$ is defined by the equation
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1} + \epsilon,
(\#eq:lmdef)
\end{equation}
where $\beta_0, \beta_1, \ldots, \beta_{p-1}$ are known as **regression coefficients**.

The model in Equation \@ref(eq:lmdef) is a **statistical model** because there is uncertainty in the response. 

Suppose we have sampled $n$ observations from a population. The response values are denoted $Y_1, Y_2, \ldots, Y_n$. The value of predictor $j$ for observation $i$ is denoted by $x_{i,j}$. 

Let $Y_1, Y_2, \ldots, Y_n$ denote the observed values of the response variable. . We have also measured $p-1$ predictor variables $X_1, X_2, \ldots, X_{p-1}$ denote $p-1$ predictor variables. The error for the $i$th observation is denoted $\epsilon_i$.

The linear model for the responses is defined by the system of equations
\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
(\#eq:lmSystem)
\end{equation}

The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices.

Let $\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]^T$ be a column vector containing the $n$ responses, $$\mathbf{X} = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \cdots & x_{1,p-1} \\
1 & x_{2,1} & x_{2,2} & \cdots & x_{2,p-1} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{n,1} & x_{n,2} & \cdots & x_{n,p-1}
\end{bmatrix}$$
be a matrix containing the observed predictor values (and a column of 1s),
and $\mathbf{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]^T$ be a column vector contained the $n$ errors, and $\mathbf{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]^T$ be a column vector containing the $p$ regression coefficients. Then the system of equations defining the linear model in \@ref(eq:lmSystem) can be written as 
$$\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}.$$
Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model. 

## Summarizing the components of a linear models

Before we progress any further, it is very informative to consider some properties of the objects discussed in Equation  \@ref(eq:lmSystem), specifically, with respect to whether they are observable and whether they are model is random variables. 

We've already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable.

On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. Our analyses and inference will always be conditional on the predictor variables, so we treat them as non-random. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation  \@ref(eq:lmSystem) is for the errors to be random.

We summarize this information in the table below for the objects previously discussed using the various notations introduced.

Notation | Description | Observable | Random
--- | --- | --- |--- 
$Y$ | response variable | Yes | Yes
$X$ | predictor variable | Yes | No
$Y_i$ | response value for the $i$th observation | Yes | Yes
$X_j$ | the $j$th predictor variable | Yes | No
$x_{i,j}$ | the value of the $j$th predictor variable for the $i$th observation | Yes | No
$\beta_j$ | the regression coefficient associated with the $j$th predictor variable | No | No
$\epsilon_i$ | the error associated with observation $i$ | No | Yes
$\mathbf{y}$ | the $n\times 1$ column vector of response values | Yes | Yes
$\mathbf{X}$ | the $n\times p$ matrix of predictor values | Yes | No
$\mathbf{\beta}$ | the $p\times 1$ column vector of regression coefficients | No | No
$\mathbf{\epsilon}$ | the $n\times 1$ column vector of errors | No | Yes

<!-- ## Scatter plots and linear regression -->

  <!-- Scatter plots are a convenient way to study the potential relationship between a single response and a single predictor variable. -->

  <!-- ### Height inheritability -->

  <!-- Karl Pearson (1857-1936) organized the collection of $n=1375$ heights of mothers in the United Kingdom under the age of 65 and one of their adult daughters over the age of 18.  These data are available in the `Heights` data set in the **alr4** package. We are interested in the inheritance from the mother to the daughter, so the mother's height (`mheight`) is used as the predictor variable and the daughter's height (`dheight`) is used as the response variable. -->

  <!-- Questions of interest: -->

  <!-- * Do taller mothers tend to have taller daughters -->
  <!-- * Do shorter mothers tend to have shorter daughters? -->

  <!-- ```{r} -->
  <!-- data(Heights, package = "alr4") -->
  <!-- str(Heights) -->
  <!-- plot(dheight ~ mheight, data = Heights, -->
              <!--      xlab = "mother's height (in)", -->
              <!--      ylab = "daughter's height (in)", -->
              <!--      xlim = c(55, 75), ylim = c(55, 75)) -->
  <!-- ``` -->
  <!-- There seems to be a clear trend between mother's heights and daughter's heights. The taller the mother, the taller the daughter *tends* to be. -->

  <!-- ### Predicting snowfall -->

  <!-- The `ftcollinssnow` data set in the **alr4** package measures late (September 1st until December 31st) and early (January 1st to June 30th) season snowfall for Fort Collins, CO between Late 1900 and Early 1993. -->

  <!-- Question of interest: Can late season snowfall predict snowfall in the early part of the next year? -->


  <!-- ```{r} -->
  <!-- data("ftcollinssnow", package = "alr4") # load data -->
<!-- str(ftcollinssnow) # examine structure -->
<!-- plot(Late ~ Early, data = ftcollinssnow) # plot data -->
<!-- # add "line of best fit" -->
  <!-- abline(lm(Late ~ Early, data = ftcollinssnow), lty = 2) -->
  <!-- # sample mean line -->
  <!-- abline(mean(ftcollinssnow$Late), 0) -->
  <!-- ``` -->

  <!-- A plot of the snowfall data for the two time periods suggest that this relationship is weak or they may be uncorrelated. -->
  <!-- * The dashed line indicates the "linear of best fit" , while the solid line indicates the average of the Late snowfall. -->

  <!-- ### Turkey growth -->
  <!-- Pens of turkeys were fed the same diet, except that each pen was supplemented with a `Dose` of amino acid methionine as a percentage of the total diet of the birds. The amino acid methionine was provided using three different `Source`s (one standard and two experimental). The `Weight` gain (g) of the turkeys was measured. These data are available in the `turkey` data in the **alr4** package. -->

  <!-- Questions of interest: -->

  <!-- * Is there a relationship between weight gain of the turkeys and the dose amount? If so, is the relationship linear? -->
  <!-- * Does the source of the methionine impact the weight gain of the turkeys? -->

  <!-- Consider a plot of the average `Weight` gain (g) of the turkeys as a function of the `Dose` amount (% of diet), separating the groups by the `Source` of the methionine. -->

  <!-- ```{r} -->
  <!-- data(turkey, package = "alr4") -->
  <!-- str(turkey) -->
  <!-- summary(turkey) # the source factor (S) is not a factor -->
<!-- turkey$S = factor(turkey$S) -->
  <!-- levels(turkey$S) <- c("control", "new source a", "new source b") -->
  <!-- names(turkey) <- c("Dose", "Gain", "Source", "Replications", "SD") # rename variables -->
<!-- # create turkey data ggplot -->
  <!-- library(ggplot2) # load ggplot2 package -->
<!-- gg_turkey <- ggplot(turkey, -->
                           <!--                     mapping = aes(x = Dose, y = Gain, -->
                                                                    <!--                                   color = Source, shape = Source)) -->
  <!-- gg_turkey + geom_point() + geom_line() -->
  <!-- ``` -->

  <!-- Weight gain increases with dose amount, but doesn't appear to be linear. -->

<!-- The amino acid source may slightly affect the growth trajectory of the turkeys. -->

<!-- An alternative version of the previous plot using the **lattice** package -->

<!-- ```{r} -->
<!-- library(lattice) # load lattice package -->
<!-- xyplot(Gain ~ Dose, data = turkey, groups = Source, -->
<!--        auto.key = TRUE, type = "b") -->
<!-- ``` -->

<!--chapter:end:05-defining-a-linear-model.Rmd-->

---
title: "Joshua French"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: html_notebook
---

## Simple Linear Regression

The response variable $Y$ is mod

We will let $Y$ denote the response variable and $X$ the regressor variable.

* $Y$



### Model description
The simple linear regression model is described by the mean function 
$$E(Y│X=x)=\beta_0+\beta_1 x,$$

and variance function

$$var(Y│X=x)=\sigma^2,$$
where:

* $Y$ is the response variable
* $X$ is a regressor variable
* $\beta_0$ and $\beta_1$ are known as *regression parameters* or *regression coefficients*.

Note: 

* The values that $Y$ takes are modeled as random variables.
* The values that $X$ takes are modeled as known, non-random values.

### Interpreting the regression coefficients

* The *intercept*, $\beta_0$, is the mean response when $X=0$.
  * i.e., $\beta_0=E(Y|X=0)$.
* The *slope*, $\beta_1$, is the mean change in the response when $X$ increases by 1 unit.
  * i.e., $\beta_1 = E(Y|X=x+1)-E(Y|X=x)$.

```{r}
plot(c(-1, 3), c(-1, 3), type = "n", xlab = expression(italic(X)), ylab = expression(italic(Y)), asp = 1)
abline(a = 1, b = 1)
lines(c(0, 0.25), c(0, 0))
lines(c(0, 0.25), c(1, 1))
lines(c(0.25, 0.25), c(0, 1))
lines(c(0.25, 0.5), c(0.5, 0.5))
text(0.5, 0.5, expression(italic(beta)[0]), pos = 4)
lines(c(1, 2), c(2, 2))
lines(c(2, 2), c(2, 3))
lines(c(2, 2.25), c(2.5, 2.5))
text(2.25, 2.5, expression(italic(beta)[1]), pos = 4)

``
# 
# ## Simple Linear Regression
# 
# The simple linear regression model consists of the mean function 
# $$E(Y│X=x)=\beta_0+\beta_1 x,$$
# and variance function
# 
# $$var(Y│X=x)=\sigma^2,$$
# where:
# 
# - $Y$ is the response
# - $X$ is a regressor variable
# - $\beta_0$ and $\beta_1$ are known as **regression parameters** or **coefficients**.
# 
# # Simple Linear Regression
# 
# Chapter 2 of LMWR2, Chapter 2 and 3 of ALR4
# 
# 
# ## Simple Linear Regression
# 
# The simple linear regression model consists of the mean function 
# $$E(Y│X=x)=\beta_0+\beta_1 x,$$
# 
# and variance function
# 
# $$var(Y│X=x)=\sigma^2,$$
# where:
# 
# - $Y$ is the response
# - $X$ is a regressor variable
# - $\beta_0$ and $\beta_1$ are known as **regression parameters** or **coefficients**.
# 
# ## Understanding the model
# ```{r}
# set.seed(1)
# x = runif(10, 60, 100)
# y = x + rnorm(10)
# plot(x,y, pch=20, xlab = 'Midyear Evaluation', ylab = 'Year-End Evaluation', main = 'Y = -0.74 +1.01 X')
# abline(lm(y~x), lwd = 2, col = 'red')
# ```
# 
# 
# ## Understanding the model 
# ```{r out.width='1000px'}
# library(knitr)
# include_graphics('images/simple_lin_reg.png')
# ```
# 
# ## Understand Coefficients
# 
# ```{r}
# include_graphics('images/coefficients.png', dpi=80)
# ```
# 
# ## View from single observation
# Assume that we have n observations or cases for which the response and predictor variables are measured.
# 
# -	The responses are denoted $y_1,y_2,\dots ,y_n$.
# -	The regressor values for regressor X are denoted $x_1,x_2,\dots,x_n$.
# 
# Each response will deviate from its associated mean, so our statistical model must include an additional source of variation.
# 
# The statistical model for each response is
# $$y_i=\beta_0+\beta_1 x_i+\epsilon_i,\quad   i=1,2,\dots,n,$$
# 
# where $\epsilon_i$ denotes the deviation of $y_i$ from its mean.
# 
# -	The $\epsilon_i$ are known as errors.
# 
# ## Conditions on Error
# 
# Conditional on knowing the regressor values, the errors have: 
# 
# -	Mean 0
# -	Variance $\sigma^2$ (constant)
# -	And are uncorrelated.
# 
# Mathematically, this is the same as: 
# 
# -	$E(\epsilon_i│X=x_i )=0$ 
# -	$var(\epsilon_i│X=x_i )=\sigma^2$
# -	$cov(\epsilon_i,\epsilon_j )=0$  when $i\neq j$.
# 
# ## What about the response?
# 
# - Mean:
# 
#   $$E[Y_i|X_i] = E[\beta_0+\beta_1X_i + \epsilon_i]= \beta_0 + \beta_1X_i$$
#   $$Var(Y_i|X_i) = Var(\beta_0+\beta_1X_i + \epsilon_i) = Var(\epsilon_i) = \sigma^2$$
#   $$cov(Y_i, Y_j) = 0,\quad i \neq j$$
#   The responses $Y_i$ come from probability distributions whose means are $\beta_0+\beta_1X_i$ and whose variances are $\sigma^2$, the same for all levels of $X$. Further, any two responses $Y_i$ and $Y_j$ are uncorrelated. 
#   
# ## Example
# 
# ```{r}
# include_graphics('images/simple_lin_reg_values.png',dpi = 100)
# ```

<!--chapter:end:06-fitting-a-linear-model.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

