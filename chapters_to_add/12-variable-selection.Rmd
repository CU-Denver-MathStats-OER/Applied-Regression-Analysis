---
title: "Variable Selection Part 1"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
  df_print: paged
---

  # Model Selection: What is the right number of regressors we should include?

  Variable selection is intended to (objectively) find the "best" subset of predictors. So why not throw the whole kitchen sink into our model?

  ## Motivating Example: Predicting Life Expectancy

  The `state` datasets in the base R package (no package needed to access the data) contains various data sets with data from all 50 states. We'll be working with the dataset `state.x77` which has 50 observations (one for each state) with the following variables of interest:

- `Population`: Population estimate as of July 1, 1975
- `Income`: Per capita income (1974)
- `Illiteracy`: Illiteracy rate (1970, percent of population)
- `Life Exp`: life expectancy in years (1969–71)
- `Murder`: murder and non-negligent manslaughter rate per 100,000 population (1976)
- `HS Grad`: percent high-school graduates (1970)
- `Frost`: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city
- `Area`: land area in square miles

## Loading the Data

```{r}
data(state)
statedata <- data.frame(state.x77, row.names = state.abb)
#head(statedata)
summary(statedata)
```

**Which regressors should we include if we want to predict `Life Exp` as the response variable?**

## Question 1: Why might it be a bad a idea to simply include all regressors?



## Question 2: What metrics/methods have we used to compare models thus far?


# Variable Selection

There are two aspects to variable selection:

- The strategy used to search for the "optimal" model.
- The criterion used to compare models.

## Testing-Based Procedures

## Backward elimination

**Backward elimination** is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed.

## Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision?

```{r}
lmod <- lm(Life.Exp ~ ., data = statedata)
faraway::sumary(lmod)
```



```{r}
back1 <- update(lmod, . ~ . - Area)
faraway::sumary(back1)
```


- This does not imply the other variables are not related to the response.

```{r}
lmod1 <- lm(Life.Exp ~ Illiteracy, data = statedata)
faraway::sumary(lmod1)
```

## Forward Selection

**Forward selection** starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor.

- For example, first add the predictor with the smallest $p$-value.
- Then compare models with the first predictor plus a second predictor and add the predictor which has the smallest $p$-value.

## Stepwise Regression

**Stepwise regression** is a combination of backward elimination and forward selection.

- This addresses the situation where variables are added or removed early in the process and we want to change our mind.
Stepwise selection can miss the optimal model because we do not consider all possible models due to the one-at-a-time nature of adding/removing regressors.
- $p$-values should not be taken as very accurate in stepwise searches because we are bound to see small $p$-values due to chance alone.
- Stepwise selection tends to produce simpler models that are not necessarily the best for prediction.

## Model Hierarchy

We must respect hierarchy in models when it is naturally present.

- In polynomial models, $X^2$ is a higher order term than $X$.
- A lower order term should be retained if a higher order term is retained to increase the flexibility.
	- The model $Y=\beta_0+\beta_2 X^2+\epsilon$, the maximum/minimum value MUST occur $x=0$
  - For the model $y=\beta_0+\beta_1 X+\beta_2 X^2+\epsilon$, the maximum/minimum value can occur anywhere along the real line (depending on what the data suggest).
- If we fit the model $y=\beta_0+\beta_1 X+\beta_2 X^2+\epsilon$ and $\beta_1$ is not significant, it would NOT make sense to remove $X$ from the model but still keep $X^2$.

## Criterion-Based Procedures

**Akaike’s Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)** are two information-based criteria for variable selection.

## Akaike’s Information Criterion (AIC)

$\mbox{AIC}(\mathcal{M})=-2L(\mathcal{M})+2p_{\mathcal{M}}$, where $\mathcal{M}$ is the model, $L(\mathcal{M})$ is the **log-likelihood** of the model using the **MLE** estimates of the parameters, and $p_{\mathcal{M}}$ is the number of regression coefficients in model $\mathcal{M}$.

For linear regression models, $-2L(\mathcal{M})=n\log{(\mbox{RSS}_{\mathcal{M}}/n)} + c$, where $c$ is a constant that depends only on the observed data and not on the model, and $\mbox{RSS}_{\mathcal{M}}$ is the RSS of model $\mathcal{M}$. The constant $c$ is the same for a given data set, so they can be ignored when comparing models that based on the same data set.

### Interpreting AIC

The formula for AIC is derived from a metric that can be used to measure how far a model is from the true model.

- As $\mbox{RSS}_{\mathcal{M}}$ gets smaller (better fit), $n\log{(\mbox{RSS}_{\mathcal{M}}/n)}$ gets smaller (becomes more negative).
  - Adding more predictors (that are not collinear) will improve the fit.
- As $p_{\mathcal{M}}$ gets bigger, the second term of AIC gets larger.
  - The second component penalizes the model according its complexity.
  - The more parameters, the larger the penalty.
- Models with more parameters will fit better (reducing the RSS), but will be penalized more for having additional parameters.
- AIC provides a balance between fit and simplicity.
  - AIC identifies good fitting models (small RSS) that are simple (not a lot of predictors).
- **We choose the model the minimizes the AIC**.

### Exhaustive Model Searches

The `leaps` package searches all possible combinations of predictors.

- For each value of $p$ (number of predictors), it finds the variables that give the minimum RSS.
- For each value of $p$, the model that minimizes the RSS will have the smallest AIC, BIC, adjusted $R_a^2$, and Mallow’s $C_p$ (we'll discuss these soon).
- By default, `regsubsets` only goes up to $p=9$.  You have to set `nvmax = j`, where $j$ is the number of regressors you want to consider.


```{r}
# may need to install.package the first time
library(leaps) # you need to load package every time you want to use it

# model selection by exhaustive search
b <- regsubsets(Life.Exp ~ ., data = statedata)
rs <- summary(b) # summarize model that minimizes RSS for each p
rs$which # nicer output
```


## Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination?


### Computing the AIC

```{r}
# What output is stored after running regsubsets
summary(rs)
```

```{r}
n <- nrow(statedata) #number observation n=50
rss <- rs$rss # rss calculated for each model

# Compute AIC using the formula
AIC <- n * log(rss/n) + (2:8)*2  # we start at 2 since include intercept in all
plot(AIC ~ I(1:7), ylab = "AIC", xlab = "Number of Predictors", pch = 16)
```


### Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric?

## Bayesian Information Criterion (BIC)

The Bayesian Information Criterion (BIC) is another criteria that is often and is almost the same as AIC.


$$BIC(\mathcal{M})=-2L(\mathcal{M})+\log{(n)}p_{\mathcal{M}}.$$


  ```{r}
(BIC <- rs$bic) # Exactly values from rs summary
#(BIC2 <- n * log(rss/n) + log(n)* (2:8))  # Using the formula
#BIC - BIC2 # The two differ by a constant
plot(BIC ~ I(1:7), ylab = "BIC", xlab = "Number of Predictors", pch = 16)
```

## Question 6: Interpret the output from the BIC plot above. What is the best model according to this metric?


- The `car` package has a `subsets` function that takes the generates nice, labeled BIC plots generated from the `regsubsets` function.

```{r}
library(car)
subsets(b, statistic = "bic", legend = FALSE)
```


-------

  # Appendix

  ## Maximum Likelihood Estimates (MLE)

  The **likelihood function** $L(\theta)= L( \theta \mid x_1, x_2, \ldots x_n)$ gives the likelihood of the parameter $\theta$ given the observed data. A **maximum likelihood estimate (MLE)**, $\mathbf{\hat{\theta}_{\rm MLE}}$,}} is
a value of $\theta$ that maximizes the likelihood function.

**MLE is a process for finding the best parameter(s) for a model based on a given dataset**


  Let $f(x; \theta)$ denote the pdf of a random variable $X$ with associated parameter $\theta$. Suppose $X_1, X_2, \ldots , X_n$ are random samples from this distribution, and $x_1, x_2, \ldots , x_n$ are the
corresponding observed values.

$$ L(\theta \mid x_1, x_2, \ldots , x_n) = f(x_1; \theta) f(x_2; \theta) \ldots f(x_n; \theta) = \prod_{i=1}^n f(x_i; \theta).$$

  ### Example: Finding and MLE

  Find the MLE for $\lambda$ where $x_1, x_2, \ldots , x_n$ comes from $X \sim \mbox{Exp}(\lambda)$ with $f(x; \lambda) = \lambda e^{-\lambda x}$.

1. Find a formula for the likelihood function.

$$ L(\lambda \mid x_1, x_2, \ldots , x_n) = \left(\lambda e^{-\lambda x_1} \right)\left(\lambda e^{-\lambda x_2} \right) \ldots \left(\lambda e^{-\lambda x_n} \right) = \lambda^n e^{-\lambda \sum_{i=1}^n x_i} .$$

  2. Optimize the likelihood function. Find the value of $\lambda$ that makes the observed data most likely to occur.

$$\frac{d}{d \lambda} \left( \lambda^n e^{-\lambda \sum_{i=1}^n x_i} \right) $$

  Often this is really messy to solve. Taking the natural log of both sides often simplifies the calculation.

The **log-likelihood** function is $y = \ln{L(\theta \mid x_1, x_2, \ldots , x_n) }$ (often written with $\log$ though we mean $\ln$).

- Since the natural log is an increasing function, the value of $\theta$ that maximizes (or minimizes) $L(\theta \mid x_1, x_2, \ldots , x_n)$ is the same value of $\theta$ that maximizes (or minimizes) $y = \ln{L(\theta \mid x_1, x_2, \ldots , x_n) }$.

$$y = \ln{\left(\lambda^n e^{-\lambda \sum_{i=1}^n x_i}\right)} = n \ln{(\lambda)}- \lambda \sum_{i=1}^n x_i$$
  It actually is easier to optimize the log-likelihood function in this case:

  $$
  \begin{aligned}
\frac{d}{d \lambda} \ln{\left(\lambda^n e^{-\lambda \sum_{i=1}^n x_i}\right)} &= \frac{d}{d \lambda} \left( n \ln{(\lambda)}- \lambda \sum_{i=1}^n x_i \right) \\
&= \frac{n}{\lambda} - \sum_{i=1}^n x_i
\end{aligned}
$$

  We have a critical value at $\lambda = \frac{\sum x_i}{n} = \bar{x}$ which is the value of $\lambda$ that maximizes the likelihood function. If we assume the sample was randomly selected from an exponential distribution, then given the observed data, the most likely value for $\lambda$ is $\bar{x}$. This makes practical sense since if $X \sim \mbox{Exp}(\lambda)$, $E(X) = \mu = \lambda$.

### Pros of Using MLE's to Estimate Population Parameters

- MLE's give estimates that make practical sense (see example above).
- **Consistency**: As the sample size gets larger and larger, MLE's converge to the actual value of the parameter.
- **Normality**: As we get more data, MLE's converge to a normal distribution.
- **Efficiency**: They have the smallest possible variance for a consistent estimator.

## Model Selection: What is the right number of regressors we should include?

Variable selection is intended to (objectively) find the "best" subset of predictors. So why not throw the whole kitchen sink into our model?

## Motivating Example: Predicting Life Expectancy

The `state` datasets in the base R package (no package needed to access the data) contains various data sets with data from all 50 states. We'll be working with the dataset `state.x77` which has 50 observations (one for each state) with the following variables of interest:

  - `Population`: Population estimate as of July 1, 1975
- `Income`: Per capita income (1974)
- `Illiteracy`: Illiteracy rate (1970, percent of population)
- `Life Exp`: life expectancy in years (1969–71)
- `Murder`: murder and non-negligent manslaughter rate per 100,000 population (1976)
- `HS Grad`: percent high-school graduates (1970)
- `Frost`: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city
- `Area`: land area in square miles

## Loading the Data

```{r}
data(state)
statedata <- data.frame(state.x77, row.names = state.abb)
#head(statedata)
summary(statedata)
```

```{r}
lmod <- lm(Life.Exp ~ ., data = statedata)
faraway::sumary(lmod)
```

## Recap from Last Class

- Choosing more variables is not always preferable.
- A solid yet simple model is often preferred.
- When deciding how many predictors to include (exclude), its complicated! We should consider several criteria.


## Testing Based Procedures

- **Backward elimination** is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed.
- **Forward selection** starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor.
- **Stepwise regression** is a combination of backward elimination and forward selection.


```{r}
# This handy function does stepwise regression
# Evaluation based on AIC
step(lmod, direction = "both")
```

## Search Strategies

An exhaustive search looks at all possible models using all available regressors.

- This is not feasible unless the number of regressors is relatively small.
- If the number of regressors (including the intercept) is $p$, there are $2^p$ possible models.

Because of our error criteria, our search often simplifies to finding the model that minimizes $\mbox{RSS}_{\mathcal{M}}$ for each value of $p_{\mathcal{M}}$. This is the best subset searching strategy.

### Finding the Best Subsets

The `leaps` package performs a thorough search for the best subsets of predictors for each model size.

- Since the algorithm returns a best model for each size, the results do not depend on the a penalty model (such as AIC and BIC).
- For each model size ,it finds the variables that give the minimum RSS.
- By default, `regsubsets` only goes up to $p=9$.  You have to set `nvmax = j`, where $j$ is the number of regressors you want to consider.


```{r}
# may need to install.package the first time
library(leaps) # you need to load package every time you want to use it

# model selection by best subset search
best <- regsubsets(Life.Exp ~ ., data = statedata)
bsum <- summary(best) # summarize model that minimizes RSS for each p
bsum$which # nicer output
```

```{r}
# What output is stored after running regsubsets
summary(bsum)
```


## Review of Criterion-Based Procedures Thus Far

- RSS (and $R^2$) is a measurement of the error between the data and a model.
- RSS will decrease when we add more predictors, regardless if they predict anything.
- Therefore $R^2 = 1 - \mbox{RSS}/\mbox{TSS}$ increases, regardless.
- $p$-values should not be taken as very accurate in stepwise or best subset searches because we'll see small $p$-values due to chance alone.

- So we shouldn't just consider RSS or $R^2$since we'll always choose the most complicated model.
- We shouldn't just consider $p$-values since we will always get false positives.
- We only want to add predictors if they significantly help improve the prediction.


## Akaike’s Information Criterion (AIC) and Bayesian Information Criteria (BIC)

$$\mbox{AIC}(\mathcal{M})= n\log{(\mbox{RSS}_{\mathcal{M}}/n)} +2p_{\mathcal{M}} +c.$$
  $$\mbox{BIC}(\mathcal{M})= n\log{(\mbox{RSS}_{\mathcal{M}}/n)} + \log{(n)} p_{\mathcal{M}} +c.$$

  - Both AIC and BIC are criteria that balance fit and complexity.
- As $RSS$ goes down (yay!), AIC and BIC goes down.
- As $p_{\mathcal{M}}$ goes up, there is a penalty for making things more complicated.
- BIC assigns a bigger penalty for adding more predictors, so it will slightly favor simple models to complex models (compared to AIC).
- The constant $c$ is the same for all models created from the same data, so it can be ignored.
- **We choose the model the minimizes the AIC and/or BIC**.


```{r}
# Storing values we'll use
p <- 2:8 # number of predictors (including intercept)
n <- nrow(statedata) # n=50 observations
rss <- bsum$rss # rss of each best subset
```

```{r}
BIC <- bsum$bic # Exactly values from rs summary
plot(BIC ~ p, ylab = "BIC", xlab = "Number of Predictors (incl intercept)", pch = 16)
```


- The `car` package has a `subsets` function that takes the generates nice, labeled BIC (or other statistics, not AIC though) plots generated from the `regsubsets` function.


```{r}
library(car)
subsets(best, statistic = "bic", legend = FALSE) # stat can be “bic”, “cp”, “adjr2”, “rsq”, “rss”
```

```{r}
AIC <- BIC + p * (2 - log(n)) # Compute AIC from BIC
plot(AIC ~ p, ylab = "BIC", xlab = "Number of Predictors (incl intercept)", pch = 16)
```

### Optional if You Want to Compare With Formulas


```{r, eval = FALSE}
# This computes BIC from the formula (ignoring the constant c)
BIC2 <- n * log(rss/n) + log(n) * p  # include the intercept when giving p
BIC - BIC2  # This tells you what the constant c is.
```


```{r, eval = FALSE}
# This computes AIC from the formula (ignoring the constant c)
AIC2 <- n * log(rss/n) + 2 * p  # include the intercept when giving p
AIC - AIC2  # This tells you what the constant c is.
```


## Adjusted $R^2$

The **adjusted $R^2$** is another criterion that penalizes for the number of parameters in the model. Adjusted $R^2$, $R_a^2$** is a better criterion for assessing model fit than $R^2$.

For model $\mathcal{M}$ with $p_{\mathcal{M}}$ regression coefficients,

$$R_a^2=1- \frac{\mbox{RSS}_\mathcal{M}/(n-p_\mathcal{M})}{\mbox{TSS}/(n-1)} = 1 - \left(\frac{n-1}{n-p_{\mathcal{M}}} \right) \left( 1-R^2 \right) = 1 - \frac{\hat{\sigma}^2_{\mathcal{M}}}{\hat{\sigma}^2_{\rm null}}.$$

  **Adding a regressor to a model only increases $R_a^2$ if the regressor has some predictive value.**

  - Minimizing the variance of the prediction error amounts to minimizing $\hat{\sigma}^2_{\mathcal{M}}$.
- The smaller that $\hat{\sigma}^2_{\mathcal{M}}$  becomes the larger $R^2_a$ becomes.
- **We favor models that produce larger $R_a^2$.**

  ### Computing the Adjusted $R^2$

  ```{r}
#faraway::sumary(lmod) #gives R^2 for full model
#summary(lmod) # both R^2 and R_a^2 for full model
(adjr <- bsum$adjr) #pulls R_a^2 from regsubsets for each subset
```

```{r}
plot(adjr ~ p, ylab = expression({R^2}[a]),
     xlab = "Number of Predictors", pch = 16)
```

```{r}
subsets(best, statistic = "adjr2", legend = FALSE)
```

## Mean Square Error (MSE)

The **Mean Square Error (MSE)** of an estimator measures the average squared distance between the estimator and the parameter:

  $$\mbox{MSE} (\hat{\theta})  = E \left( (\hat{\theta} - \theta)^2 \right) = \mbox{Var} (\hat{\theta}) + \left( \mbox{Bias}(\hat{\theta})\right)^2$$

  - MSE is a criterion the combines bias and efficiency.
- If two estimators are unbiased, one is more efficient than the other if and only if it has a smaller MSE.
- We favor models with smaller mean squared error, but the search algorithm is very important, otherwise you just use the model with the most regressors.


## Mallow's $C_p$ Statistic

Mallow’s $C_p$ statistic is a criterion designed to quantify the predictive usefulness of a model. Mallow’s $C_p$ statistic is used to estimate the average **mean square error** of the prediction,

$$ \frac{1}{\sigma^2} \sum_i MSE(\hat{y}_i) =  \frac{1}{\sigma^2} \sum_iE\big( (\hat{y}_i - E(y_i))^2 \big)$$

  The average of the mean square errors can be approximated by Mallow's $C_p$ Statistic:

$$C_{p_{\mathcal{M}}} = \frac{\mbox{RSS}_{\mathcal{M}}}{\hat{\sigma}^2} + 2p_{\mathcal{M}} - n$$.

- For the model with all regressors (model $\Omega$ with $p_{\Omega}$ regression coefficients), we have $C_{p_{\Omega}}=p_{\Omega}$
- If a model with $p_{\mathcal{M}}$ regression coefficients fits the data well and has little or no bias, then $E(C_{p_{\mathcal{M}}}) \approx p_{\mathcal{M}}$.
  - A model with a biased fit will have $C_{p_{\mathcal{M}}}$ much larger than $p_{\mathcal{M}}$.
  - Models with $C_{p_{\mathcal{M}}}$ less than $p_{\mathcal{M}}$ do not show evidence of bias.
- It is common to plot $C_{p_{\mathcal{M}}}$ versus $p_{\mathcal{M}}$ and compare this to $45^{\circ}$ line $C_{p_{\mathcal{M}}}= p_{\mathcal{M}}$ .
- **We favor models with small $p_{\mathcal{M}}$ and $C_{p_{\mathcal{M}}}$ close to $p_\mathcal{M}$.**

### Computing Mallow's $C_p$ Statistic

```{r}
cp <- bsum$cp # Display the C_p for each value of p
plot(cp ~ p, ylab = expression({C_p}),
     xlab = "Number of Predictors", pch = 16)
abline(0,1) # plots line y=x
```


```{r}
subsets(best, statistic = "cp", legend = FALSE)
abline(0,1)
```

### Question 7: Interpret the output from the $C_p$ plot above. What is the best model according to this metric?

- Four predictors (including the intercept) seems about right.
- Five predictors could be a suitable choice too.


```{r}
bmod <- lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
```

### Root Mean Squared Error

The **RMSE (root mean squared error)** is simply the square root of the MSE, and is sometimes used in place of the MSE.

- The RMSE or MSE will produce identical variable selection results since they are 1-1 transformations of each other.

## Cross-validation

In the previous example, we can pat ourselves on the back and say we removed four predictors and that causes only a minor reduction in fit. Well done, but a better question might be: **what would the effect of removing these variables be on a new independent sample?**

  - Well, we just used all of our sample data to construct this model.
- We need to see how well our data does with new data (not used in construction of the model).
- How can we see how good our model works?

  **Cross-validation**  breaks the data into a **training dataset** and a **test dataset** to get a more accurate assessment of the predictive accuracy of a model.

1. A model is fit to the training dataset.
2. The fitted model is used to predict the responses of the test dataset.
3. An error criterion (e.g, the MSE) is calculated for the test dataset.

**When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE (or RMSE).**

  ## Methods For Splitting the Data

  There are many variations of how to choose the training and testing datasets for crossvalidation.

### Leave-One Out Crossvalidation

**Leave-one-out crossvalidation** uses each observation (individually) as a test data set, using the other $n-1$ observations as the training data.


#### Should We Inlcude Population?

```{r}
ersq <- numeric(n)

for (i in 1:n){
  train.ds <- statedata[-i, ]
  test.ds <- statedata[i, ]
  tmod <- lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = train.ds)
  predy <- predict(tmod, new = test.ds)
  y <- test.ds[1,4]
  ersq[i] <- (predy - y)^2
}

(tmse <- sum(ersq))
```

```{r}
ersq <- numeric(n)

for (i in 1:n){
  train.ds <- statedata[-i, ]
  test.ds <- statedata[i, ]
  tmod <- lm(Life.Exp ~ Murder + HS.Grad + Frost, data = train.ds)
  predy <- predict(tmod, new = test.ds)
  y <- test.ds[1,4]
  ersq[i] <- (predy - y)^2
}

(tmse <- sum(ersq))
```


### $k$-Fold Crossvalidation

**$k$-fold crossvalidation** breaks the data into $k$ unique sets.

- For each set, the other $k-1$ sets are used as training data, and then the fitted model is used to predict the responses for the $k$th testing set.
- We must fit $k$ models to determine the mean squared error.

## Example of $k$-fold Crossvalidation

Let's comparison the full model to model with `Population`, `Murder`, `HS.Grad`, and `Frost` predictors using the RMSE criterion and both 10-fold crossvalidation and leave-one-out crossvalidation.

The `caret` package (short for Classification And REgression Training) contains functions to streamline the model training process for regression and classification problems.

```{r}
library(caret)

# define training/test (control) data
cv_10fold <- trainControl(method="cv", number = 10) # 10-fold crossvalidation train/test data

# Set up fill and model with our 4 regressors
f1 = Life.Exp ~ . # formula for full model
f2 = Life.Exp~Population + Murder + HS.Grad + Frost

# Using training data to construct each model
modela <- train(f1, data = statedata, trControl=cv_10fold, method = "lm") #full
modelb <- train(f2, data = statedata, trControl=cv_10fold, method = "lm") #with 4 reg
print(modela) # full, 10-fold
```

```{r}
print(modelb) # reduced, 10-fold
```

```{r}
# leave-one-out crossvalidation train/test data
cv_loo <- trainControl(method="LOOCV")

modelfull <- train(f1, data = statedata, trControl=cv_loo, method = "lm") #full
modelred <- train(f2, data = statedata, trControl=cv_loo, method = "lm") #with 4 reg
print(modelfull) # full, leave one out
```

```{r}
print(modelred) # reduced, leave one out
```


## There's Still More to Consider!

  ```{r}
#library(car) #needed but we already loaded
influencePlot(lmod)
```

Let's remove Alaska since it is a high leverage point. Then identify best subset using $R^2_a$ as our search criterion.


```{r}
best <- regsubsets(Life.Exp ~., data = statedata, subset = (state.abb != "AK"))
bsum <- summary(best)
bsum$which[which.max(bsum$adjr), ]
```

## Summary

There are other mechanisms for choosing the training and test datasets, but these are the most common.

- **When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE or RMSE.**
- You typically don’t do an exhaustive search or stepwise selection search.
- You often use one of the other selection criteria/search strategies to narrow down the possible models to a few final candidate models and then use cross-validation to make a final decision.
- Iteration and experimentation are essential to finding better models BUT be very careful not to overtrain your model to the sample data!

## Exercise

For the teengamb data in the faraway package, use the methods learned in this chapter to identify the “best” models.

```{r}
library(faraway)
data(teengamb) # load data
summary(teengamb)
```


```{r}
# fit full model
lmod <- lm(gamble ~ ., data = teengamb)
sumary(lmod) # determine least significant predictor
```

```{r}
# Values we'll need
n <- nobs(lmod)
p <- 2:5
```

```{r}
# perform backward elimination using update function on previous model
# use alpha_crit = 0.05
lmod <- update(lmod, . ~ . - status)
sumary(lmod)
lmod <- update(lmod, . ~ . - verbal)
sumary(lmod)
```

```{r}
# model selection in terms of AIC
library(leaps)
# model selection by exhaustive search
best <- regsubsets(gamble ~ ., data = teengamb)
bsum <- summary(best) # summarize model that minimizes RSS for each p
bsum$which # best subset models (in terms of RSS)
```

```{r}
# calculate AIC of each model
aic <- bsum$bic + (2 - log(n)) * p
# plot AIC vs p
plot(aic ~ p, xlab = "Number of Predictors", ylab = "AIC", pch =16)
```


```{r}
# calculate BIC of each model
# plot BIC vs p
library(car)
subsets(best, statistic = "bic", legend = FALSE)
```

```{r}
# Construct Cp plot
subsets(best, statistic = "cp", legend = FALSE)
abline(0, 1)
```

```{r}
# Construct adjusted R^2 plot
subsets(best, statistic = "adjr2", legend = FALSE)
```

```{r}
# backward elimination
lmod <- lm(gamble ~ ., data = teengamb)
step(lmod) #with AIC
step(lmod, k = log(n)) # with BIC
```

```{r}
library(caret)
f1 = gamble ~ sex + income
f2 = gamble ~ sex + verbal + income

# 5-fold crossvalidation train/test data
cv_5fold <- trainControl(method = "cv", number = 5)
model1 <- train(f1, data = teengamb, trControl = cv_5fold,
                method = "lm")
model2 <- train(f2, data = teengamb, trControl = cv_5fold,
                method = "lm")

# compare mse (rmse) for the two models using 5-fold cv
print(model1) # p = 3
print(model2) # p = 4
```


We prefer the model with smaller RMSE and MSE. This can switch depending on the random 5-fold data set selected.

```{r}
# trying an interaction model
f3 = gamble ~ sex*income
model3 <- train(f3, data = teengamb, trControl = cv_5fold,
                method = "lm")

print(model3) # even better
```


residualPlots(lm(f3, data = teengamb))

```{r}
# try a transformed model
# trying another model
f4 = sqrt(gamble) ~ sex*income
model4 <- train(f4, data = teengamb, trControl = cv_5fold,
                method = "lm")
print(model4)
```


```{r}
residualPlots(lm(f4, data = teengamb))
```


Not comparable to previous model since the response is transformed.  Should really go through variable selection process again.


