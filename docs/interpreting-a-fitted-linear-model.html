<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Interpreting a fitted linear model | Joshua French</title>
  <meta name="description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Interpreting a fitted linear model | Joshua French" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Interpreting a fitted linear model | Joshua French" />
  
  <meta name="twitter:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

<meta name="author" content="Chapter 4 Interpreting a fitted linear model | Joshua French" />


<meta name="date" content="2022-07-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-model-estimation.html"/>
<link rel="next" href="linear-model-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with Linear Regression</a></li>

<li class="divider"></li>
<li><a href="index.html#preliminaries">Preliminaries<span></span></a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#setting-up-r-and-r-studio-desktop"><i class="fa fa-check"></i><b>1.1</b> Setting up R and R Studio Desktop<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.2</b> Running code, scripts, and comments<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.3</b> Assignment<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#functions"><i class="fa fa-check"></i><b>1.4</b> Functions<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.8</b> Vectors<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.8.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.8.2</b> Categorical vectors<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.8.3</b> Extracting parts of a vector<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.9</b> Helpful functions<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.9.1</b> General functions<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.9.2</b> Functions related to statistical distributions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.10</b> Data Frames<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#direct-creation"><i class="fa fa-check"></i><b>1.10.1</b> Direct creation<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.10.2</b> Importing Data<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.10.3</b> Extracting parts of a data frame<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#using-the-pipe-operator"><i class="fa fa-check"></i><b>1.11</b> Using the pipe operator<span></span></a></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#dealing-with-common-problems"><i class="fa fa-check"></i><b>1.12</b> Dealing with common problems<span></span></a></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.13</b> Ecosystem debate<span></span></a></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#additional-information"><i class="fa fa-check"></i><b>1.14</b> Additional information<span></span></a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="r-foundations.html"><a href="r-foundations.html#comparing-assignment-operators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing assignment operators<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html"><i class="fa fa-check"></i><b>2</b> Data cleaning and exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#raw-palmer-penguins-data"><i class="fa fa-check"></i><b>2.1</b> Raw Palmer penguins data<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#initial-data-cleaning"><i class="fa fa-check"></i><b>2.2</b> Initial data cleaning<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numerical-summarization-of-data"><i class="fa fa-check"></i><b>2.3</b> Numerical summarization of data<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numeric-data"><i class="fa fa-check"></i><b>2.3.1</b> Numeric data<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#categorical-data"><i class="fa fa-check"></i><b>2.3.2</b> Categorical data<span></span></a></li>
<li class="chapter" data-level="2.3.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-summary-function"><i class="fa fa-check"></i><b>2.3.3</b> The <code>summary</code> function<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.4</b> Visual summaries of data<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-ggplot-recipe"><i class="fa fa-check"></i><b>2.4.1</b> The ggplot recipe<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#univariate-plots"><i class="fa fa-check"></i><b>2.4.2</b> Univariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#bivariate-plots"><i class="fa fa-check"></i><b>2.4.3</b> Bivariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#multivariate-plots"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#facetted-plots-and-alternatives"><i class="fa fa-check"></i><b>2.4.5</b> Facetted plots (and alternatives)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#a-plan-for-data-cleaning-and-exploration"><i class="fa fa-check"></i><b>2.5</b> A plan for data cleaning and exploration<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#final-notes-on-missing-or-erroneous-data"><i class="fa fa-check"></i><b>2.6</b> Final notes on missing or erroneous data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>3</b> Linear model estimation<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>3.1</b> A simple motivating example<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>3.2</b> Defining a linear model<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>3.2.1</b> Necessary components and notation<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>3.2.2</b> Standard definition of linear model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>3.3</b> Estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>3.3.1</b> Fitted values, residuals, and RSS<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>3.3.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>3.4</b> Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5</b> Estimation of the multiple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>3.5.1</b> Using matrix notation to represent a linear model<span></span></a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>3.5.2</b> Residuals, fitted values, and RSS for multiple linear regression<span></span></a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.5.3</b> OLS estimator of the regression coefficients<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr"><i class="fa fa-check"></i><b>3.6</b> Penguins multiple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#categorical-predictors"><i class="fa fa-check"></i><b>3.7</b> Categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#indicator-variables"><i class="fa fa-check"></i><b>3.7.1</b> Indicator variables<span></span></a></li>
<li class="chapter" data-level="3.7.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#parallel-and-separate-lines-models"><i class="fa fa-check"></i><b>3.7.2</b> Parallel and separate lines models<span></span></a></li>
<li class="chapter" data-level="3.7.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#extensions"><i class="fa fa-check"></i><b>3.7.3</b> Extensions<span></span></a></li>
<li class="chapter" data-level="3.7.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#avoiding-an-easy-mistake"><i class="fa fa-check"></i><b>3.7.4</b> Avoiding an easy mistake<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr2"><i class="fa fa-check"></i><b>3.8</b> Penguins example with categorical predictor<span></span></a></li>
<li class="chapter" data-level="3.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#evaluating-model-fit"><i class="fa fa-check"></i><b>3.9</b> Evaluating model fit<span></span></a></li>
<li class="chapter" data-level="3.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary"><i class="fa fa-check"></i><b>3.10</b> Summary<span></span></a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-terms"><i class="fa fa-check"></i><b>3.10.1</b> Summary of terms<span></span></a></li>
<li class="chapter" data-level="3.10.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-functions"><i class="fa fa-check"></i><b>3.10.2</b> Summary of functions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#going-deeper"><i class="fa fa-check"></i><b>3.11</b> Going Deeper<span></span></a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#degrees-of-freedom"><i class="fa fa-check"></i><b>3.11.1</b> Degrees of freedom<span></span></a></li>
<li class="chapter" data-level="3.11.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#slr-derivation"><i class="fa fa-check"></i><b>3.11.2</b> Derivation of the OLS estimator for the simple linear regression model coefficients<span></span></a></li>
<li class="chapter" data-level="3.11.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>3.11.3</b> Manual calculation Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.11.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#mlr-derivation"><i class="fa fa-check"></i><b>3.11.4</b> Derivation of the OLS estimator for the multiple linear regression model coefficients<span></span></a></li>
<li class="chapter" data-level="3.11.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example"><i class="fa fa-check"></i><b>3.11.5</b> Manual calculation of Penguins multiple linear regression example<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html"><i class="fa fa-check"></i><b>4</b> Interpreting a fitted linear model<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#standard-mathemtical-interpretation"><i class="fa fa-check"></i><b>4.1</b> Standard mathemtical interpretation<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-in-simple-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-for-first-order-multiple-linear-regression-models"><i class="fa fa-check"></i><b>4.1.2</b> Coefficient interpretation for first-order multiple linear regression models<span></span></a></li>
<li class="chapter" data-level="4.1.3" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#penguins-examples"><i class="fa fa-check"></i><b>4.1.3</b> Penguins examples<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#going-deeper-1"><i class="fa fa-check"></i><b>4.2</b> Going deeper<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#orthogonality"><i class="fa fa-check"></i><b>4.2.1</b> Orthogonality<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-model-inference.html"><a href="linear-model-inference.html"><i class="fa fa-check"></i><b>5</b> Linear model inference<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#galapogos-example-testing-all-regressors"><i class="fa fa-check"></i><b>5.1</b> Galapogos Example: Testing All Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-1"><i class="fa fa-check"></i><b>5.1.1</b> Question 1<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-2"><i class="fa fa-check"></i><b>5.1.2</b> Question 2<span></span></a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#setting-up-the-hypotheses"><i class="fa fa-check"></i><b>5.1.3</b> Setting Up The Hypotheses<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-3"><i class="fa fa-check"></i><b>5.2</b> Question 3<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#measuring-significance"><i class="fa fa-check"></i><b>5.2.1</b> Measuring Significance<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#general-f-test-for-comparing-two-nested-regression-models"><i class="fa fa-check"></i><b>5.2.2</b> General <span class="math inline">\(F\)</span> Test for Comparing Two Nested Regression Models:<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-4"><i class="fa fa-check"></i><b>5.2.3</b> Question 4<span></span></a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-5"><i class="fa fa-check"></i><b>5.2.4</b> Question 5<span></span></a></li>
<li class="chapter" data-level="5.2.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-6"><i class="fa fa-check"></i><b>5.2.5</b> Question 6<span></span></a></li>
<li class="chapter" data-level="5.2.6" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-7"><i class="fa fa-check"></i><b>5.2.6</b> Question 7<span></span></a></li>
<li class="chapter" data-level="5.2.7" data-path="linear-model-inference.html"><a href="linear-model-inference.html#interpreting-the-results"><i class="fa fa-check"></i><b>5.2.7</b> Interpreting the Results<span></span></a></li>
<li class="chapter" data-level="5.2.8" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-8"><i class="fa fa-check"></i><b>5.2.8</b> Question 8<span></span></a></li>
<li class="chapter" data-level="5.2.9" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-9"><i class="fa fa-check"></i><b>5.2.9</b> Question 9<span></span></a></li>
<li class="chapter" data-level="5.2.10" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-10"><i class="fa fa-check"></i><b>5.2.10</b> Question 10<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#testing-a-pair-of-regressors"><i class="fa fa-check"></i><b>5.3</b> Testing a Pair of Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-11"><i class="fa fa-check"></i><b>5.3.1</b> Question 11<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#permutation-tests"><i class="fa fa-check"></i><b>5.4</b> Permutation Tests<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#assumptions-for-testing-regressors"><i class="fa fa-check"></i><b>5.4.1</b> Assumptions for Testing Regressors<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#motivating-idea-behind-permutation-tests"><i class="fa fa-check"></i><b>5.4.2</b> Motivating idea behind permutation tests<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#permutation-test-on-two-regressors"><i class="fa fa-check"></i><b>5.5</b> Permutation Test on Two Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-1-set-up-the-hypotheses-for-this-test."><i class="fa fa-check"></i><b>5.5.1</b> Question 1: Set up the hypotheses for this test.<span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-2-compute-the-f-statistic-from-the-general-f-test."><i class="fa fa-check"></i><b>5.5.2</b> Question 2: Compute the <span class="math inline">\(F\)</span>-statistic from the general <span class="math inline">\(F\)</span> test.<span></span></a></li>
<li class="chapter" data-level="5.5.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-3-compute-the-p-value-of-the-corresponding-f-statistic-from-the-general-f-test."><i class="fa fa-check"></i><b>5.5.3</b> Question 3: Compute the <span class="math inline">\(p\)</span>-value of the corresponding <span class="math inline">\(F\)</span>-statistic from the general <span class="math inline">\(F\)</span> test.<span></span></a></li>
<li class="chapter" data-level="5.5.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#sample-permutation-test"><i class="fa fa-check"></i><b>5.5.4</b> Sample Permutation Test<span></span></a></li>
<li class="chapter" data-level="5.5.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-4-compare-the-two-p-values-from-the-two-methods.-do-you-think-the-difference-is-significant-which-p-value-do-you-think-is-more-accurate-why"><i class="fa fa-check"></i><b>5.5.5</b> Question 4: Compare the two p-values from the two methods. Do you think the difference is significant? Which p-value do you think is more accurate? Why?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-model-inference.html"><a href="linear-model-inference.html#testing-whether-one-regressor-can-be-dropped"><i class="fa fa-check"></i><b>5.6</b> Testing whether one regressor can be dropped<span></span></a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#setting-up-the-hypotheses-1"><i class="fa fa-check"></i><b>5.6.1</b> Setting up the hypotheses<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#extracting-pertinent-statistics-from-theoretical-test"><i class="fa fa-check"></i><b>5.6.2</b> Extracting pertinent statistics from theoretical test<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-5-perform-a-pertmutation-test-to-test-the-hypotheses-above."><i class="fa fa-check"></i><b>5.6.3</b> Question 5: Perform a pertmutation test to test the hypotheses above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="linear-model-inference.html"><a href="linear-model-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.7</b> Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#constructing-a-95-ci-for-area-assuming-all-4-other-predictors-in-the-model"><i class="fa fa-check"></i><b>5.7.1</b> Constructing a 95% CI for Area (assuming all 4 other predictors in the model)<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-6-based-on-the-output-above-construct-a-95-confidence-interval-to-estimate-the-value-of-beta_rm-area."><i class="fa fa-check"></i><b>5.7.2</b> Question 6: Based on the output above, construct a 95% confidence interval to estimate the value of <span class="math inline">\(\beta_{\rm Area}\)</span>.<span></span></a></li>
<li class="chapter" data-level="5.7.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-7-cconstruct-a-95-confidence-interval-to-estimate-the-value-of-beta_rm-elevation."><i class="fa fa-check"></i><b>5.7.3</b> Question 7: Cconstruct a 95% confidence interval to estimate the value of <span class="math inline">\(\beta_{\rm Elevation}\)</span>.<span></span></a></li>
<li class="chapter" data-level="5.7.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#finding-confidence-intervals-for-all-regressors"><i class="fa fa-check"></i><b>5.7.4</b> Finding Confidence Intervals for All Regressors<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="linear-model-inference.html"><a href="linear-model-inference.html#confidence-regions"><i class="fa fa-check"></i><b>5.8</b> Confidence Regions<span></span></a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-8-based-on-the-confidence-region-above-is-it-plausible-that-beta_rm-area-beta_rm-adjacent0-why-or-why-not"><i class="fa fa-check"></i><b>5.8.1</b> Question 8: Based on the confidence region above, is it plausible that <span class="math inline">\(\beta_{\rm Area}= \beta_{\rm Adjacent}=0\)</span>? Why or why not?<span></span></a></li>
<li class="chapter" data-level="5.8.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-9-based-on-the-confidence-region-above-is-it-plausible-that-beta_rm-area--0.6-and-beta_rm-adjacent-0.045-why-or-why-not"><i class="fa fa-check"></i><b>5.8.2</b> Question 9: Based on the confidence region above, is it plausible that <span class="math inline">\(\beta_{\rm Area}= -0.6\)</span> and <span class="math inline">\(\beta_{\rm Adjacent}=-0.045\)</span>? Why or why not?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="linear-model-inference.html"><a href="linear-model-inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.9</b> Bootstrap Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#bootstrap-process"><i class="fa fa-check"></i><b>5.9.1</b> Bootstrap Process<span></span></a></li>
<li class="chapter" data-level="5.9.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#comparing-with-parametric-confidence-intervals"><i class="fa fa-check"></i><b>5.9.2</b> Comparing with Parametric Confidence Intervals<span></span></a></li>
<li class="chapter" data-level="5.9.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#visualizing-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.9.3</b> Visualizing Bootstrap Confidence Intervals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="linear-model-inference.html"><a href="linear-model-inference.html#sampling-experimentation-generalization-and-causation"><i class="fa fa-check"></i><b>5.10</b> Sampling Experimentation, Generalization, and Causation<span></span></a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#designed-experiments"><i class="fa fa-check"></i><b>5.10.1</b> Designed Experiments<span></span></a></li>
<li class="chapter" data-level="5.10.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#observational-studies"><i class="fa fa-check"></i><b>5.10.2</b> Observational Studies<span></span></a></li>
<li class="chapter" data-level="5.10.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#experimental-and-observational-predictors"><i class="fa fa-check"></i><b>5.10.3</b> Experimental and Observational Predictors<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Prediction<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="prediction.html"><a href="prediction.html#example-fat-data"><i class="fa fa-check"></i><b>6.1</b> Example: fat data<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="prediction.html"><a href="prediction.html#predicting-percent-body-fat"><i class="fa fa-check"></i><b>6.2</b> Predicting Percent Body Fat<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="prediction.html"><a href="prediction.html#question-1-what-would-the-body-fat-be-for-a-person-that-has-the-following-measurements-note-the-first-value-is-a-place-holder-for-the-intercept-in-the-model"><i class="fa fa-check"></i><b>6.3</b> Question 1: What would the body fat be for a person that has the following measurements (note the first value is a place holder for the intercept in the model)?<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="prediction.html"><a href="prediction.html#the-general-setup"><i class="fa fa-check"></i><b>6.4</b> The General Setup<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="prediction.html"><a href="prediction.html#types-of-predictions"><i class="fa fa-check"></i><b>6.5</b> Types of Predictions<span></span></a></li>
<li class="chapter" data-level="6.6" data-path="prediction.html"><a href="prediction.html#question-2-using-properties-of-variance-and-underlying-assumptions-of-our-model-find-the-variance-of-widehaty_0-mathbfx_0t-widehatboldsymbolbeta."><i class="fa fa-check"></i><b>6.6</b> Question 2: Using properties of variance and underlying assumptions of our model, find the variance of <span class="math inline">\(\widehat{y}_0 = \mathbf{x}_0^T \widehat{\boldsymbol\beta}\)</span>.<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="prediction.html"><a href="prediction.html#quantifying-the-uncertainty-of-a-prediction"><i class="fa fa-check"></i><b>6.7</b> Quantifying the Uncertainty of a Prediction<span></span></a></li>
<li class="chapter" data-level="6.8" data-path="prediction.html"><a href="prediction.html#example-prediction-interval-for-median-man"><i class="fa fa-check"></i><b>6.8</b> Example: Prediction Interval for Median Man<span></span></a></li>
<li class="chapter" data-level="6.9" data-path="prediction.html"><a href="prediction.html#question-3-predict-median-mans-percent-body-fat."><i class="fa fa-check"></i><b>6.9</b> Question 3: Predict Median Man’s percent body fat.<span></span></a></li>
<li class="chapter" data-level="6.10" data-path="prediction.html"><a href="prediction.html#constructing-interval-estimates-using-the-predict-function"><i class="fa fa-check"></i><b>6.10</b> Constructing Interval Estimates Using the <code>predict</code> Function<span></span></a></li>
<li class="chapter" data-level="6.11" data-path="prediction.html"><a href="prediction.html#question-4-give-a-practical-interpretation-of-each-interval-estimate-above-in-the-context-of-this-example."><i class="fa fa-check"></i><b>6.11</b> Question 4: Give a practical interpretation of each interval estimate above in the context of this example.<span></span></a></li>
<li class="chapter" data-level="6.12" data-path="prediction.html"><a href="prediction.html#question-5-explain-why-it-makes-sense-that-the-95-prediction-interval-is-much-wider-than-the-95-confidence-interval"><i class="fa fa-check"></i><b>6.12</b> Question 5: Explain why it makes sense that the 95% prediction interval is much wider than the 95% confidence interval?<span></span></a></li>
<li class="chapter" data-level="6.13" data-path="prediction.html"><a href="prediction.html#extrapolation"><i class="fa fa-check"></i><b>6.13</b> Extrapolation<span></span></a></li>
<li class="chapter" data-level="6.14" data-path="prediction.html"><a href="prediction.html#question-6-what-happens-to-the-interval-estimates-when-we-predict-body-fat-for-a-man-named-nine-d.-five-why-does-this-make-practical-sense"><i class="fa fa-check"></i><b>6.14</b> Question 6: What happens to the interval estimates when we predict body fat for a man named Nine D. Five? Why does this make practical sense?<span></span></a></li>
<li class="chapter" data-level="6.15" data-path="prediction.html"><a href="prediction.html#sources-of-uncertainty"><i class="fa fa-check"></i><b>6.15</b> Sources of Uncertainty<span></span></a></li>
<li class="chapter" data-level="6.16" data-path="prediction.html"><a href="prediction.html#what-can-go-wrong-with-predictions"><i class="fa fa-check"></i><b>6.16</b> What Can Go Wrong With Predictions?<span></span></a></li>
<li class="chapter" data-level="6.17" data-path="prediction.html"><a href="prediction.html#question-7-teen-gambling-example"><i class="fa fa-check"></i><b>6.17</b> Question 7: Teen Gambling Example<span></span></a></li>
<li class="chapter" data-level="6.18" data-path="prediction.html"><a href="prediction.html#appendix-computing-standard-errors-for-confidence-and-prediction-intervals"><i class="fa fa-check"></i><b>6.18</b> Appendix: Computing Standard Errors for Confidence and Prediction Intervals<span></span></a>
<ul>
<li class="chapter" data-level="6.18.1" data-path="prediction.html"><a href="prediction.html#confidence-interval-standard-error"><i class="fa fa-check"></i><b>6.18.1</b> Confidence interval standard error:<span></span></a></li>
<li class="chapter" data-level="6.18.2" data-path="prediction.html"><a href="prediction.html#prediction-interval-standard-error"><i class="fa fa-check"></i><b>6.18.2</b> Prediction interval standard error:<span></span></a></li>
<li class="chapter" data-level="6.18.3" data-path="prediction.html"><a href="prediction.html#underlying-distrubtion"><i class="fa fa-check"></i><b>6.18.3</b> Underlying distrubtion<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html"><i class="fa fa-check"></i><b>7</b> Assumptions Stated and Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-concisely-stated"><i class="fa fa-check"></i><b>7.1</b> Standard assumptions concisely stated<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-prioritized"><i class="fa fa-check"></i><b>7.2</b> Standard assumptions prioritized<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Basic regression diagnostics<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#motivating-example"><i class="fa fa-check"></i><b>8.1</b> Motivating Example<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#standard-assumptions-revisited"><i class="fa fa-check"></i><b>8.2</b> Standard Assumptions Revisited<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#theoretical-properties"><i class="fa fa-check"></i><b>8.3</b> Theoretical Properties<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practical-considerations"><i class="fa fa-check"></i><b>8.4</b> Practical Considerations<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#standard-assumptions-prioritized-1"><i class="fa fa-check"></i><b>8.5</b> Standard Assumptions Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#running-a-statistical-test"><i class="fa fa-check"></i><b>8.5.1</b> Running a Statistical Test<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#checking-normality-of-residuals"><i class="fa fa-check"></i><b>8.6</b> Checking Normality of Residuals<span></span></a></li>
<li class="chapter" data-level="8.7" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#q-q-plots"><i class="fa fa-check"></i><b>8.7</b> Q-Q Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#interpreting-q-q-plots"><i class="fa fa-check"></i><b>8.7.1</b> Interpreting q-q Plots<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#the-shapiro-wilk-test-for-normality"><i class="fa fa-check"></i><b>8.8</b> The Shapiro-Wilk test for Normality<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-interpret-the-result-of-this-test-in-practical-terms."><i class="fa fa-check"></i><b>8.9</b> Question 1: Interpret the result of this test in practical terms.<span></span></a></li>
<li class="chapter" data-level="8.10" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#checking-for-correlated-errorrs"><i class="fa fa-check"></i><b>8.10</b> Checking for Correlated Errorrs<span></span></a></li>
<li class="chapter" data-level="8.11" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#motivating-example-global-warming"><i class="fa fa-check"></i><b>8.11</b> Motivating Example: Global warming<span></span></a></li>
<li class="chapter" data-level="8.12" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-residuals-against-time"><i class="fa fa-check"></i><b>8.12</b> Plotting Residuals Against Time<span></span></a></li>
<li class="chapter" data-level="8.13" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-do-you-notice-any-correlation-in-the-errors"><i class="fa fa-check"></i><b>8.13</b> Question 2: Do you notice any correlation in the errors?<span></span></a></li>
<li class="chapter" data-level="8.14" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-successive-pairs-of-residuals"><i class="fa fa-check"></i><b>8.14</b> Plotting Successive Pairs of Residuals<span></span></a></li>
<li class="chapter" data-level="8.15" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-interpret-the-plot-above.-how-does-this-confirm-a-positive-serial-correlation"><i class="fa fa-check"></i><b>8.15</b> Question 3: Interpret the plot above. How does this confirm a positive serial correlation?<span></span></a></li>
<li class="chapter" data-level="8.16" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#the-durbin-watson-test-for-uncorrelated-errors"><i class="fa fa-check"></i><b>8.16</b> The Durbin-Watson Test for Uncorrelated Errors<span></span></a></li>
<li class="chapter" data-level="8.17" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-interpret-the-result-of-this-test-in-practical-terms."><i class="fa fa-check"></i><b>8.17</b> Question 4: Interpret the result of this test in practical terms.<span></span></a></li>
<li class="chapter" data-level="8.18" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practice-example-a-model-for-sat-scores"><i class="fa fa-check"></i><b>8.18</b> Practice Example: A Model for SAT Scores<span></span></a></li>
<li class="chapter" data-level="8.19" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-methods-for-checking-error-assumptions"><i class="fa fa-check"></i><b>8.19</b> Summary of Methods for Checking Error Assumptions<span></span></a></li>
<li class="chapter" data-level="8.20" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-useful-r-functions-for-checking-error-assumptions"><i class="fa fa-check"></i><b>8.20</b> Summary of useful R functions for checking error assumptions<span></span></a></li>
<li class="chapter" data-level="8.21" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#residuals"><i class="fa fa-check"></i><b>8.21</b> Residuals:<span></span></a></li>
<li class="chapter" data-level="8.22" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#mean-zero-error-assumption"><i class="fa fa-check"></i><b>8.22</b> Mean-zero error assumption:<span></span></a></li>
<li class="chapter" data-level="8.23" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#constant-error-variance-assumption"><i class="fa fa-check"></i><b>8.23</b> Constant error variance assumption:<span></span></a></li>
<li class="chapter" data-level="8.24" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#to-assess-error-normality"><i class="fa fa-check"></i><b>8.24</b> To assess error normality:<span></span></a></li>
<li class="chapter" data-level="8.25" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#correlated-errors"><i class="fa fa-check"></i><b>8.25</b> Correlated Errors<span></span></a></li>
<li class="chapter" data-level="8.26" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#unusual-observations"><i class="fa fa-check"></i><b>8.26</b> Unusual Observations<span></span></a></li>
<li class="chapter" data-level="8.27" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#identifying-leverage-points"><i class="fa fa-check"></i><b>8.27</b> Identifying Leverage Points<span></span></a></li>
<li class="chapter" data-level="8.28" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-the-hat-matrix-and-leverage-values-for-savings-model"><i class="fa fa-check"></i><b>8.28</b> Computing the Hat Matrix and Leverage Values for Savings Model<span></span></a></li>
<li class="chapter" data-level="8.29" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#half-normal-plots"><i class="fa fa-check"></i><b>8.29</b> Half-Normal Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.29.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#creating-a-half-normal-plot"><i class="fa fa-check"></i><b>8.29.1</b> Creating a Half-Normal Plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.30" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-complete-the-code-cell-below-to-create-a-half-normal-plot"><i class="fa fa-check"></i><b>8.30</b> Question 1: Complete the code cell below to create a half-normal plot<span></span></a>
<ul>
<li class="chapter" data-level="8.30.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#example-with-no-leverage-points"><i class="fa fa-check"></i><b>8.30.1</b> Example With No Leverage Points<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.31" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots"><i class="fa fa-check"></i><b>8.31</b> Index Plots<span></span></a></li>
<li class="chapter" data-level="8.32" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#identifying-outliers"><i class="fa fa-check"></i><b>8.32</b> Identifying outliers<span></span></a></li>
<li class="chapter" data-level="8.33" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#examples-of-outliers"><i class="fa fa-check"></i><b>8.33</b> Examples of Outliers<span></span></a></li>
<li class="chapter" data-level="8.34" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leave-one-out-statistics"><i class="fa fa-check"></i><b>8.34</b> Leave-One-Out Statistics<span></span></a></li>
<li class="chapter" data-level="8.35" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#studentized-residuals"><i class="fa fa-check"></i><b>8.35</b> Studentized Residuals<span></span></a>
<ul>
<li class="chapter" data-level="8.35.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#bonferonni-correction"><i class="fa fa-check"></i><b>8.35.1</b> Bonferonni correction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.36" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-why-do-we-divide-by-2n-and-not-just-n"><i class="fa fa-check"></i><b>8.36</b> Question 3: Why do we divide by <span class="math inline">\(2n\)</span> and not just <span class="math inline">\(n\)</span>?<span></span></a>
<ul>
<li class="chapter" data-level="8.36.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-studentized-residuals"><i class="fa fa-check"></i><b>8.36.1</b> Computing Studentized Residuals<span></span></a></li>
<li class="chapter" data-level="8.36.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-correction-p-value"><i class="fa fa-check"></i><b>8.36.2</b> Computing Bonferonni correction P-value<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.37" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-based-on-the-output-above-does-this-model-have-any-outliers"><i class="fa fa-check"></i><b>8.37</b> Question 4: Based on the output above, does this model have any outliers?<span></span></a>
<ul>
<li class="chapter" data-level="8.37.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#using-the-outliertest-function"><i class="fa fa-check"></i><b>8.37.1</b> Using the outlierTest Function<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.38" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots-of-studentized-residuals-and-bonferroni-p-values"><i class="fa fa-check"></i><b>8.38</b> Index Plots of Studentized Residuals and Bonferroni p-values<span></span></a></li>
<li class="chapter" data-level="8.39" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-unusual-observations"><i class="fa fa-check"></i><b>8.39</b> Summary of Unusual Observations<span></span></a></li>
<li class="chapter" data-level="8.40" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>8.40</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.41" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leave-one-out-statistics-1"><i class="fa fa-check"></i><b>8.41</b> Leave-One-Out Statistics<span></span></a></li>
<li class="chapter" data-level="8.42" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-what-is-the-value-of-the-leave-one-out-residual-corresponding-to-the-obervation-from-zambia"><i class="fa fa-check"></i><b>8.42</b> Question 1: What is the value of the leave-one-out residual corresponding to the obervation from Zambia?<span></span></a></li>
<li class="chapter" data-level="8.43" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-in-analyzing-another-marketing-dataset-the-largest-leave-one-out-residual-value-is-12.48.-do-you-believe-this-observation-is-more-of-an-outlier-in-marketing-model-compared-to-zambia-in-the-savings-model"><i class="fa fa-check"></i><b>8.43</b> Question 2: In analyzing another marketing dataset, the largest leave-one-out residual value is <span class="math inline">\(12.48\)</span>. Do you believe this observation is more of an outlier in marketing model compared to Zambia in the savings model?<span></span></a></li>
<li class="chapter" data-level="8.44" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#studentized-residuals-1"><i class="fa fa-check"></i><b>8.44</b> Studentized Residuals<span></span></a>
<ul>
<li class="chapter" data-level="8.44.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-studentized-residuals-1"><i class="fa fa-check"></i><b>8.44.1</b> Computing Studentized Residuals<span></span></a></li>
<li class="chapter" data-level="8.44.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-studentized-residuals"><i class="fa fa-check"></i><b>8.44.2</b> Plotting Studentized Residuals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.45" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#testing-for-outliers"><i class="fa fa-check"></i><b>8.45</b> Testing for Outliers<span></span></a>
<ul>
<li class="chapter" data-level="8.45.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#types-of-errors-and-the-significance-level"><i class="fa fa-check"></i><b>8.45.1</b> Types of Errors and the Significance Level<span></span></a></li>
<li class="chapter" data-level="8.45.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#bonferonni-correction-1"><i class="fa fa-check"></i><b>8.45.2</b> Bonferonni correction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.46" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-what-is-the-maximum-value-for-the-probability-of-making-a-type-i-error-if-the-results-of-the-two-individual-tests-are-significant."><i class="fa fa-check"></i><b>8.46</b> Question 3: What is the maximum value for the probability of making a Type I Error if the results of the two individual tests are significant.<span></span></a>
<ul>
<li class="chapter" data-level="8.46.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-critical-values"><i class="fa fa-check"></i><b>8.46.1</b> Computing Bonferonni Critical Values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.47" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-based-on-the-output-above-does-this-model-have-any-outliers-1"><i class="fa fa-check"></i><b>8.47</b> Question 4: Based on the output above, does this model have any outliers?<span></span></a>
<ul>
<li class="chapter" data-level="8.47.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-p-values-with-outliertest-function"><i class="fa fa-check"></i><b>8.47.1</b> Computing Bonferonni p-Values with outlierTest Function<span></span></a></li>
<li class="chapter" data-level="8.47.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots-of-bonferroni-p-values"><i class="fa fa-check"></i><b>8.47.2</b> Index Plots of Bonferroni p-values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.48" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#caution-with-outliers"><i class="fa fa-check"></i><b>8.48</b> Caution With Outliers:<span></span></a></li>
<li class="chapter" data-level="8.49" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations"><i class="fa fa-check"></i><b>8.49</b> Influential Observations<span></span></a></li>
<li class="chapter" data-level="8.50" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#change-in-regression-coefficients-dfbeta"><i class="fa fa-check"></i><b>8.50</b> Change in Regression Coefficients, DFBETA<span></span></a></li>
<li class="chapter" data-level="8.51" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-5-how-does-the-model-fit-change-when-we-remove-libya-from-the-data-which-coefficients-seem-most-influenced"><i class="fa fa-check"></i><b>8.51</b> Question 5: How does the model fit change when we remove Libya from the data? Which coefficients seem most influenced?<span></span></a>
<ul>
<li class="chapter" data-level="8.51.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#ploting-dfbeta-and-dfbetas"><i class="fa fa-check"></i><b>8.51.1</b> Ploting DFBETA and DFBETAs<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.52" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#cooks-distance"><i class="fa fa-check"></i><b>8.52</b> Cook’s Distance<span></span></a>
<ul>
<li class="chapter" data-level="8.52.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-and-plotting-cooks-distance"><i class="fa fa-check"></i><b>8.52.1</b> Computing and Plotting Cook’s Distance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.53" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influence-plots"><i class="fa fa-check"></i><b>8.53</b> Influence Plots<span></span></a></li>
<li class="chapter" data-level="8.54" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#what-should-we-do-about-outliers-and-influential-observations"><i class="fa fa-check"></i><b>8.54</b> What should we do about outliers and influential observations?<span></span></a></li>
<li class="chapter" data-level="8.55" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#should-we-correct-or-delete-the-observations"><i class="fa fa-check"></i><b>8.55</b> Should we correct or delete the observation(s)?<span></span></a></li>
<li class="chapter" data-level="8.56" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#should-we-keep-them-and-fit-a-different-model"><i class="fa fa-check"></i><b>8.56</b> Should we keep them and fit a different model?<span></span></a></li>
<li class="chapter" data-level="8.57" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-1"><i class="fa fa-check"></i><b>8.57</b> Summary<span></span></a></li>
<li class="chapter" data-level="8.58" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practice-sat-example"><i class="fa fa-check"></i><b>8.58</b> Practice: SAT Example<span></span></a></li>
<li class="chapter" data-level="8.59" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#part-1-fit-a-model-with-the-total-sat-score-as-the-response-and-expend-salary-ratio-and-takers-as-regressors."><i class="fa fa-check"></i><b>8.59</b> Part 1: Fit a model with the total SAT score as the response and expend, salary, ratio, and takers as regressors.<span></span></a></li>
<li class="chapter" data-level="8.60" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#part-2-perform-regression-diagnostics-on-this-model-to-answer-the-following-questions."><i class="fa fa-check"></i><b>8.60</b> Part 2: Perform regression diagnostics on this model to answer the following questions.<span></span></a></li>
<li class="chapter" data-level="8.61" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#glossary"><i class="fa fa-check"></i><b>8.61</b> Glossary<span></span></a></li>
<li class="chapter" data-level="8.62" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-methods-for-identifying-unusual-observations"><i class="fa fa-check"></i><b>8.62</b> Summary of methods for identifying unusual observations<span></span></a>
<ul>
<li class="chapter" data-level="8.62.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leverage-points"><i class="fa fa-check"></i><b>8.62.1</b> Leverage points:<span></span></a></li>
<li class="chapter" data-level="8.62.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers-1"><i class="fa fa-check"></i><b>8.62.2</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.62.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations-1"><i class="fa fa-check"></i><b>8.62.3</b> Influential observations:<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.63" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-useful-r-functions-for-identifying-unusual-observations"><i class="fa fa-check"></i><b>8.63</b> Summary of useful R functions for identifying unusual observations<span></span></a>
<ul>
<li class="chapter" data-level="8.63.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leverage-points-1"><i class="fa fa-check"></i><b>8.63.1</b> Leverage points<span></span></a></li>
<li class="chapter" data-level="8.63.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers-2"><i class="fa fa-check"></i><b>8.63.2</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.63.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations-2"><i class="fa fa-check"></i><b>8.63.3</b> Influential observations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.64" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#model-structure"><i class="fa fa-check"></i><b>8.64</b> Model Structure<span></span></a>
<ul>
<li class="chapter" data-level="8.64.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.64.1</b> Residual Plots<span></span></a></li>
<li class="chapter" data-level="8.64.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#example"><i class="fa fa-check"></i><b>8.64.2</b> Example<span></span></a></li>
<li class="chapter" data-level="8.64.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-comment-on-any-patterns-you-observe-in-the-residual-plots-above."><i class="fa fa-check"></i><b>8.64.3</b> Question 1: Comment on any patterns you observe in the residual plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.65" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#tests-to-determine-nonlinearity"><i class="fa fa-check"></i><b>8.65</b> Tests to Determine Nonlinearity<span></span></a>
<ul>
<li class="chapter" data-level="8.65.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#lack-of-fit-test"><i class="fa fa-check"></i><b>8.65.1</b> Lack of Fit Test<span></span></a></li>
<li class="chapter" data-level="8.65.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-does-the-output-above-support-the-claim-that-the-income-variable-has-a-somewhat-nonlinear-pattern-explain-why-or-why-not."><i class="fa fa-check"></i><b>8.65.2</b> Question 2: Does the output above support the claim that the income variable has a somewhat nonlinear pattern? Explain why or why not.<span></span></a></li>
<li class="chapter" data-level="8.65.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#tukeys-test-for-nonadditivity"><i class="fa fa-check"></i><b>8.65.3</b> Tukey’s Test for Nonadditivity<span></span></a></li>
<li class="chapter" data-level="8.65.4" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-summarize-the-output-above-in-practical-terms."><i class="fa fa-check"></i><b>8.65.4</b> Question 3: Summarize the output above in practical terms.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.66" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#marginal-model-plot"><i class="fa fa-check"></i><b>8.66</b> Marginal Model Plot<span></span></a>
<ul>
<li class="chapter" data-level="8.66.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-interpret-the-output-of-the-marginal-model-plots-above."><i class="fa fa-check"></i><b>8.66.1</b> Question 4: Interpret the output of the marginal model plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.67" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#added-variable-plots"><i class="fa fa-check"></i><b>8.67</b> Added Variable Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.67.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#creating-added-variable-plots"><i class="fa fa-check"></i><b>8.67.1</b> Creating Added Variable Plots<span></span></a></li>
<li class="chapter" data-level="8.67.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#interpreting-added-variable-plots"><i class="fa fa-check"></i><b>8.67.2</b> Interpreting Added Variable Plots<span></span></a></li>
<li class="chapter" data-level="8.67.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-5-interpret-the-output-of-the-added-value-plots-above."><i class="fa fa-check"></i><b>8.67.3</b> Question 5: Interpret the output of the added value plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.68" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#component-plus-residual-plots"><i class="fa fa-check"></i><b>8.68</b> Component Plus Residual Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.68.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-6-do-the-cr-plots-provide-evidence-of-clear-nonlinear-relationships-for-any-of-the-variables"><i class="fa fa-check"></i><b>8.68.1</b> Question 6: Do the cr plots provide evidence of clear nonlinear relationships for any of the variables?<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html"><i class="fa fa-check"></i><b>9</b> Assessing and addressing collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#motivating-example-driver-seat-settings"><i class="fa fa-check"></i><b>9.1</b> Motivating Example: Driver Seat Settings<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-1-does-r2-value-of-the-model-seem-consistent-with-the-p-values-for-each-coefficient-why-or-why-not"><i class="fa fa-check"></i><b>9.1.1</b> Question 1: Does <span class="math inline">\(R^2\)</span> value of the model seem consistent with the p-values for each coefficient? Why or why not?<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-2-what-are-some-methods-we-have-use-to-check-for-correlations-between-regressors"><i class="fa fa-check"></i><b>9.1.2</b> Question 2: What are some methods we have use to check for correlations between regressors?<span></span></a></li>
<li class="chapter" data-level="9.1.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-3-do-you-see-any-large-pairwise-correlations-do-these-make-practical-sense-in-this-context"><i class="fa fa-check"></i><b>9.1.3</b> Question 3: Do you see any large pairwise correlations? Do these make practical sense in this context?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#collinearity"><i class="fa fa-check"></i><b>9.2</b> Collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#why-is-collinearity-problematic"><i class="fa fa-check"></i><b>9.2.1</b> Why is Collinearity Problematic?<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-4-comment-on-the-changes-to-the-coefficients-from-model-1-and-model-2."><i class="fa fa-check"></i><b>9.2.2</b> Question 4: Comment on the changes to the coefficients from Model 1 and Model 2.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#methods-for-detecting-collinearity"><i class="fa fa-check"></i><b>9.3</b> Methods for Detecting Collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#examine-the-correlation-matrix"><i class="fa fa-check"></i><b>9.3.1</b> Examine the Correlation Matrix<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#regress-on-predictors"><i class="fa fa-check"></i><b>9.3.2</b> Regress on Predictor(s)<span></span></a></li>
<li class="chapter" data-level="9.3.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#the-variance-inflation-factor-vif"><i class="fa fa-check"></i><b>9.3.3</b> The Variance Inflation Factor (VIF)<span></span></a></li>
<li class="chapter" data-level="9.3.4" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-5-which-predictors-seem-to-have-a-problem-with-collinearity"><i class="fa fa-check"></i><b>9.3.4</b> Question 5: Which predictors seem to have a problem with collinearity?<span></span></a></li>
<li class="chapter" data-level="9.3.5" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#interpreting-vif"><i class="fa fa-check"></i><b>9.3.5</b> Interpreting VIF<span></span></a></li>
<li class="chapter" data-level="9.3.6" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-6-play-around-with-model-and-see-what-happens-when-your-remove-correlated-variables."><i class="fa fa-check"></i><b>9.3.6</b> Question 6: Play around with model and see what happens when your remove correlated variables.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#examine-the-eigenvalues-of-xtx."><i class="fa fa-check"></i><b>9.4</b> Examine the eigenvalues of <span class="math inline">\(X^TX\)</span>.<span></span></a></li>
<li class="chapter" data-level="9.5" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#including-the-intercept-and-scaling-regressors"><i class="fa fa-check"></i><b>9.5</b> Including the Intercept and Scaling Regressors<span></span></a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#which-regressors-are-leading-to-large-condition-indices"><i class="fa fa-check"></i><b>9.5.1</b> Which Regressors are Leading to Large Condition Indices?<span></span></a></li>
<li class="chapter" data-level="9.5.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-7-based-on-the-output-above-which-regressors-should-be-removed-which-should-we-remove-first"><i class="fa fa-check"></i><b>9.5.2</b> Question 7: Based on the output above, which regressors should be removed? Which should we remove first?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#dropping-regressors-from-our-model"><i class="fa fa-check"></i><b>9.6</b> Dropping Regressors from our model<span></span></a></li>
<li class="chapter" data-level="9.7" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#wrap-up"><i class="fa fa-check"></i><b>9.7</b> Wrap-Up<span></span></a></li>
<li class="chapter" data-level="9.8" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#note-on-predictions"><i class="fa fa-check"></i><b>9.8</b> Note on predictions<span></span></a></li>
<li class="chapter" data-level="9.9" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-8-what-methods-can-be-utilized-to-look-for-and-fix-problems-with-collinearity"><i class="fa fa-check"></i><b>9.9</b> Question 8: What methods can be utilized to look for and fix problems with collinearity?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>10</b> Variable Selection<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="variable-selection.html"><a href="variable-selection.html#loading-the-data"><i class="fa fa-check"></i><b>10.1</b> Loading the Data<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="variable-selection.html"><a href="variable-selection.html#question-1-why-might-it-be-a-bad-a-idea-to-simply-include-all-regressors"><i class="fa fa-check"></i><b>10.2</b> Question 1: Why might it be a bad a idea to simply include all regressors?<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="variable-selection.html"><a href="variable-selection.html#question-2-what-metricsmethods-have-we-used-to-compare-models-thus-far"><i class="fa fa-check"></i><b>10.3</b> Question 2: What metrics/methods have we used to compare models thus far?<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="variable-selection.html"><a href="variable-selection.html#testing-based-procedures"><i class="fa fa-check"></i><b>10.4</b> Testing-Based Procedures<span></span></a></li>
<li class="chapter" data-level="10.5" data-path="variable-selection.html"><a href="variable-selection.html#backward-elimination"><i class="fa fa-check"></i><b>10.5</b> Backward elimination<span></span></a></li>
<li class="chapter" data-level="10.6" data-path="variable-selection.html"><a href="variable-selection.html#question-3-which-of-the-predictors-would-you-remove-from-the-full-model-what-criteria-did-you-use-to-make-that-decision"><i class="fa fa-check"></i><b>10.6</b> Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision?<span></span></a></li>
<li class="chapter" data-level="10.7" data-path="variable-selection.html"><a href="variable-selection.html#forward-selection"><i class="fa fa-check"></i><b>10.7</b> Forward Selection<span></span></a></li>
<li class="chapter" data-level="10.8" data-path="variable-selection.html"><a href="variable-selection.html#stepwise-regression"><i class="fa fa-check"></i><b>10.8</b> Stepwise Regression<span></span></a></li>
<li class="chapter" data-level="10.9" data-path="variable-selection.html"><a href="variable-selection.html#model-hierarchy"><i class="fa fa-check"></i><b>10.9</b> Model Hierarchy<span></span></a></li>
<li class="chapter" data-level="10.10" data-path="variable-selection.html"><a href="variable-selection.html#criterion-based-procedures"><i class="fa fa-check"></i><b>10.10</b> Criterion-Based Procedures<span></span></a></li>
<li class="chapter" data-level="10.11" data-path="variable-selection.html"><a href="variable-selection.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>10.11</b> Akaike’s Information Criterion (AIC)<span></span></a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="variable-selection.html"><a href="variable-selection.html#interpreting-aic"><i class="fa fa-check"></i><b>10.11.1</b> Interpreting AIC<span></span></a></li>
<li class="chapter" data-level="10.11.2" data-path="variable-selection.html"><a href="variable-selection.html#exhaustive-model-searches"><i class="fa fa-check"></i><b>10.11.2</b> Exhaustive Model Searches<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="variable-selection.html"><a href="variable-selection.html#question-4-interpret-the-output-from-the-code-above.-is-this-consistent-with-the-model-we-obtained-using-backward-elimination"><i class="fa fa-check"></i><b>10.12</b> Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination?<span></span></a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="variable-selection.html"><a href="variable-selection.html#computing-the-aic"><i class="fa fa-check"></i><b>10.12.1</b> Computing the AIC<span></span></a></li>
<li class="chapter" data-level="10.12.2" data-path="variable-selection.html"><a href="variable-selection.html#question-5-interpret-the-output-from-the-aic-plots-above.-what-is-the-best-model-according-to-this-metric"><i class="fa fa-check"></i><b>10.12.2</b> Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="variable-selection.html"><a href="variable-selection.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>10.13</b> Bayesian Information Criterion (BIC)<span></span></a></li>
<li class="chapter" data-level="10.14" data-path="variable-selection.html"><a href="variable-selection.html#summary-2"><i class="fa fa-check"></i><b>10.14</b> Summary<span></span></a></li>
<li class="chapter" data-level="10.15" data-path="variable-selection.html"><a href="variable-selection.html#exercise"><i class="fa fa-check"></i><b>10.15</b> Exercise<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>11</b> Transformations<span></span></a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="transformations.html"><a href="transformations.html#question-7-compare-the-graphic-below-with-the-type-of-bulge-seen-in-your-data-move-along-the-ladder-of-transformations-for-your-response-or-predictors-to-determine-a-helpful-transformation."><i class="fa fa-check"></i><b>11.0.1</b> Question 7: Compare the graphic below with the type of “bulge” seen in your data; move along the “ladder of transformations” for your response or predictors to determine a helpful transformation.<span></span></a></li>
<li class="chapter" data-level="11.0.2" data-path="transformations.html"><a href="transformations.html#question-8-which-transformation-of-income-seems-like-a-better-fit"><i class="fa fa-check"></i><b>11.0.2</b> Question 8: Which transformation of income seems like a better fit?<span></span></a></li>
<li class="chapter" data-level="11.0.3" data-path="transformations.html"><a href="transformations.html#polynomial-transformations"><i class="fa fa-check"></i><b>11.0.3</b> Polynomial Transformations<span></span></a></li>
<li class="chapter" data-level="11.0.4" data-path="transformations.html"><a href="transformations.html#logistic-transforation"><i class="fa fa-check"></i><b>11.0.4</b> Logistic Transforation<span></span></a></li>
<li class="chapter" data-level="11.0.5" data-path="transformations.html"><a href="transformations.html#closing-comments-on-transformations"><i class="fa fa-check"></i><b>11.0.5</b> Closing Comments on Transformations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="advanced-regressor-variables.html"><a href="advanced-regressor-variables.html"><i class="fa fa-check"></i><b>12</b> Advanced regressor variables<span></span></a></li>
<li class="chapter" data-level="13" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html"><i class="fa fa-check"></i><b>13</b> More on categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#indicatordummy-variables"><i class="fa fa-check"></i><b>13.1</b> Indicator/dummy variables<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#common-of-linear-models-with-categorical-predictors"><i class="fa fa-check"></i><b>13.2</b> Common of linear models with categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#one-way-anova"><i class="fa fa-check"></i><b>13.2.1</b> One-way ANOVA<span></span></a></li>
<li class="chapter" data-level="13.2.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#main-effects-models"><i class="fa fa-check"></i><b>13.2.2</b> Main effects models<span></span></a></li>
<li class="chapter" data-level="13.2.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interaction-models"><i class="fa fa-check"></i><b>13.2.3</b> Interaction models<span></span></a></li>
<li class="chapter" data-level="13.2.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#extensions-1"><i class="fa fa-check"></i><b>13.2.4</b> Extensions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploring-data-for-one-factor-models-example-with-united-nations-dataset"><i class="fa fa-check"></i><b>13.3</b> Exploring Data for One-Factor Models: Example with United Nations Dataset<span></span></a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#load-the-un11-dataset-below-and-read-the-help-documentation-to-familiarize-yourself-with-the-data.-how-many-observations-are-in-the-dataset-how-many-variables"><i class="fa fa-check"></i><b>13.3.1</b> 1. Load the <code>UN11</code> dataset below and read the help documentation to familiarize yourself with the data. How many observations are in the dataset? How many variables?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>13.4</b> Exploratory Data Analysis<span></span></a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#create-side-by-side-boxplots-for-the-variable-lifeexpf-for-each-of-the-categories-of-the-group-variable-and-describe-any-interesting-observations-about-the-data."><i class="fa fa-check"></i><b>13.4.1</b> 2. Create side-by-side boxplots for the variable <strong>lifeExpF</strong> for each of the categories of the <strong>group</strong> variable and describe any interesting observations about the data.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#how-do-we-include-factor-variables-as-regressors"><i class="fa fa-check"></i><b>13.5</b> How do we include factor variables as regressors?<span></span></a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#explain-what-the-code-below-is-doing-what-happens-if-we-remove-the-0-from-the-commands"><i class="fa fa-check"></i><b>13.5.1</b> 3. Explain what the code below is doing? What happens if we remove the <span class="math inline">\(+ 0\)</span> from the commands?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#the-one-factor-model"><i class="fa fa-check"></i><b>13.6</b> The one-factor model<span></span></a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#let-y-denote-the-response-variable-lifeexpf.-what-would-be-problematic-about-using-the-linear-model-proposed-below"><i class="fa fa-check"></i><b>13.6.1</b> 4. Let <span class="math inline">\(Y\)</span> denote the response variable <code>lifeExpF</code>. What would be problematic about using the linear model proposed below?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-ggplot2"><i class="fa fa-check"></i><b>13.7</b> Exploratory Data Analsis with <code>ggplot2</code><span></span></a></li>
<li class="chapter" data-level="13.8" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-lattice"><i class="fa fa-check"></i><b>13.8</b> Exploratory Data Analsis with <code>lattice</code><span></span></a></li>
<li class="chapter" data-level="13.9" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-base-graphics"><i class="fa fa-check"></i><b>13.9</b> Exploratory Data Analsis with Base Graphics<span></span></a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#what-are-some-interesting-observationspatterns-you-can-see-in-the-plots-above"><i class="fa fa-check"></i><b>13.9.1</b> 1. What are some interesting observations/patterns you can see in the plots above?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#comparing-different-models"><i class="fa fa-check"></i><b>13.10</b> Comparing Different Models<span></span></a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-simple-linear-model-with-logppgdp-as-the-regressor-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.1</b> 2. Enter code to fit a simple linear model with <code>log(ppgdp)</code> as the regressor and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-one-way-model-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.2</b> 3. Enter code to fit a one-way model with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-main-effects-model-with-logppgdp-as-a-predictor-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.3</b> 4. Enter code to fit a main effects model with <code>log(ppgdp)</code> as a predictor with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-an-interaction-model-with-logppgdp-as-a-predictor-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response"><i class="fa fa-check"></i><b>13.10.4</b> 5. Enter code to fit an interaction model with <code>log(ppgdp)</code> as a predictor with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#model-matrix-for-the-interaction-model"><i class="fa fa-check"></i><b>13.11</b> Model Matrix for the Interaction Model<span></span></a>
<ul>
<li class="chapter" data-level="13.11.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#how-are-the-last-two-columns-of-x-computed"><i class="fa fa-check"></i><b>13.11.1</b> 6. How are the last two columns of <span class="math inline">\(X\)</span> computed?<span></span></a></li>
<li class="chapter" data-level="13.11.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#find-formulas-for-elifeexpf-vert-ppgdp-x-groupoecd-elifeexpf-vert-ppgdp-x-group-other-and-elifeexpf-vert-ppgdp-x-group-africa."><i class="fa fa-check"></i><b>13.11.2</b> 7. Find formulas for E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group=oecd), E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = other) and E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = africa).<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#plotting-the-interaction-model"><i class="fa fa-check"></i><b>13.12</b> Plotting the Interaction Model<span></span></a>
<ul>
<li class="chapter" data-level="13.12.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interpret-each-coefficient-in-your-previous-formulas-in-the-context-of-this-dataset."><i class="fa fa-check"></i><b>13.12.1</b> 8. Interpret each coefficient in your previous formulas in the context of this dataset.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.13" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interpreting-coefficients-in-the-interaction-model"><i class="fa fa-check"></i><b>13.13</b> Interpreting Coefficients in the Interaction Model<span></span></a></li>
<li class="chapter" data-level="13.14" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-interaction-model"><i class="fa fa-check"></i><b>13.14</b> Effects Plot for Interaction Model<span></span></a></li>
<li class="chapter" data-level="13.15" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-interaction-model-with-log-scale"><i class="fa fa-check"></i><b>13.15</b> Effects Plot for Interaction Model with Log Scale<span></span></a>
<ul>
<li class="chapter" data-level="13.15.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#what-are-some-interesting-observations-we-can-infer-from-the-plot-above"><i class="fa fa-check"></i><b>13.15.1</b> 9. What are some interesting observations we can infer from the plot above?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.16" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#main-effects-model"><i class="fa fa-check"></i><b>13.16</b> Main Effects Model<span></span></a>
<ul>
<li class="chapter" data-level="13.16.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#find-formulas-for-elifeexpf-vert-ppgdp-x-groupoecd-elifeexpf-vert-ppgdp-x-group-other-and-elifeexpf-vert-ppgdp-x-group-africa.-1"><i class="fa fa-check"></i><b>13.16.1</b> 10. Find formulas for E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group=oecd), E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = other) and E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = africa).<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.17" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#plotting-the-main-effects-model"><i class="fa fa-check"></i><b>13.17</b> Plotting the Main Effects Model<span></span></a></li>
<li class="chapter" data-level="13.18" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-the-main-effects-model"><i class="fa fa-check"></i><b>13.18</b> Effects Plot for the Main Effects Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="advanced-model-structure.html"><a href="advanced-model-structure.html"><i class="fa fa-check"></i><b>14</b> Advanced model structure<span></span></a></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html"><i class="fa fa-check"></i><b>A</b> Overview of matrix facts<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#notation"><i class="fa fa-check"></i><b>A.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="A.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>A.2</b> Basic mathematical operations<span></span></a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>A.2.1</b> Addition and subtraction<span></span></a></li>
<li class="chapter" data-level="A.2.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>A.2.2</b> Scalar multiplication<span></span></a></li>
<li class="chapter" data-level="A.2.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>A.2.3</b> Matrix multiplication<span></span></a></li>
<li class="chapter" data-level="A.2.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose"><i class="fa fa-check"></i><b>A.2.4</b> Transpose<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>A.3</b> Basic mathematical properties<span></span></a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>A.3.1</b> Associative property<span></span></a></li>
<li class="chapter" data-level="A.3.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>A.3.2</b> Distributive property<span></span></a></li>
<li class="chapter" data-level="A.3.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>A.3.3</b> No commutative property<span></span></a></li>
<li class="chapter" data-level="A.3.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose-related-properties"><i class="fa fa-check"></i><b>A.3.4</b> Transpose-related properties<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>A.4</b> Special matrices<span></span></a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>A.4.1</b> Square matrices<span></span></a></li>
<li class="chapter" data-level="A.4.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>A.4.2</b> Identity matrix<span></span></a></li>
<li class="chapter" data-level="A.4.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#diagonal-matrices"><i class="fa fa-check"></i><b>A.4.3</b> Diagonal matrices<span></span></a></li>
<li class="chapter" data-level="A.4.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#symmetric-matrices"><i class="fa fa-check"></i><b>A.4.4</b> Symmetric matrices<span></span></a></li>
<li class="chapter" data-level="A.4.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#idempotent-matrices"><i class="fa fa-check"></i><b>A.4.5</b> Idempotent matrices<span></span></a></li>
<li class="chapter" data-level="A.4.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.4.6</b> Positive definite matrices<span></span></a></li>
<li class="chapter" data-level="A.4.7" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#inverse-matrix"><i class="fa fa-check"></i><b>A.4.7</b> Inverse matrix<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>A.5</b> Matrix derivatives<span></span></a></li>
<li class="chapter" data-level="A.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#additional-topics"><i class="fa fa-check"></i><b>A.6</b> Additional topics<span></span></a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#determinant"><i class="fa fa-check"></i><b>A.6.1</b> Determinant<span></span></a></li>
<li class="chapter" data-level="A.6.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#linearly-independent-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Linearly independent vectors<span></span></a></li>
<li class="chapter" data-level="A.6.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#rank"><i class="fa fa-check"></i><b>A.6.3</b> Rank<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="prob-review.html"><a href="prob-review.html"><i class="fa fa-check"></i><b>B</b> Overview of probability, random variables, and random vectors<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="prob-review.html"><a href="prob-review.html#probability-basics"><i class="fa fa-check"></i><b>B.1</b> Probability Basics<span></span></a></li>
<li class="chapter" data-level="B.2" data-path="prob-review.html"><a href="prob-review.html#random-variables"><i class="fa fa-check"></i><b>B.2</b> Random Variables<span></span></a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="prob-review.html"><a href="prob-review.html#discrete-random-variables"><i class="fa fa-check"></i><b>B.2.1</b> Discrete random variables<span></span></a></li>
<li class="chapter" data-level="B.2.2" data-path="prob-review.html"><a href="prob-review.html#continuous-random-variables"><i class="fa fa-check"></i><b>B.2.2</b> Continuous random variables<span></span></a></li>
<li class="chapter" data-level="B.2.3" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-random-variables"><i class="fa fa-check"></i><b>B.2.3</b> Useful facts for transformations of random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="prob-review.html"><a href="prob-review.html#multivariate-distributions"><i class="fa fa-check"></i><b>B.3</b> Multivariate distributions<span></span></a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="prob-review.html"><a href="prob-review.html#basic-properties"><i class="fa fa-check"></i><b>B.3.1</b> Basic properties<span></span></a></li>
<li class="chapter" data-level="B.3.2" data-path="prob-review.html"><a href="prob-review.html#marginal-distributions"><i class="fa fa-check"></i><b>B.3.2</b> Marginal distributions<span></span></a></li>
<li class="chapter" data-level="B.3.3" data-path="prob-review.html"><a href="prob-review.html#independence-of-random-variables"><i class="fa fa-check"></i><b>B.3.3</b> Independence of random variables<span></span></a></li>
<li class="chapter" data-level="B.3.4" data-path="prob-review.html"><a href="prob-review.html#conditional-distributions"><i class="fa fa-check"></i><b>B.3.4</b> Conditional distributions<span></span></a></li>
<li class="chapter" data-level="B.3.5" data-path="prob-review.html"><a href="prob-review.html#covariance"><i class="fa fa-check"></i><b>B.3.5</b> Covariance<span></span></a></li>
<li class="chapter" data-level="B.3.6" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>B.3.6</b> Useful facts for transformations of multiple random variables<span></span></a></li>
<li class="chapter" data-level="B.3.7" data-path="prob-review.html"><a href="prob-review.html#binomial-distribution-example"><i class="fa fa-check"></i><b>B.3.7</b> Binomial distribution example<span></span></a></li>
<li class="chapter" data-level="B.3.8" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example"><i class="fa fa-check"></i><b>B.3.8</b> Continuous bivariate distribution example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="prob-review.html"><a href="prob-review.html#random-vectors"><i class="fa fa-check"></i><b>B.4</b> Random vectors<span></span></a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="prob-review.html"><a href="prob-review.html#definition-1"><i class="fa fa-check"></i><b>B.4.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="B.4.2" data-path="prob-review.html"><a href="prob-review.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>B.4.2</b> Mean, variance, and covariance<span></span></a></li>
<li class="chapter" data-level="B.4.3" data-path="prob-review.html"><a href="prob-review.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Properties of transformations of random vectors<span></span></a></li>
<li class="chapter" data-level="B.4.4" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example-continued"><i class="fa fa-check"></i><b>B.4.4</b> Continuous bivariate distribution example continued<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="prob-review.html"><a href="prob-review.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>B.5</b> Multivariate normal (Gaussian) distribution<span></span></a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="prob-review.html"><a href="prob-review.html#definition-2"><i class="fa fa-check"></i><b>B.5.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="B.5.2" data-path="prob-review.html"><a href="prob-review.html#linear-functions-of-a-multivariate-normal-random-vector"><i class="fa fa-check"></i><b>B.5.2</b> Linear functions of a multivariate normal random vector<span></span></a></li>
<li class="chapter" data-level="B.5.3" data-path="prob-review.html"><a href="prob-review.html#ols-example"><i class="fa fa-check"></i><b>B.5.3</b> OLS example<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="chapter" data-level="C" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html"><i class="fa fa-check"></i><b>C</b> Defining and fitting a linear model<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#background-and-terminology"><i class="fa fa-check"></i><b>C.1</b> Background and terminology<span></span></a></li>
<li class="chapter" data-level="C.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#goals-of-regression"><i class="fa fa-check"></i><b>C.2</b> Goals of regression<span></span></a></li>
<li class="chapter" data-level="C.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>C.3</b> Definition of a linear model<span></span></a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#basic-construction-and-relationships"><i class="fa fa-check"></i><b>C.3.1</b> Basic construction and relationships<span></span></a></li>
<li class="chapter" data-level="C.3.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#as-a-system-of-equations"><i class="fa fa-check"></i><b>C.3.2</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="C.3.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#using-matrix-notation"><i class="fa fa-check"></i><b>C.3.3</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#summarizing-the-components-of-a-linear-model"><i class="fa fa-check"></i><b>C.4</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="C.5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#types-of-regression-models"><i class="fa fa-check"></i><b>C.5</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="C.6" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#standard-linear-model-assumptions-and-implications"><i class="fa fa-check"></i><b>C.6</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="C.7" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#parameter-estimation-for-linear-models"><i class="fa fa-check"></i><b>C.7</b> Parameter estimation for linear models<span></span></a></li>
<li class="chapter" data-level="C.8" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#ols-estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>C.8</b> OLS estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#visualizing-the-rss-as-a-function-of-the-estimated-coefficients"><i class="fa fa-check"></i><b>C.8.1</b> Visualizing the RSS as a function of the estimated coefficients<span></span></a></li>
<li class="chapter" data-level="C.8.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#ols-estimators-of-the-simple-linear-regression-parameters-1"><i class="fa fa-check"></i><b>C.8.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>C.9</b> Penguins simple linear regression example<span></span></a>
<ul>
<li class="chapter" data-level="C.9.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#derivation-of-ols-simple-linear-regression-estimators"><i class="fa fa-check"></i><b>C.9.1</b> Derivation of OLS simple linear regression estimators<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Joshua French</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpreting-a-fitted-linear-model" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Interpreting a fitted linear model<a href="interpreting-a-fitted-linear-model.html#interpreting-a-fitted-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Interpreting a fitted model is a critical part of a regression analysis and aids us in determining the roles and impact each variable plays in describing the behavior of the response variable.</p>
<div id="standard-mathemtical-interpretation" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Standard mathemtical interpretation<a href="interpreting-a-fitted-linear-model.html#standard-mathemtical-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The standard approach to interpreting the coefficients of a fitted linear model is to consider the expected change in the response in relation to the regressors in the model.</p>
<p>Consider the typical multiple linear regression model of the response
<span class="math display" id="eq:mlr-equation-ch4">\[
\begin{equation}
Y=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon.\tag{4.1}
\end{equation}
\]</span></p>
<p>As discussed in Chapter <a href="linear-model-estimation.html#linear-model-estimation">3</a>, we treat the values of our regressor variables as being fixed, known values. The error term is treated as a random variable, and consequently, the response variable is also a random variable. Additionally, we assume that the errors all have mean 0, conditional on the values of the regressor variables. More formally, we write this assumption as</p>
<p><span class="math display" id="eq:mean-error-assumption">\[\begin{equation}
E(\epsilon \mid X_1, X_2, \ldots, X_{p-1})=0.\tag{4.2}

\end{equation}\]</span></p>
<p>Recall that we use the notation <span class="math inline">\(\mathbb{X} = \{X_1,\ldots,X_{p-1}\}\)</span> to denote the set of all regressors, which will help us simplify the derivations below. Thus, the assumption in Equation <a href="interpreting-a-fitted-linear-model.html#eq:mean-error-assumption">(4.2)</a> can be expressed as <span class="math inline">\(E(\epsilon \mid \mathbb{X})=0\)</span>. Using the assumption in Equation <a href="interpreting-a-fitted-linear-model.html#eq:mean-error-assumption">(4.2)</a> and applying it to the model in Equation <a href="interpreting-a-fitted-linear-model.html#eq:mlr-equation-ch4">(4.1)</a>, we see that
<span class="math display">\[
\begin{align}
&amp; E(Y\mid X_1, X_2, \ldots, X_{p-1}) \\
&amp;= E(Y \mid \mathbb{X}) \\
&amp;= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}+\epsilon \mid \mathbb{X}) \\
&amp;= E(\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}\mid \mathbb{X}) + E(\epsilon \mid \mathbb{X}) \\
&amp;=\beta_0+\beta_1 X_1 +\ldots + \beta_{p-1}X_{p-1}
\end{align}
\]</span>
since all terms in the first summand of line 4 are fixed, non-random values conditional on <span class="math inline">\(\mathbb{X}\)</span> and the second summand is 0 by assumption. If you are rusty with properties of random variables, consider reviewing the material in Appendix <a href="prob-review.html#prob-review">B</a>.</p>
<p>Using the facts above we discuss interpretation of simple linear regression models, multiple linear regression models with basic numeric predictors as regressors, and interpretation for parallel and separate lines regression model.</p>
<div id="coefficient-interpretation-in-simple-linear-regression" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Coefficient interpretation in simple linear regression<a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-in-simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a simple linear regression model, so that
<span class="math display" id="eq:slr-equation">\[\begin{equation}
E(Y\mid X)=\beta_0 + \beta_1 X. \tag{4.3}
\end{equation}\]</span>
The interpretations of the coefficients are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when the regressor is 0, i.e., <span class="math inline">\(\beta_0=E(Y\mid X=0)\)</span>.</li>
<li><span class="math inline">\(\beta_1\)</span> is the expected change in the response when the regressor increases 1 unit, i.e., <span class="math inline">\(\beta_1=E(Y\mid X=x^*+1)-E(Y\mid X=x^*)\)</span>, where <span class="math inline">\(x^*\)</span> is a fixed real number.</li>
</ul>
<p>Regarding the interpretation of <span class="math inline">\(\beta_0\)</span>, from the regression model in Equation <a href="interpreting-a-fitted-linear-model.html#eq:slr-equation">(4.3)</a>, notice that
<span class="math display">\[
\begin{align}
E(Y\mid X = 0) &amp;= \beta_0 + \beta_1 \cdot 0 \\
&amp;= \beta_0.
\end{align}
\]</span>
This is why <span class="math inline">\(\beta_0\)</span> is the expected value of the response variable when the regressor is zero.</p>
<p>Similarly, for <span class="math inline">\(\beta_1\)</span>, we notice that
<span class="math display">\[
\begin{align}
E(Y\mid X=x^*+1)-E(Y\mid X=x^*) &amp;= [\beta_0 + \beta_1 (x^* + 1)] - [\beta_0 + \beta_1 x^*] \\
&amp;= \beta_1.
\end{align}
\]</span>
Thus, <span class="math inline">\(\beta_1\)</span> literally equals the change in the expected response when the regressor increases by 1 unit.</p>
</div>
<div id="coefficient-interpretation-for-first-order-multiple-linear-regression-models" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Coefficient interpretation for first-order multiple linear regression models<a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-for-first-order-multiple-linear-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a multiple linear regression model, so that
<span class="math display" id="eq:mlr-equation">\[\begin{equation}
E(Y\mid X_1,\ldots,X_{p-1})=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1}.\tag{4.4}
\end{equation}\]</span>
Extending the definition of <span class="math inline">\(\mathbb{X}\)</span>, we denote the set of regressors without <span class="math inline">\(X_j\)</span> as <span class="math inline">\(\mathbb{X}_{-j} = \mathbb{X}\setminus\{X_j\}\)</span>.</p>
<p>The interpretations of the coefficients more the model in Equation <a href="interpreting-a-fitted-linear-model.html#eq:mlr-equation">(4.4)</a> are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when all regressors are 0, i.e., <span class="math inline">\(\beta_0=E(Y\mid X_1=0,\ldots,X_{p-1}=0)\)</span>.</li>
<li><span class="math inline">\(\beta_j\)</span> is the expected change in the response when regressor <span class="math inline">\(j\)</span> increases 1 unit and the other regressors stay the same, i.e., <span class="math inline">\(\beta_j=E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*)\)</span> where <span class="math inline">\(\mathbf{x}_{-j}^*=[x^*_1,\ldots,x_{j-1}^*,x_{j+1}^*,\ldots,x_{p-1}^*]\in \mathbb{R}^{p-2}\)</span> is a vector with <span class="math inline">\(p-2\)</span> fixed values (the number of regressors excluding <span class="math inline">\(X_j\)</span>) and <span class="math inline">\(x_j^*\)</span> is a fixed real number.</li>
</ul>
<p>Regarding the interpretation of <span class="math inline">\(\beta_0\)</span>, from the regression model in Equation <a href="interpreting-a-fitted-linear-model.html#eq:mlr-equation">(4.4)</a>, notice that
<span class="math display">\[
\begin{align}
E(Y\mid X=x^*+1)-E(Y\mid X=x^*) &amp;= [\beta_0 + \beta_1 (x^* + 1)] - [\beta_0 + \beta_1 x^*] \\
&amp;= \beta_1.
\end{align}
\]</span></p>
<p>It is quite common for the mathematical interpretation of the intercept to be nonsensical because we are extrapolating outside the range of the observed data. e.g., we might predict the height of a newborn child (age = 0) to be negative, which isn’t possible in reality.</p>
<p>For <span class="math inline">\(\beta_j\)</span>, we notice that
<span class="math display">\[
\begin{align}
&amp; E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j}+1)-E(Y\mid \mathbb{X}_{-j} = \mathbf{x}^*_{-j}, X_{j+1} = x_{j})\\
&amp;=  \biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_j(x^*_j+1) + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl] \\
&amp;\quad -\biggl[\beta_0 + \sum_{k=1}^{j-1}\beta_kx^*_k + \beta_jx^*_j + \sum_{k=j+1}^{p-1}\beta_kx^*_k\biggl]\\
&amp;= \beta_j.
\end{align}
\]</span></p>
<p>A notable problem with the mathematical interpretation of multiple regression models is that a single predictor can be used more than once in the model. E.g., in the regression model <span class="math inline">\(E(Y\mid X) = \beta_0 + \beta_1 X + \beta_2 X^2\)</span>, <span class="math inline">\(X\)</span> is used in both the second and third terms. So it is not possible to increase <span class="math inline">\(X\)</span> while keeping <span class="math inline">\(X^2\)</span> fixed. The mathematical interpretation given in this section is applicable to first-order linear regression models, where a <strong>first-order linear regression model</strong> is a multiple linear regression model in which no regressor is a function of any other regressor.</p>
<p>Another notable problem is that the regressors used in our model our often observational in nature, meaning that we do not control them. Thus, it doesn’t make sense to say “we increase variable <span class="math inline">\(X\)</span> by 1 unit.” An alternative approach, suggested in <span class="citation"><a href="#ref-lmwr2" role="doc-biblioref">Faraway</a> (<a href="#ref-lmwr2" role="doc-biblioref">2014</a>)</span>, is to consider the expected response difference between observations that are identical with respect to all attributes except the variable under consideration, which varies by only a single unit. While mathematically, the result is the same, the interpretation is more philosophically palatable.</p>
</div>
<div id="penguins-examples" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Penguins examples<a href="interpreting-a-fitted-linear-model.html#penguins-examples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To apply the examples given above, we interpret the models fit to the <code>penguins</code> data introduced in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a> and continued in Section <a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>.</p>
<p>From Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>, the fitted simple linear regression model is
<span class="math display">\[
\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g})=26.9+0.004 \,\mathtt{body\_mass\_g}.
\]</span></p>
<p>The expected bill length of a penguin with a body mass of 0 grams is 26.9 mm. We discussed the absurdity of this interpretation in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>. If two penguins were identical except that one penguin was 1 gram heavier, then we would expect the heavier penguin to have a bill length 0.004 mm longer.</p>
<p>From Section <a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>, the fitted multiple linear regression model is
<span class="math display">\[
\hat{E}(\mathtt{bill\_length\_mm}\mid \mathtt{body\_mass\_g}, \mathtt{flipper\_length\_mm})=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\]</span> We discuss basic interpretation of multiple linear regression model in Chapter @ref(interpreting-a-fitted linear-model).</p>
<p>Some interpretations of the coefficients are:</p>
<ul>
<li>We expect a penguin with a body mass of 0 grams and a flipper length of 0 mm to to have a bill length of -3.44 mm.</li>
<li>For two penguins that are identical except that one penguin has a body mass 1 gram larger, we expect the heavier penguin to have a bill length 0.0007 mm longer than the other penguin.</li>
<li>For two penguins that are identical except that one penguin has a flipper length 1 mm longer, we expect the penguin with longer flippers to have a bill length 0.22 mm longer.</li>
</ul>
</div>
</div>
<div id="going-deeper-1" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Going deeper<a href="interpreting-a-fitted-linear-model.html#going-deeper-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="orthogonality" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Orthogonality<a href="interpreting-a-fitted-linear-model.html#orthogonality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let
<span class="math display">\[\mathbf{X}_[j]=[x_{1,j},\ldots,x_{n,j}]\]</span>
denote the <span class="math inline">\(n\times 1\)</span> column vector of observed values for regressor <span class="math inline">\(X_j\)</span>. (We can’t use the notation <span class="math inline">\(\mathbf{x}_j\)</span> because that is the <span class="math inline">\(p\times 1\)</span> vector of regressor values for the <span class="math inline">\(j\)</span>th observation). Regressors, <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> are <strong>orthogonal</strong> if <span class="math inline">\(\mathbf{X}_{[j]}^T \mathbf{X}_{[k]}=0\)</span>.</p>
<p>Let <span class="math inline">\(\boldsymbol{1}_{n\times1}\)</span> denote an <span class="math inline">\(n\times 1\)</span> column vector of 1s. The definition of orthogonal vectors above implies that <span class="math inline">\(\mathbf{X}_{[j]}\)</span> is orthogonal to <span class="math inline">\(\boldsymbol{1}_{n\times1}\)</span> if <span class="math display">\[
\mathbf{X}_{[j]}^T \boldsymbol{1}_{n\times1} = \sum_{i=1}^n x_{i,j} = 0,\]</span>
i.e., if the values in <span class="math inline">\(\mathbf{X}_{[j]}\)</span> sum to zero.</p>
<p>Let <span class="math inline">\(\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}\)</span> denote the sample mean of <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\bar{\mathbf{x}}_j = \bar{x}_j \boldsymbol{1}_{n\times 1}\)</span> denote the column vector that repeats <span class="math inline">\(\bar{x}_j\)</span> <span class="math inline">\(n\)</span> times.</p>
<p><strong>Centering</strong> <span class="math inline">\(\mathbf{X}_{[j]}\)</span> involves subtracting the sample mean of <span class="math inline">\(\mathbf{X}_{[j]}\)</span> from <span class="math inline">\(\mathbf{X}_{[j]}\)</span>, i.e., <span class="math inline">\(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j\)</span>.</p>
<p>Regressors <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> are <strong>uncorrelated</strong> if they are orthogonal after being centered, i.e., if
<span class="math display">\[
(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k)=0.
\]</span>
Note that the sample covariance between vectors <span class="math inline">\(\mathbf{X}_{[j]}\)</span> and <span class="math inline">\(\mathbf{X}_{[k]}\)</span> is
<span class="math display">\[
\begin{align*}
\widehat{\mathrm{cov}}(\mathbf{X}_{[j]}, \mathbf{X}_{[k]}) &amp;= \frac{1}{n-1}\sum_{i=1}^n (x_{i,j} - \bar{x}_j)(x_{i,k} - \bar{x}_k) \\
 &amp;= \frac{1}{n-1}(\mathbf{X}_{[j]} - \bar{\mathbf{x}}_j)^T (\mathbf{X}_{[k]} - \bar{\mathbf{x}}_k).
\end{align*}
\]</span>
Thus, two centered regressors are orthogonal if their covariance is zero.</p>
<p>It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response. Specifically:</p>
<p><em>If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors.</em></p>
<p>Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s.</p>
<p>We consider a simple example with <span class="math inline">\(n=5\)</span> observations to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below:</p>
<ul>
<li><code>y</code> is a vector of response values.</li>
<li><code>ones</code> is the column vector of 1s.</li>
<li><code>X1</code> is a column vector of regressor values.</li>
<li><code>X2</code> is a column vector of regressor values chosen to be orthogonal to <code>x1</code> but not to <code>ones</code>.</li>
<li><code>X3</code> is a column vector of regressor values orthogonal to both <code>x1</code> and <code>ones</code>.</li>
<li><code>X4</code> is a column vector of regressor values orthogonal to <code>ones</code>, <code>x1</code>, and <code>x3</code>, but not <code>x2</code>.</li>
<li><code>X5</code> is a column vector of regressor value sorthogonal to <code>ones</code> and <code>x1</code>, but not the other regressor vectors.</li>
</ul>
<p>In the code below, we define vectors <code>y</code>, <code>X1</code>, and <code>X2</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="interpreting-a-fitted-linear-model.html#cb1-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">9</span>)       <span class="co"># create an arbitrary response vector</span></span>
<span id="cb1-2"><a href="interpreting-a-fitted-linear-model.html#cb1-2" aria-hidden="true" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">7</span>)      <span class="co"># create regressor 1</span></span>
<span id="cb1-3"><a href="interpreting-a-fitted-linear-model.html#cb1-3" aria-hidden="true" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">5</span><span class="sc">/</span><span class="dv">7</span>)  <span class="co"># create regressor 2 to be orthogonal to x1</span></span></code></pre></div>
<p>Note that the <code>crossprod</code> function computes the crossproduct of two vectors or matrices, so that <code>crossprod(A, B)</code> computes <span class="math inline">\(\mathbf{A}^T B\)</span>, where the vectors or matrices must have the correct dimension for the multiplication to be performed.</p>
<p>The regressor vectors <code>X1</code> and <code>X2</code> are orthogonal since their crossproduct <span class="math inline">\(\mathbf{X}_{[1]}^T \mathbf{X}_{[2]}\)</span> (in R, <code>crossprod(X1, X2)</code>) equals zero, as shown in the code below.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="interpreting-a-fitted-linear-model.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># crossproduct is zero, so X1 and X2 are orthogonal</span></span>
<span id="cb2-2"><a href="interpreting-a-fitted-linear-model.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(X1, X2) </span></code></pre></div>
<pre><code>##      [,1]
## [1,]    0</code></pre>
<p>In the code below, we regress <code>y</code> on <code>x1</code> without an intercept (<code>lmod1</code>). The estimated coefficient for <code>X1</code> is <span class="math inline">\(\hat{\beta}_1=0.893\)</span>. Next, we then regress <code>y</code> on <code>X1</code> and <code>X2</code> without an intercept (<code>lmod2</code>). The estimated coefficients for <code>X1</code> and <code>X2</code> are <span class="math inline">\(\hat{\beta}_1=0.893\)</span> and <span class="math inline">\(\hat{\beta}_2=0.221\)</span>, respectively. Because <code>X1</code> and <code>X2</code> are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for <code>X1</code> stays the same in both models.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="interpreting-a-fitted-linear-model.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># y regressed on X1 without an intercept</span></span>
<span id="cb4-2"><a href="interpreting-a-fitted-linear-model.html#cb4-2" aria-hidden="true" tabindex="-1"></a>lmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb4-3"><a href="interpreting-a-fitted-linear-model.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod1)</span></code></pre></div>
<pre><code>##       x1 
## 0.893401</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="interpreting-a-fitted-linear-model.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># y regressed on X1 and X2 without an intercept</span></span>
<span id="cb6-2"><a href="interpreting-a-fitted-linear-model.html#cb6-2" aria-hidden="true" tabindex="-1"></a>lmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2 <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb6-3"><a href="interpreting-a-fitted-linear-model.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod2)</span></code></pre></div>
<pre><code>##        x1        x2 
## 0.8934010 0.2210526</code></pre>
<p>The previous models (<code>lmod1</code> and <code>lmod2</code>) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our <span class="math inline">\(\mathbf{X}\)</span> matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s.</p>
<p>However, neither <code>X1</code> nor <code>X2</code> is orthogonal with the column of ones. We define the vector <code>ones</code> below, which is a column of 1s, and compute the crossproduct between <code>ones</code> and the two regressors. Since the crossproducts are not zero, <code>X1</code> and <code>X2</code> are not orthogonal to the column of ones.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="interpreting-a-fitted-linear-model.html#cb8-1" aria-hidden="true" tabindex="-1"></a>ones <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">5</span>)   <span class="co"># column of 1s</span></span>
<span id="cb8-2"><a href="interpreting-a-fitted-linear-model.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(ones, X1) <span class="co"># not zero, so not orthogonal</span></span></code></pre></div>
<pre><code>##      [,1]
## [1,]   31</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="interpreting-a-fitted-linear-model.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(ones, X2) <span class="co"># not zero, so not orthogonal</span></span></code></pre></div>
<pre><code>##            [,1]
## [1,] -0.2857143</code></pre>
<p>We create <code>lmod3</code> by adding adding a column of ones to <code>lmod2</code> (i.e., if we include the intercept in the model). The the coefficients for both <code>X1</code> and <code>X2</code> change when going from <code>lmod2</code> to <code>lmod3</code> because these regressors are not orthogonal to the column of 1s. Comparing the coefficients <code>lmod2</code> above and <code>lmod3</code>, <span class="math inline">\(\hat{\beta}_1\)</span> changes from <span class="math inline">\(0.893\)</span> to <span class="math inline">\(0.397\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> changes from <span class="math inline">\(0.221\)</span> to <span class="math inline">\(0.279\)</span>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="interpreting-a-fitted-linear-model.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod2) <span class="co"># coefficients for lmod2</span></span></code></pre></div>
<pre><code>##        x1        x2 
## 0.8934010 0.2210526</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="interpreting-a-fitted-linear-model.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># y regressed on X1 and X2 with an intercept</span></span>
<span id="cb14-2"><a href="interpreting-a-fitted-linear-model.html#cb14-2" aria-hidden="true" tabindex="-1"></a>lmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2)</span>
<span id="cb14-3"><a href="interpreting-a-fitted-linear-model.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod3) <span class="co"># coefficients for lmod3</span></span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   3.1547101   0.3969746   0.2791657</code></pre>
<p>For orthogonality of our regressors to be most impactful, the model’s regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesn’t impact the estimated coefficients of the other regressors. In the code below, we define centered regressors <code>x3</code> and <code>x4</code> to be uncorrelated, i.e., <code>X3</code> and <code>X4</code> have sample mean zero and are orthogonal to each other.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="interpreting-a-fitted-linear-model.html#cb16-1" aria-hidden="true" tabindex="-1"></a>X3 <span class="ot">&lt;-</span>  <span class="fu">c</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>) <span class="co"># sample mean is zero</span></span>
<span id="cb16-2"><a href="interpreting-a-fitted-linear-model.html#cb16-2" aria-hidden="true" tabindex="-1"></a>X4 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)  <span class="co"># sample mean is zero</span></span>
<span id="cb16-3"><a href="interpreting-a-fitted-linear-model.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(X3, X4)              <span class="co"># 0, so X3 and X4 are uncorrelated and orthogonal</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>If we fit linear regression models with any combination of <code>ones</code>, <code>X3</code>, or <code>X4</code> as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="interpreting-a-fitted-linear-model.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>))           <span class="co"># only column of 1s</span></span>
<span id="cb18-2"><a href="interpreting-a-fitted-linear-model.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x3 <span class="sc">-</span> <span class="dv">1</span>))      <span class="co"># only x3</span></span>
<span id="cb18-3"><a href="interpreting-a-fitted-linear-model.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x4 <span class="sc">-</span> <span class="dv">1</span>))      <span class="co"># only x4</span></span>
<span id="cb18-4"><a href="interpreting-a-fitted-linear-model.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x3))          <span class="co"># 1s and x3</span></span>
<span id="cb18-5"><a href="interpreting-a-fitted-linear-model.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x4))          <span class="co"># 1s and x4</span></span>
<span id="cb18-6"><a href="interpreting-a-fitted-linear-model.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x3 <span class="sc">+</span> x4 <span class="sc">-</span> <span class="dv">1</span>)) <span class="co"># x3 and x4</span></span>
<span id="cb18-7"><a href="interpreting-a-fitted-linear-model.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x3 <span class="sc">+</span> x4))     <span class="co"># 1s, x3, and x4</span></span></code></pre></div>
<p>We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesn’t impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were <span class="math inline">\(\hat{\beta}_{0}=5.6\)</span>, <span class="math inline">\(\hat{\beta}_{3}=1.0\)</span>, <span class="math inline">\(\hat{\beta}_{4}=-0.5\)</span> when the relevant regressor was included in the model.</p>
<p>The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the <span class="math inline">\(\mathbf{X}\)</span> matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, <code>x1</code>, <code>x2</code>, <code>x3</code>, and <code>x4</code>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="interpreting-a-fitted-linear-model.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(<span class="fu">model.matrix</span>(<span class="sc">~</span> X1 <span class="sc">+</span> X2 <span class="sc">+</span> X3 <span class="sc">+</span> X4))</span></code></pre></div>
<pre><code>##             (Intercept)  X1         X2 X3        X4
## (Intercept)   5.0000000  31 -0.2857143  0 0.0000000
## X1           31.0000000 197  0.0000000  0 0.0000000
## X2           -0.2857143   0 15.5102041 -5 0.2857143
## X3            0.0000000   0 -5.0000000  2 0.0000000
## X4            0.0000000   0  0.2857143  0 2.0000000</code></pre>
<p>Consider the sequence of models below.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="interpreting-a-fitted-linear-model.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## (Intercept) 
##         5.6</code></pre>
<p>The model with only an intercept has an estimated coefficient of <span class="math inline">\(\hat{\beta}_{int}=5.6\)</span>. If we add the <code>X1</code> to the model with an intercept, then both coefficients change because they are not orthogonal to each other.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="interpreting-a-fitted-linear-model.html#cb23-1" aria-hidden="true" tabindex="-1"></a>lmod4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1) <span class="co"># model with 1s and x1</span></span>
<span id="cb23-2"><a href="interpreting-a-fitted-linear-model.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod4)</span></code></pre></div>
<pre><code>## (Intercept)          x1 
##         2.5         0.5</code></pre>
<p>If we add <code>X2</code> to <code>lmod4</code>, we might think that only <span class="math inline">\(\hat{\beta}_{0}\)</span> will change because <code>X1</code> and <code>X2</code> are orthogonal to each other. However, because <code>X2</code> is not orthogonal to all of the other regressors in the model (<code>X1</code> and the column of 1s), both <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> will change. The easiest way to realize this is to look at <code>lmod2</code> above with only <code>x1</code> and <code>x2</code>. When we add the column of 1s to <code>lmod2</code>, both <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_2\)</span> will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="interpreting-a-fitted-linear-model.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   3.1547101   0.3969746   0.2791657</code></pre>
<p>However, note that <code>X3</code> is orthogonal to the column of 1s and <code>X1</code>. Thus, if we add <code>X3</code> to <code>lmod4</code>, which includes both a column of 1s and <code>X1</code>, <code>X3</code> will not change the estimated coefficients for the intercept or <code>X1</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="interpreting-a-fitted-linear-model.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x3 
##         2.5         0.5         1.0</code></pre>
<p>Additionally, since <code>X4</code> is orthogonal to the column of 1s, <code>x1</code>, and <code>x3</code>, adding <code>X4</code> to the previous model will not change the estimated coefficients for any of the other variables already in the model.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="interpreting-a-fitted-linear-model.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3 <span class="sc">+</span> x4))</span></code></pre></div>
<pre><code>## (Intercept)          x1          x3          x4 
##         2.5         0.5         1.0        -0.5</code></pre>
<p>Lastly, if we can partition our <span class="math inline">\(\mathbf{X}\)</span> matrix such that <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is a block diagonal matrix, then none of the blocks of variables will affect the estimated coefficients of the other variables.</p>
<p>Define a new regressor <code>X5</code> below. <code>X5</code> is orthogonal to the column of 1s and <code>X1</code>, but not <code>X4</code>.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="interpreting-a-fitted-linear-model.html#cb31-1" aria-hidden="true" tabindex="-1"></a>X5 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>) <span class="co"># orthogonal to ones, x1, not x4</span></span>
<span id="cb31-2"><a href="interpreting-a-fitted-linear-model.html#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># note block of 0s</span></span>
<span id="cb31-3"><a href="interpreting-a-fitted-linear-model.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">crossprod</span>(<span class="fu">cbind</span>(ones, X1, X4, X5))</span></code></pre></div>
<pre><code>##      ones  X1 X4 X5
## ones    5  31  0  0
## X1     31 197  0  0
## X4      0   0  2 -1
## X5      0   0 -1  2</code></pre>
<p>Note the block of zeros in the lower left and uper right corners of the crossproduct matrix above. The block containing <code>ones</code> and <code>X1</code> is orthogonal to the block containing <code>X4</code> and <code>X5</code>. This means that if we fit the model with only the column of 1s and <code>X1</code>, the model only with <code>X4</code> and <code>X5</code>, and then fit the model with the column of 1s, <code>x1</code>, <code>x4</code>, and <code>x5</code>, then the coefficients <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are not impacted when <code>X4</code> and <code>X5</code> are added to the model. Similarly, <span class="math inline">\(\hat{\beta}_{4}\)</span> and <span class="math inline">\(\hat{\beta}_{5}\)</span> are not impacted when the column of 1s and <code>X1</code> are added to the model with <code>X4</code> and <code>X5</code>. See the output below.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="interpreting-a-fitted-linear-model.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x1)           <span class="co"># model with 1s and x1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1)
## 
## Coefficients:
## (Intercept)           x1  
##         2.5          0.5</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="interpreting-a-fitted-linear-model.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x4 <span class="sc">+</span> x5 <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># model with x4 and x5 only</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x4 + x5 - 1)
## 
## Coefficients:
## x4  x5  
## -3  -5</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="interpreting-a-fitted-linear-model.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x4 <span class="sc">+</span> x5) <span class="co"># model with 1s, x1, x4, x5</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x4 + x5)
## 
## Coefficients:
## (Intercept)           x1           x4           x5  
##         2.5          0.5         -3.0         -5.0</code></pre>
<!-- ## Example: Fuel Consumption Data -->
<!-- ```{r} -->
<!-- data(fuel2001, package = "alr4") -->
<!-- ``` -->
<!-- The variables (for the year 2001 unless otherwise noted) are: -->
<!-- - `Drivers`: Number of Licensed drivers in the state -->
<!-- - `FuelC`: Gasoline sold for road use (1000s of gal.) -->
<!-- - `Miles`: Miles of Federal-aid highway miles in the state -->
<!-- - `Pop`: 2001 population age 16 and over -->
<!-- - `Tax`: Gasoline state tax rate (cents/gallon) -->
<!-- ## Setting Up a Linear Model -->
<!-- What is the relationship between fuel consumption and various regressors for the 50 United States and the District of Columbia?   -->
<!-- ### Adjusting Units -->
<!-- - Some of the variables are adjusted for population. Others are are not. -->
<!-- - Some dollar values are given in thousands of dollars. Others are given in dollars. -->
<!-- - Some units of fuel are given in gallons. Others are given in 1000's of gallons. -->
<!-- - Our model should have regressor variables with compatible units with the response variable. -->
<!-- 1. **Create a new variable called `Fuel` that converts units of `FuelC` from 1000's of gallons to gallons per person**. -->
<!-- 2. **Create a new variable called `Income1k`that converts the units of `Income` from dollars per capita to 1000's of dollars per capita.** -->
<!-- 3. **Convert the units of `Drivers` from number of drivers to number of drivers per capita**. -->
<!-- - `Fuel`: Average amount of gasoline sold for road use per person (Gallons/person) -->
<!-- - `Income1K`: Average personal income (in thousands) for the year 2000 per person ($1K/person) -->
<!-- - `Dlic`: Number of licensed drivers per 1000 persons (licensed drivers/1K persons)  -->
<!-- ```{r} -->
<!-- # create new regressors/transformed responses to fuel2001 data frame -->
<!-- fuel2001$Fuel <- 1000*fuel2001$FuelC/fuel2001$Pop -->
<!-- fuel2001$Dlic <- 1000*fuel2001$Drivers/fuel2001$Pop -->
<!-- fuel2001$Income1K <- fuel2001$Income/1000 -->
<!-- ``` -->
<!-- ```{r} -->
<!-- summary(fuel2001) -->
<!-- ``` -->
<!-- ### Fitting a Model -->
<!-- We will set up a regression model to determine how `Fuel` (gallons per person) is related to `Tax` (cents per gallon), -->
<!-- `Dlic` (drivers per capita), `Income1k` (thousands of dollars of income per capita), and `Miles` (federal highway miles). -->
<!-- $$E( \mbox{Fuel} \ | \ \mbox{Tax, Dlic, Income1K, Miles})=\beta_0+\beta_1 (\mbox{Tax}) +\beta_2 (\mbox{Dlic}) + \beta_3 (\mbox{Income1K}) + \beta_4 \log{(\mbox{Miles})}$$ -->
<!-- ```{r} -->
<!-- # fit model -->
<!-- lmod <- lm(Fuel ~ Tax + Dlic + Income1K + log(Miles), data = fuel2001) -->
<!-- # summarize model -->
<!-- faraway::sumary(lmod) -->
<!-- ``` -->
<!-- We see the fitted model is: -->
<!-- $$\widehat{\mbox{E}}(\mbox{Fuel} \ | \ \mbox{Tax, Dlic, Income1k, Miles}) = 154.19 - 4.24 (\mbox{Tax}) + 0.47 (\mbox{Dlic}) - 6.14(\mbox{Income1K}) + 26.76 \log{(\mbox{26.76})}$$ -->
<!-- This equation represents the estimated conditional mean of Fuel given fixed values of the regressors Tax, Dlic, Income1K, and Miles. -->
<!-- ## Interpreting the Coefficients -->
<!-- Estimated coefficients are usually interpreted as a **rate of change.**  -->
<!-- - If we increase a regressor by 1 unit (and hold all others constant), what is the predicted change in the response variable? -->
<!-- 4. **Interpret the practical meaning of $\beta_1 = -4.24$. Pay attention to units when giving your interpretation.** -->
<!-- - The sign of a parameter estimate indicates the direction of the relationship between the regressor and the response (when all other regressors are constant). -->
<!-- - The sign of the effect of a regressor is often more important than its magnitude. -->
<!-- - If regressors are highly correlated with other regressors, both the magnitude and sign of an estimated coefficient may change depending on the values of the other regressors are in the model. -->
<!-- ## Example: Berkeley Guidance Study -->
<!-- Data from the Berkeley guidance study of children born in 1928-29 in Berkeley, CA. BGSgirls contains data from just the girls in the study. -->
<!-- ```{r} -->
<!-- data(BGSgirls, package = "alr4") -->
<!-- head(BGSgirls) -->
<!-- ``` -->
<!-- ## Dictionary of Data -->
<!-- - `BMI18`: the body mass index at age 18 -->
<!-- -  `WT2`, `WT9`, and `WT18`: the weights at ages 2, 9, and 18 (in kg) for the $n=70$ girls in the study. -->
<!-- ## Analysing Relations Between Regressors -->
<!-- ```{r} -->
<!-- # basic scatterplot matrix -->
<!-- pairs(~ BMI18 + WT2 + WT9 + WT18, data = BGSgirls) -->
<!-- ``` -->
<!-- 5. **Based on the scatter plot matrix above, does there seem to be any relations among the regressors? Explain why these relations make practical sense.** -->
<!-- 6. **How can we adjust our model to account for the relations between the regressors?** -->
<!-- ## Adjusting the Regressors -->
<!-- 7. **Create a new regressor called `DW9` that is the weight gain from age 2 to 9.** -->
<!-- 8. **Create a new regressor called `DW18` that is the weight gain from age 9 to 18.** -->
<!-- ```{r} -->
<!-- BGSgirls$DW9 <- BGSgirls$WT9-BGSgirls$WT2 -->
<!-- BGSgirls$DW18 <- BGSgirls$WT18-BGSgirls$WT9 -->
<!-- BGSgirls$DW218 <- BGSgirls$WT18-BGSgirls$WT2 -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # basic scatterplot matrix -->
<!-- pairs(~ BMI18 + DW9 + DW18, data = BGSgirls) -->
<!-- ``` -->
<!-- 9. **Based on the scatter plot matrix above, how can you tell that the new regressors seem to be more independent from each other?** -->
<!-- ## Comparing Different Models -->
<!-- ### BMI relation to WT2, WT9 and WT18 -->
<!-- ```{r} -->
<!-- m1 <- lm(BMI18 ~ WT2 + WT9 + WT18, BGSgirls) -->
<!-- faraway::sumary(m1) -->
<!-- ``` -->
<!-- ### BMI relation to WT2, DW9 and DW18 -->
<!-- ```{r} -->
<!-- m2 <- lm(BMI18 ~ WT2 + DW9 + DW18, BGSgirls) -->
<!-- faraway::sumary(m2) -->
<!-- ``` -->
<!-- ### BMI relation to WT2, WT9, WT18, DW9 and DW18 -->
<!-- ```{r} -->
<!-- m3 <- lm(BMI18 ~ WT2 + WT9 + WT18 + DW9 + DW18, BGSgirls) -->
<!-- faraway::sumary(m3) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- coef(m1) -->
<!-- coef(m2) -->
<!-- coef(m3) -->
<!-- ``` -->
<!-- Regressor | Model 1 | Model 2 | Model 3     -->
<!-- ----------|---------|---------|----------       -->
<!-- Intercept  | 8.298         | 8.298        | 8.298    -->
<!--   CI         | (5.00,11.62)  | (5.00,11.62) | (5.00,11.62) -->
<!-- WT2  | -0.383        | -0.065       | -0.383 -->
<!--  CI  | (-0.69,-0.08) | (-0.32,0.19) | (-0.69,-0.08) -->
<!-- WT9  | 0.032         | --           | 0.032  -->
<!--  CI  | (-0.06,0.13)  | --           | (-0.06,0.13) -->
<!-- WT18 | 0.287         | --           | -- -->
<!-- CI   | (0.23,0.34)   | --           | -- -->
<!-- DW9  |  --           | 0.318        | NA -->
<!--  CI  |  --           | (0.24,0.40)  | NA -->
<!-- DW18 |  --           | 0.287        | NA -->
<!--   CI |  --           | (0.23,0.34)  | NA -->
<!-- 10. **Comment on how the WT2 coefficient is the same/different in the different models.** -->
<!-- When regressors are correlated, interpretation of the effect of a regressor depends not only on the other regressors in the model, but also upon the linear transformation of the variables used. -->
<!-- 11. **Why are their NAs in Model 3?** -->
<!-- ## Effect Plots -->
<!-- An **effect plot** displays effect of a regressor on the mean response while holding the other regressors at their mean values. -->
<!-- $$ \hat{y} = \beta_0 + \beta_1 (\bar{X}_1) + \beta_2 (\bar{X}_2) + \ldots + \beta_{i-1} (\bar{X}_{i-1}) + \beta_i (X_i) + \beta_{i+1} (\bar{X}_{i+1}) + \ldots + + \beta_{p-1} (\bar{X}_{p-1})$$ -->
<!-- ```{r} -->
<!-- summary(lmod)$coefficients -->
<!-- ``` -->
<!-- 12. **Complete the code below to extract each of the coefficients in the Fuel Consumption model `lmod` from the coefficient array above.** -->
<!-- ```{r} -->
<!-- b0 <- summary(lmod)$coefficients[1] #beta_0 -->
<!-- b1 <- summary(lmod)$coefficients[2] #beta_1 -->
<!-- b2 <- summary(lmod)$coefficients[3] #beta_2 -->
<!-- b3 <- summary(lmod)$coefficients[4] #beta_3 -->
<!-- b4 <- summary(lmod)$coefficients[5] #beta_4 -->
<!-- ``` -->
<!-- 12. Complete the R code below to compute the sample means for each of the regressors. -->
<!-- ```{r} -->
<!-- xbar.Tax <- mean(fuel2001$Tax) -->
<!-- xbar.Dlic <- mean(fuel2001$Dlic) -->
<!-- xbar.Income1K <- mean(fuel2001$Income1K) -->
<!-- xbar.Miles <- mean(fuel2001$Miles) -->
<!-- ``` -->
<!-- 13. **What is the effect of `Tax` on expected Fuel consumption when the other regressors are fixed at the sample mean values? Write a formula to express this relation.** -->
<!-- Thus we have the model -->
<!-- $$\mbox{E}(\mbox{Fuel} \ | \ \mbox{Tax, Dlic=??, Income1K=??, log(Miles) = ??}) = ?? - ??(\mbox{Tax})$$ -->
<!-- ```{r} -->
<!-- library(effects) # for Effect function -->
<!-- # effect plot for Tax regressor -->
<!-- plot(Effect("Tax", lmod)) -->
<!-- ``` -->
<!-- 14. **If instead of fixing the values of the regressors at their mean, we choose other values such as the minimum value of each of regressors. What effect (if any) would this have on the graph above?** -->
<!-- ## Regressors on Logarithmic Scale -->
<!-- Logarithms are commonly used both for the response and for regressors. -->
<!-- ```{r} -->
<!-- summary(fuel2001) -->
<!-- ``` -->
<!-- 15. **Based on the summary output above, why do you think we used a log scale on `Miles`?** -->
<!-- In the code block below, we create new variables that are the natural log and log base 10 of Miles and recreate the linear model using each of these new variables. -->
<!-- ```{r} -->
<!-- fuel2001$LnMiles <- log(fuel2001$Miles) -->
<!-- fuel2001$LogMiles <- log10(fuel2001$Miles) -->
<!-- lmod.ln <- lm(Fuel ~ Tax + Dlic + Income1K + LnMiles, data = fuel2001) -->
<!-- lmod.log <- lm(Fuel ~ Tax + Dlic + Income1K + LogMiles, data = fuel2001) -->
<!-- faraway::sumary(lmod.ln) #Check that model is the same -->
<!-- faraway::sumary(lmod.log) #Check that model is the same -->
<!-- ``` -->
<!-- ## Below we create an effects plot on a natural log scale. -->
<!-- ```{r} -->
<!-- plot(Effect("LnMiles", lmod.ln),  -->
<!--      main = "ln(Miles) effect plot") -->
<!-- ``` -->
<!-- ## Below we create an effects plot on a log10 scale. -->
<!-- ```{r} -->
<!-- plot(Effect("LogMiles", lmod.log),  -->
<!--      main = "log(Miles) effect plot") -->
<!-- ``` -->
<!-- ## Below we create an effects plot on a regular scale. -->
<!-- ```{r} -->
<!-- plot(Effect("Miles", lmod,  -->
<!--             xlevels = list(Miles = seq(1, 3e5, len = 301)))) -->
<!-- ``` -->
<!-- The effect of increasing Miles is greater in states with fewer miles of roadway, with relatively little change in states with the most roadway.  -->
<!-- This is the usual effect of logarithms: the fitted effect changes most rapidly when the regressor is small and less rapidly when the predictor is large. -->
<!-- ## Interpreting Coefficients with Log Scale on Regressor -->
<!-- ### Natural Log Scale  -->
<!-- Regressor $X_j$ increasing by 1% while the other regressors remain constant is associated with a $\beta_j/100$ increase in the response variable, on average. -->
<!-- 16. **Interpret the meaning of the coefficient associated to the natural log of Miles which you can find below.** -->
<!-- ```{r} -->
<!-- summary(lmod.ln)$coefficients[5, 1] #ln coeff -->
<!-- ``` -->
<!-- ### Common Log (base 10) Scale  -->
<!-- Regressor $X_j$ increasing by a factor of 10 (an increase of 900%) while the other regressors remain constant is associated with a $\beta_j$ increase in the response variable, on average. -->
<!-- 17. **Interpret the meaning of the coefficient associated to the log base 10 of Miles which you can find below.** -->
<!-- ```{r} -->
<!-- summary(lmod.log)$coefficients[5, 1] #log coeff -->
<!-- ``` -->
<!-- ## Log-Level Interpretation -->
<!-- It is common for responses to be transformed to a logarithmic scale for theoretical or practical considerations.   -->
<!-- $$\mbox{E}( \log{Y} \ | \ X)= \beta_0 + \beta_1 X.$$ -->
<!-- This is sometimes called a **log-level model**.  -->
<!-- - A unit increase in $X$ is associated with a change in the mean $Y$ by the multiplicative effect $\exp^{\beta_1}$. -->
<!-- - **Thus $beta_1$ is the continuous exponential growth/decay rate.** -->
<!-- It is often acceptable to approximate the expected value of a log by the log of the expected value: -->
<!-- $$\log{( \mbox{E}(Y \ | \ X=x) )} \approx\mbox{E}(\log{Y} \ | \ X=x)$$ -->
<!-- Thus, we have -->
<!-- $$\mbox{E}(Y \ | \ X=x) \approx e^{\rm{E}(\log{Y} \ | \ X=x)} = e^{\beta_0 + \beta_1 X}=e^{\beta_0}e^{\beta_1X}.$$ -->
<!-- ## Log-log Interpretation -->
<!-- Consider the log-log simple linear regression model -->
<!-- $$\mbox{E}( \log{Y} \ | \ X) = \beta_0 + \beta_1 \log{X}.$$ -->
<!-- When we scale $X$ by a factor of $c$, the response is predicted to grow by a factor of $c^{\beta_1}$, on average. -->
<!-- ## Summary of Interpretations (Simple Linear Regression) -->
<!-- **Level-level**: $\mathbf{\mbox{E}(Y \ | \ X=x)} = \boldsymbol\beta_0 + \boldsymbol\beta_1 X$: The predicted change in the response is $\beta_1$ when we increase $X$ by 1 unit, on average. -->
<!-- **Level-log**: $\mathbf{\mbox{E}(Y \ | \ X=x)} = \boldsymbol\beta_0 + \boldsymbol\beta_1 \log{X}$: When we increase $X$ by 1%, the response is predicted to increase by  $\beta_j/100$, on average. -->
<!-- **Log-level**: $\mathbf{\mbox{E}(\log{Y} \ | \ X=x) = \boldsymbol\beta_0 + \boldsymbol\beta_1 X}$: A unit increase in $X$ is predicted to change the response by a factor of $e^{\beta_1}$, on average. The continuous growth rate is $\beta_1$. -->
<!-- **Log-log**: $\mathbf{\mbox{E}(\log{Y} \ | \ X=x) = \boldsymbol\beta_0 + \boldsymbol\beta_1 \log{X}}$: When we scale $X$ by a factor of $c$, the response is predicted to grow by a factor of $c^{\beta_1}$, on average. -->
<!-- ## More Practice -->
<!-- 1. For a log-level model, interpret the relationship between $X$ and the mean of $Y$ when $X$ increases by 1 unit and $\beta_j=0.3$ and the other predictors do not change. -->
<!-- 2. For a log-log model, what is the expected change in $Y$ if we multiply $X$ by a factor of $c$. -->
<!-- <!-- 3. For a log-log model, interpret the relationship between $X$ and the expected value of $Y$ when $X$ increases by $10\%$ and $\beta_1=0.3$. -->
<p>–&gt;</p>
<!-- ## Exercises -->
<!-- 1. If given a set of data with several variables, how would you decide what the response variable and the predictor variables would be? -->
<!-- 1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are considered random? Which are considered fixed? -->
<!-- 1. Which objects in the linear model formula in Equation \@ref(eq:lmdef) are observable? Which are not observable? -->
<!-- 1. What are the typical goals of a regression analysis? -->
<!-- 1. List the typical assumptions made for the errors in a linear model. -->
<!-- 1. Without using a formula, what is the basic difference between a linear model and a non-linear model? -->
<!-- 1. Assuming that $\boldsymbol{\epsilon} ~ N(\mathbf{0}_{n\times 1}, \sigma^2 I_n)$ and $\mathbf{y}  = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, show that: -->
<!--     a. $E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}$. -->
<!--     a. $\mathrm{var}(\mathbf{y}) = \sigma^2 I_n$. -->
<!-- 1. In the context of simple linear regression under the standard assumptions, show that: -->
<!--     a. $\beta_0=E(Y|X=0)$. -->
<!--     a. $\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)$. -->
<!-- 1. In the context of multiple linear regression under the standard assumptions, show that: -->
<!--     a. $\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)$. -->
<!--     b. For $j=1,2,\ldots,p-1$, $\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})$ where $\mathbf{x}^*$ is a fixed vector of the appropriate size. -->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-lmwr2" class="csl-entry">
Faraway, Julian J. 2014. <em>Linear Models with r</em>. Second. Chapman; Hall/CRC.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-model-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-model-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["A-Progessive-Introduction-to-Linear-Models.pdf", "A-Progessive-Introduction-to-Linear-Models.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
