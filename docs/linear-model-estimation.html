<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear model estimation | Joshua French</title>
  <meta name="description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear model estimation | Joshua French" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear model estimation | Joshua French" />
  
  <meta name="twitter:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

<meta name="author" content="Chapter 3 Linear model estimation | Joshua French" />


<meta name="date" content="2022-07-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-cleaning-and-exploration.html"/>
<link rel="next" href="interpreting-a-fitted-linear-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with Linear Regression</a></li>

<li class="divider"></li>
<li><a href="index.html#preliminaries">Preliminaries<span></span></a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#setting-up-r-and-r-studio-desktop"><i class="fa fa-check"></i><b>1.1</b> Setting up R and R Studio Desktop<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.2</b> Running code, scripts, and comments<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.3</b> Assignment<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#functions"><i class="fa fa-check"></i><b>1.4</b> Functions<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.8</b> Vectors<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.8.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.8.2</b> Categorical vectors<span></span></a></li>
<li class="chapter" data-level="1.8.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.8.3</b> Extracting parts of a vector<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.9</b> Helpful functions<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.9.1</b> General functions<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.9.2</b> Functions related to statistical distributions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.10</b> Data Frames<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#direct-creation"><i class="fa fa-check"></i><b>1.10.1</b> Direct creation<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.10.2</b> Importing Data<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.10.3</b> Extracting parts of a data frame<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#using-the-pipe-operator"><i class="fa fa-check"></i><b>1.11</b> Using the pipe operator<span></span></a></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#dealing-with-common-problems"><i class="fa fa-check"></i><b>1.12</b> Dealing with common problems<span></span></a></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.13</b> Ecosystem debate<span></span></a></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#additional-information"><i class="fa fa-check"></i><b>1.14</b> Additional information<span></span></a>
<ul>
<li class="chapter" data-level="1.14.1" data-path="r-foundations.html"><a href="r-foundations.html#comparing-assignment-operators"><i class="fa fa-check"></i><b>1.14.1</b> Comparing assignment operators<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html"><i class="fa fa-check"></i><b>2</b> Data cleaning and exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#raw-palmer-penguins-data"><i class="fa fa-check"></i><b>2.1</b> Raw Palmer penguins data<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#initial-data-cleaning"><i class="fa fa-check"></i><b>2.2</b> Initial data cleaning<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numerical-summarization-of-data"><i class="fa fa-check"></i><b>2.3</b> Numerical summarization of data<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#numeric-data"><i class="fa fa-check"></i><b>2.3.1</b> Numeric data<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#categorical-data"><i class="fa fa-check"></i><b>2.3.2</b> Categorical data<span></span></a></li>
<li class="chapter" data-level="2.3.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-summary-function"><i class="fa fa-check"></i><b>2.3.3</b> The <code>summary</code> function<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.4</b> Visual summaries of data<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#the-ggplot-recipe"><i class="fa fa-check"></i><b>2.4.1</b> The ggplot recipe<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#univariate-plots"><i class="fa fa-check"></i><b>2.4.2</b> Univariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#bivariate-plots"><i class="fa fa-check"></i><b>2.4.3</b> Bivariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#multivariate-plots"><i class="fa fa-check"></i><b>2.4.4</b> Multivariate plots<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#facetted-plots-and-alternatives"><i class="fa fa-check"></i><b>2.4.5</b> Facetted plots (and alternatives)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#a-plan-for-data-cleaning-and-exploration"><i class="fa fa-check"></i><b>2.5</b> A plan for data cleaning and exploration<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="data-cleaning-and-exploration.html"><a href="data-cleaning-and-exploration.html#final-notes-on-missing-or-erroneous-data"><i class="fa fa-check"></i><b>2.6</b> Final notes on missing or erroneous data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>3</b> Linear model estimation<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>3.1</b> A simple motivating example<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>3.2</b> Defining a linear model<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>3.2.1</b> Necessary components and notation<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>3.2.2</b> Standard definition of linear model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>3.3</b> Estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>3.3.1</b> Fitted values, residuals, and RSS<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>3.3.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>3.4</b> Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.5</b> Estimation of the multiple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>3.5.1</b> Using matrix notation to represent a linear model<span></span></a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>3.5.2</b> Residuals, fitted values, and RSS for multiple linear regression<span></span></a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients"><i class="fa fa-check"></i><b>3.5.3</b> OLS estimator of the regression coefficients<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr"><i class="fa fa-check"></i><b>3.6</b> Penguins multiple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#categorical-predictors"><i class="fa fa-check"></i><b>3.7</b> Categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#indicator-variables"><i class="fa fa-check"></i><b>3.7.1</b> Indicator variables<span></span></a></li>
<li class="chapter" data-level="3.7.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#parallel-and-separate-lines-models"><i class="fa fa-check"></i><b>3.7.2</b> Parallel and separate lines models<span></span></a></li>
<li class="chapter" data-level="3.7.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#extensions"><i class="fa fa-check"></i><b>3.7.3</b> Extensions<span></span></a></li>
<li class="chapter" data-level="3.7.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#avoiding-an-easy-mistake"><i class="fa fa-check"></i><b>3.7.4</b> Avoiding an easy mistake<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr2"><i class="fa fa-check"></i><b>3.8</b> Penguins example with categorical predictor<span></span></a></li>
<li class="chapter" data-level="3.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#evaluating-model-fit"><i class="fa fa-check"></i><b>3.9</b> Evaluating model fit<span></span></a></li>
<li class="chapter" data-level="3.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary"><i class="fa fa-check"></i><b>3.10</b> Summary<span></span></a>
<ul>
<li class="chapter" data-level="3.10.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-terms"><i class="fa fa-check"></i><b>3.10.1</b> Summary of terms<span></span></a></li>
<li class="chapter" data-level="3.10.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-functions"><i class="fa fa-check"></i><b>3.10.2</b> Summary of functions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#going-deeper"><i class="fa fa-check"></i><b>3.11</b> Going Deeper<span></span></a>
<ul>
<li class="chapter" data-level="3.11.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#slr-derivation"><i class="fa fa-check"></i><b>3.11.1</b> Derivation of the OLS estimator for the simple linear regression model coefficients<span></span></a></li>
<li class="chapter" data-level="3.11.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>3.11.2</b> Manual calculation Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="3.11.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#mlr-derivation"><i class="fa fa-check"></i><b>3.11.3</b> Derivation of the OLS estimator for the multiple linear regression model coefficients<span></span></a></li>
<li class="chapter" data-level="3.11.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example"><i class="fa fa-check"></i><b>3.11.4</b> Manual calculation of Penguins multiple linear regression example<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html"><i class="fa fa-check"></i><b>4</b> Interpreting a fitted linear model<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#standard-mathemtical-interpretation"><i class="fa fa-check"></i><b>4.1</b> Standard mathemtical interpretation<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-in-simple-linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#coefficient-interpretation-for-first-order-multiple-linear-regression-models"><i class="fa fa-check"></i><b>4.1.2</b> Coefficient interpretation for first-order multiple linear regression models<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#exercises"><i class="fa fa-check"></i><b>4.2</b> Exercises<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#orthogonality"><i class="fa fa-check"></i><b>4.3</b> Orthogonality<span></span></a></li>
<li class="chapter" data-level="4.4" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#example-fuel-consumption-data"><i class="fa fa-check"></i><b>4.4</b> Example: Fuel Consumption Data<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#setting-up-a-linear-model"><i class="fa fa-check"></i><b>4.5</b> Setting Up a Linear Model<span></span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#adjusting-units"><i class="fa fa-check"></i><b>4.5.1</b> Adjusting Units<span></span></a></li>
<li class="chapter" data-level="4.5.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#fitting-a-model"><i class="fa fa-check"></i><b>4.5.2</b> Fitting a Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>4.6</b> Interpreting the Coefficients<span></span></a></li>
<li class="chapter" data-level="4.7" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#example-berkeley-guidance-study"><i class="fa fa-check"></i><b>4.7</b> Example: Berkeley Guidance Study<span></span></a></li>
<li class="chapter" data-level="4.8" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#dictionary-of-data"><i class="fa fa-check"></i><b>4.8</b> Dictionary of Data<span></span></a></li>
<li class="chapter" data-level="4.9" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#analysing-relations-between-regressors"><i class="fa fa-check"></i><b>4.9</b> Analysing Relations Between Regressors<span></span></a></li>
<li class="chapter" data-level="4.10" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#adjusting-the-regressors"><i class="fa fa-check"></i><b>4.10</b> Adjusting the Regressors<span></span></a></li>
<li class="chapter" data-level="4.11" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#comparing-different-models"><i class="fa fa-check"></i><b>4.11</b> Comparing Different Models<span></span></a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-wt9-and-wt18"><i class="fa fa-check"></i><b>4.11.1</b> BMI relation to WT2, WT9 and WT18<span></span></a></li>
<li class="chapter" data-level="4.11.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-dw9-and-dw18"><i class="fa fa-check"></i><b>4.11.2</b> BMI relation to WT2, DW9 and DW18<span></span></a></li>
<li class="chapter" data-level="4.11.3" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-wt9-wt18-dw9-and-dw18"><i class="fa fa-check"></i><b>4.11.3</b> BMI relation to WT2, WT9, WT18, DW9 and DW18<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#effect-plots"><i class="fa fa-check"></i><b>4.12</b> Effect Plots<span></span></a></li>
<li class="chapter" data-level="4.13" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#regressors-on-logarithmic-scale"><i class="fa fa-check"></i><b>4.13</b> Regressors on Logarithmic Scale<span></span></a></li>
<li class="chapter" data-level="4.14" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-natural-log-scale."><i class="fa fa-check"></i><b>4.14</b> Below we create an effects plot on a natural log scale.<span></span></a></li>
<li class="chapter" data-level="4.15" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-log10-scale."><i class="fa fa-check"></i><b>4.15</b> Below we create an effects plot on a log10 scale.<span></span></a></li>
<li class="chapter" data-level="4.16" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-regular-scale."><i class="fa fa-check"></i><b>4.16</b> Below we create an effects plot on a regular scale.<span></span></a></li>
<li class="chapter" data-level="4.17" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#interpreting-coefficients-with-log-scale-on-regressor"><i class="fa fa-check"></i><b>4.17</b> Interpreting Coefficients with Log Scale on Regressor<span></span></a>
<ul>
<li class="chapter" data-level="4.17.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#natural-log-scale"><i class="fa fa-check"></i><b>4.17.1</b> Natural Log Scale<span></span></a></li>
<li class="chapter" data-level="4.17.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#common-log-base-10-scale"><i class="fa fa-check"></i><b>4.17.2</b> Common Log (base 10) Scale<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.18" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#log-level-interpretation"><i class="fa fa-check"></i><b>4.18</b> Log-Level Interpretation<span></span></a></li>
<li class="chapter" data-level="4.19" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#log-log-interpretation"><i class="fa fa-check"></i><b>4.19</b> Log-log Interpretation<span></span></a></li>
<li class="chapter" data-level="4.20" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#summary-of-interpretations-simple-linear-regression"><i class="fa fa-check"></i><b>4.20</b> Summary of Interpretations (Simple Linear Regression)<span></span></a></li>
<li class="chapter" data-level="4.21" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#more-practice"><i class="fa fa-check"></i><b>4.21</b> More Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-model-inference.html"><a href="linear-model-inference.html"><i class="fa fa-check"></i><b>5</b> Linear model inference<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#galapogos-example-testing-all-regressors"><i class="fa fa-check"></i><b>5.1</b> Galapogos Example: Testing All Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-1"><i class="fa fa-check"></i><b>5.1.1</b> Question 1<span></span></a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-2"><i class="fa fa-check"></i><b>5.1.2</b> Question 2<span></span></a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#setting-up-the-hypotheses"><i class="fa fa-check"></i><b>5.1.3</b> Setting Up The Hypotheses<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-3"><i class="fa fa-check"></i><b>5.2</b> Question 3<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#measuring-significance"><i class="fa fa-check"></i><b>5.2.1</b> Measuring Significance<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#general-f-test-for-comparing-two-nested-regression-models"><i class="fa fa-check"></i><b>5.2.2</b> General <span class="math inline">\(F\)</span> Test for Comparing Two Nested Regression Models:<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-4"><i class="fa fa-check"></i><b>5.2.3</b> Question 4<span></span></a></li>
<li class="chapter" data-level="5.2.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-5"><i class="fa fa-check"></i><b>5.2.4</b> Question 5<span></span></a></li>
<li class="chapter" data-level="5.2.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-6"><i class="fa fa-check"></i><b>5.2.5</b> Question 6<span></span></a></li>
<li class="chapter" data-level="5.2.6" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-7"><i class="fa fa-check"></i><b>5.2.6</b> Question 7<span></span></a></li>
<li class="chapter" data-level="5.2.7" data-path="linear-model-inference.html"><a href="linear-model-inference.html#interpreting-the-results"><i class="fa fa-check"></i><b>5.2.7</b> Interpreting the Results<span></span></a></li>
<li class="chapter" data-level="5.2.8" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-8"><i class="fa fa-check"></i><b>5.2.8</b> Question 8<span></span></a></li>
<li class="chapter" data-level="5.2.9" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-9"><i class="fa fa-check"></i><b>5.2.9</b> Question 9<span></span></a></li>
<li class="chapter" data-level="5.2.10" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-10"><i class="fa fa-check"></i><b>5.2.10</b> Question 10<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#testing-a-pair-of-regressors"><i class="fa fa-check"></i><b>5.3</b> Testing a Pair of Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-11"><i class="fa fa-check"></i><b>5.3.1</b> Question 11<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#permutation-tests"><i class="fa fa-check"></i><b>5.4</b> Permutation Tests<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#assumptions-for-testing-regressors"><i class="fa fa-check"></i><b>5.4.1</b> Assumptions for Testing Regressors<span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#motivating-idea-behind-permutation-tests"><i class="fa fa-check"></i><b>5.4.2</b> Motivating idea behind permutation tests<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#permutation-test-on-two-regressors"><i class="fa fa-check"></i><b>5.5</b> Permutation Test on Two Regressors<span></span></a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-1-set-up-the-hypotheses-for-this-test."><i class="fa fa-check"></i><b>5.5.1</b> Question 1: Set up the hypotheses for this test.<span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-2-compute-the-f-statistic-from-the-general-f-test."><i class="fa fa-check"></i><b>5.5.2</b> Question 2: Compute the <span class="math inline">\(F\)</span>-statistic from the general <span class="math inline">\(F\)</span> test.<span></span></a></li>
<li class="chapter" data-level="5.5.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-3-compute-the-p-value-of-the-corresponding-f-statistic-from-the-general-f-test."><i class="fa fa-check"></i><b>5.5.3</b> Question 3: Compute the <span class="math inline">\(p\)</span>-value of the corresponding <span class="math inline">\(F\)</span>-statistic from the general <span class="math inline">\(F\)</span> test.<span></span></a></li>
<li class="chapter" data-level="5.5.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#sample-permutation-test"><i class="fa fa-check"></i><b>5.5.4</b> Sample Permutation Test<span></span></a></li>
<li class="chapter" data-level="5.5.5" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-4-compare-the-two-p-values-from-the-two-methods.-do-you-think-the-difference-is-significant-which-p-value-do-you-think-is-more-accurate-why"><i class="fa fa-check"></i><b>5.5.5</b> Question 4: Compare the two p-values from the two methods. Do you think the difference is significant? Which p-value do you think is more accurate? Why?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-model-inference.html"><a href="linear-model-inference.html#testing-whether-one-regressor-can-be-dropped"><i class="fa fa-check"></i><b>5.6</b> Testing whether one regressor can be dropped<span></span></a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#setting-up-the-hypotheses-1"><i class="fa fa-check"></i><b>5.6.1</b> Setting up the hypotheses<span></span></a></li>
<li class="chapter" data-level="5.6.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#extracting-pertinent-statistics-from-theoretical-test"><i class="fa fa-check"></i><b>5.6.2</b> Extracting pertinent statistics from theoretical test<span></span></a></li>
<li class="chapter" data-level="5.6.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-5-perform-a-pertmutation-test-to-test-the-hypotheses-above."><i class="fa fa-check"></i><b>5.6.3</b> Question 5: Perform a pertmutation test to test the hypotheses above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="linear-model-inference.html"><a href="linear-model-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.7</b> Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#constructing-a-95-ci-for-area-assuming-all-4-other-predictors-in-the-model"><i class="fa fa-check"></i><b>5.7.1</b> Constructing a 95% CI for Area (assuming all 4 other predictors in the model)<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-6-based-on-the-output-above-construct-a-95-confidence-interval-to-estimate-the-value-of-beta_rm-area."><i class="fa fa-check"></i><b>5.7.2</b> Question 6: Based on the output above, construct a 95% confidence interval to estimate the value of <span class="math inline">\(\beta_{\rm Area}\)</span>.<span></span></a></li>
<li class="chapter" data-level="5.7.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-7-cconstruct-a-95-confidence-interval-to-estimate-the-value-of-beta_rm-elevation."><i class="fa fa-check"></i><b>5.7.3</b> Question 7: Cconstruct a 95% confidence interval to estimate the value of <span class="math inline">\(\beta_{\rm Elevation}\)</span>.<span></span></a></li>
<li class="chapter" data-level="5.7.4" data-path="linear-model-inference.html"><a href="linear-model-inference.html#finding-confidence-intervals-for-all-regressors"><i class="fa fa-check"></i><b>5.7.4</b> Finding Confidence Intervals for All Regressors<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="linear-model-inference.html"><a href="linear-model-inference.html#confidence-regions"><i class="fa fa-check"></i><b>5.8</b> Confidence Regions<span></span></a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-8-based-on-the-confidence-region-above-is-it-plausible-that-beta_rm-area-beta_rm-adjacent0-why-or-why-not"><i class="fa fa-check"></i><b>5.8.1</b> Question 8: Based on the confidence region above, is it plausible that <span class="math inline">\(\beta_{\rm Area}= \beta_{\rm Adjacent}=0\)</span>? Why or why not?<span></span></a></li>
<li class="chapter" data-level="5.8.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#question-9-based-on-the-confidence-region-above-is-it-plausible-that-beta_rm-area--0.6-and-beta_rm-adjacent-0.045-why-or-why-not"><i class="fa fa-check"></i><b>5.8.2</b> Question 9: Based on the confidence region above, is it plausible that <span class="math inline">\(\beta_{\rm Area}= -0.6\)</span> and <span class="math inline">\(\beta_{\rm Adjacent}=-0.045\)</span>? Why or why not?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="linear-model-inference.html"><a href="linear-model-inference.html#bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.9</b> Bootstrap Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#bootstrap-process"><i class="fa fa-check"></i><b>5.9.1</b> Bootstrap Process<span></span></a></li>
<li class="chapter" data-level="5.9.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#comparing-with-parametric-confidence-intervals"><i class="fa fa-check"></i><b>5.9.2</b> Comparing with Parametric Confidence Intervals<span></span></a></li>
<li class="chapter" data-level="5.9.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#visualizing-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>5.9.3</b> Visualizing Bootstrap Confidence Intervals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="linear-model-inference.html"><a href="linear-model-inference.html#sampling-experimentation-generalization-and-causation"><i class="fa fa-check"></i><b>5.10</b> Sampling Experimentation, Generalization, and Causation<span></span></a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="linear-model-inference.html"><a href="linear-model-inference.html#designed-experiments"><i class="fa fa-check"></i><b>5.10.1</b> Designed Experiments<span></span></a></li>
<li class="chapter" data-level="5.10.2" data-path="linear-model-inference.html"><a href="linear-model-inference.html#observational-studies"><i class="fa fa-check"></i><b>5.10.2</b> Observational Studies<span></span></a></li>
<li class="chapter" data-level="5.10.3" data-path="linear-model-inference.html"><a href="linear-model-inference.html#experimental-and-observational-predictors"><i class="fa fa-check"></i><b>5.10.3</b> Experimental and Observational Predictors<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Prediction<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="prediction.html"><a href="prediction.html#example-fat-data"><i class="fa fa-check"></i><b>6.1</b> Example: fat data<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="prediction.html"><a href="prediction.html#predicting-percent-body-fat"><i class="fa fa-check"></i><b>6.2</b> Predicting Percent Body Fat<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="prediction.html"><a href="prediction.html#question-1-what-would-the-body-fat-be-for-a-person-that-has-the-following-measurements-note-the-first-value-is-a-place-holder-for-the-intercept-in-the-model"><i class="fa fa-check"></i><b>6.3</b> Question 1: What would the body fat be for a person that has the following measurements (note the first value is a place holder for the intercept in the model)?<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="prediction.html"><a href="prediction.html#the-general-setup"><i class="fa fa-check"></i><b>6.4</b> The General Setup<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="prediction.html"><a href="prediction.html#types-of-predictions"><i class="fa fa-check"></i><b>6.5</b> Types of Predictions<span></span></a></li>
<li class="chapter" data-level="6.6" data-path="prediction.html"><a href="prediction.html#question-2-using-properties-of-variance-and-underlying-assumptions-of-our-model-find-the-variance-of-widehaty_0-mathbfx_0t-widehatboldsymbolbeta."><i class="fa fa-check"></i><b>6.6</b> Question 2: Using properties of variance and underlying assumptions of our model, find the variance of <span class="math inline">\(\widehat{y}_0 = \mathbf{x}_0^T \widehat{\boldsymbol\beta}\)</span>.<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="prediction.html"><a href="prediction.html#quantifying-the-uncertainty-of-a-prediction"><i class="fa fa-check"></i><b>6.7</b> Quantifying the Uncertainty of a Prediction<span></span></a></li>
<li class="chapter" data-level="6.8" data-path="prediction.html"><a href="prediction.html#example-prediction-interval-for-median-man"><i class="fa fa-check"></i><b>6.8</b> Example: Prediction Interval for Median Man<span></span></a></li>
<li class="chapter" data-level="6.9" data-path="prediction.html"><a href="prediction.html#question-3-predict-median-mans-percent-body-fat."><i class="fa fa-check"></i><b>6.9</b> Question 3: Predict Median Man’s percent body fat.<span></span></a></li>
<li class="chapter" data-level="6.10" data-path="prediction.html"><a href="prediction.html#constructing-interval-estimates-using-the-predict-function"><i class="fa fa-check"></i><b>6.10</b> Constructing Interval Estimates Using the <code>predict</code> Function<span></span></a></li>
<li class="chapter" data-level="6.11" data-path="prediction.html"><a href="prediction.html#question-4-give-a-practical-interpretation-of-each-interval-estimate-above-in-the-context-of-this-example."><i class="fa fa-check"></i><b>6.11</b> Question 4: Give a practical interpretation of each interval estimate above in the context of this example.<span></span></a></li>
<li class="chapter" data-level="6.12" data-path="prediction.html"><a href="prediction.html#question-5-explain-why-it-makes-sense-that-the-95-prediction-interval-is-much-wider-than-the-95-confidence-interval"><i class="fa fa-check"></i><b>6.12</b> Question 5: Explain why it makes sense that the 95% prediction interval is much wider than the 95% confidence interval?<span></span></a></li>
<li class="chapter" data-level="6.13" data-path="prediction.html"><a href="prediction.html#extrapolation"><i class="fa fa-check"></i><b>6.13</b> Extrapolation<span></span></a></li>
<li class="chapter" data-level="6.14" data-path="prediction.html"><a href="prediction.html#question-6-what-happens-to-the-interval-estimates-when-we-predict-body-fat-for-a-man-named-nine-d.-five-why-does-this-make-practical-sense"><i class="fa fa-check"></i><b>6.14</b> Question 6: What happens to the interval estimates when we predict body fat for a man named Nine D. Five? Why does this make practical sense?<span></span></a></li>
<li class="chapter" data-level="6.15" data-path="prediction.html"><a href="prediction.html#sources-of-uncertainty"><i class="fa fa-check"></i><b>6.15</b> Sources of Uncertainty<span></span></a></li>
<li class="chapter" data-level="6.16" data-path="prediction.html"><a href="prediction.html#what-can-go-wrong-with-predictions"><i class="fa fa-check"></i><b>6.16</b> What Can Go Wrong With Predictions?<span></span></a></li>
<li class="chapter" data-level="6.17" data-path="prediction.html"><a href="prediction.html#question-7-teen-gambling-example"><i class="fa fa-check"></i><b>6.17</b> Question 7: Teen Gambling Example<span></span></a></li>
<li class="chapter" data-level="6.18" data-path="prediction.html"><a href="prediction.html#appendix-computing-standard-errors-for-confidence-and-prediction-intervals"><i class="fa fa-check"></i><b>6.18</b> Appendix: Computing Standard Errors for Confidence and Prediction Intervals<span></span></a>
<ul>
<li class="chapter" data-level="6.18.1" data-path="prediction.html"><a href="prediction.html#confidence-interval-standard-error"><i class="fa fa-check"></i><b>6.18.1</b> Confidence interval standard error:<span></span></a></li>
<li class="chapter" data-level="6.18.2" data-path="prediction.html"><a href="prediction.html#prediction-interval-standard-error"><i class="fa fa-check"></i><b>6.18.2</b> Prediction interval standard error:<span></span></a></li>
<li class="chapter" data-level="6.18.3" data-path="prediction.html"><a href="prediction.html#underlying-distrubtion"><i class="fa fa-check"></i><b>6.18.3</b> Underlying distrubtion<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html"><i class="fa fa-check"></i><b>7</b> Assumptions Stated and Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-concisely-stated"><i class="fa fa-check"></i><b>7.1</b> Standard assumptions concisely stated<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-prioritized"><i class="fa fa-check"></i><b>7.2</b> Standard assumptions prioritized<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html"><i class="fa fa-check"></i><b>8</b> Basic regression diagnostics<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#motivating-example"><i class="fa fa-check"></i><b>8.1</b> Motivating Example<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#standard-assumptions-revisited"><i class="fa fa-check"></i><b>8.2</b> Standard Assumptions Revisited<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#theoretical-properties"><i class="fa fa-check"></i><b>8.3</b> Theoretical Properties<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practical-considerations"><i class="fa fa-check"></i><b>8.4</b> Practical Considerations<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#standard-assumptions-prioritized-1"><i class="fa fa-check"></i><b>8.5</b> Standard Assumptions Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#running-a-statistical-test"><i class="fa fa-check"></i><b>8.5.1</b> Running a Statistical Test<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#checking-normality-of-residuals"><i class="fa fa-check"></i><b>8.6</b> Checking Normality of Residuals<span></span></a></li>
<li class="chapter" data-level="8.7" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#q-q-plots"><i class="fa fa-check"></i><b>8.7</b> Q-Q Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#interpreting-q-q-plots"><i class="fa fa-check"></i><b>8.7.1</b> Interpreting q-q Plots<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#the-shapiro-wilk-test-for-normality"><i class="fa fa-check"></i><b>8.8</b> The Shapiro-Wilk test for Normality<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-interpret-the-result-of-this-test-in-practical-terms."><i class="fa fa-check"></i><b>8.9</b> Question 1: Interpret the result of this test in practical terms.<span></span></a></li>
<li class="chapter" data-level="8.10" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#checking-for-correlated-errorrs"><i class="fa fa-check"></i><b>8.10</b> Checking for Correlated Errorrs<span></span></a></li>
<li class="chapter" data-level="8.11" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#motivating-example-global-warming"><i class="fa fa-check"></i><b>8.11</b> Motivating Example: Global warming<span></span></a></li>
<li class="chapter" data-level="8.12" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-residuals-against-time"><i class="fa fa-check"></i><b>8.12</b> Plotting Residuals Against Time<span></span></a></li>
<li class="chapter" data-level="8.13" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-do-you-notice-any-correlation-in-the-errors"><i class="fa fa-check"></i><b>8.13</b> Question 2: Do you notice any correlation in the errors?<span></span></a></li>
<li class="chapter" data-level="8.14" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-successive-pairs-of-residuals"><i class="fa fa-check"></i><b>8.14</b> Plotting Successive Pairs of Residuals<span></span></a></li>
<li class="chapter" data-level="8.15" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-interpret-the-plot-above.-how-does-this-confirm-a-positive-serial-correlation"><i class="fa fa-check"></i><b>8.15</b> Question 3: Interpret the plot above. How does this confirm a positive serial correlation?<span></span></a></li>
<li class="chapter" data-level="8.16" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#the-durbin-watson-test-for-uncorrelated-errors"><i class="fa fa-check"></i><b>8.16</b> The Durbin-Watson Test for Uncorrelated Errors<span></span></a></li>
<li class="chapter" data-level="8.17" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-interpret-the-result-of-this-test-in-practical-terms."><i class="fa fa-check"></i><b>8.17</b> Question 4: Interpret the result of this test in practical terms.<span></span></a></li>
<li class="chapter" data-level="8.18" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practice-example-a-model-for-sat-scores"><i class="fa fa-check"></i><b>8.18</b> Practice Example: A Model for SAT Scores<span></span></a></li>
<li class="chapter" data-level="8.19" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-methods-for-checking-error-assumptions"><i class="fa fa-check"></i><b>8.19</b> Summary of Methods for Checking Error Assumptions<span></span></a></li>
<li class="chapter" data-level="8.20" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-useful-r-functions-for-checking-error-assumptions"><i class="fa fa-check"></i><b>8.20</b> Summary of useful R functions for checking error assumptions<span></span></a></li>
<li class="chapter" data-level="8.21" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#residuals"><i class="fa fa-check"></i><b>8.21</b> Residuals:<span></span></a></li>
<li class="chapter" data-level="8.22" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#mean-zero-error-assumption"><i class="fa fa-check"></i><b>8.22</b> Mean-zero error assumption:<span></span></a></li>
<li class="chapter" data-level="8.23" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#constant-error-variance-assumption"><i class="fa fa-check"></i><b>8.23</b> Constant error variance assumption:<span></span></a></li>
<li class="chapter" data-level="8.24" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#to-assess-error-normality"><i class="fa fa-check"></i><b>8.24</b> To assess error normality:<span></span></a></li>
<li class="chapter" data-level="8.25" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#correlated-errors"><i class="fa fa-check"></i><b>8.25</b> Correlated Errors<span></span></a></li>
<li class="chapter" data-level="8.26" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#unusual-observations"><i class="fa fa-check"></i><b>8.26</b> Unusual Observations<span></span></a></li>
<li class="chapter" data-level="8.27" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#identifying-leverage-points"><i class="fa fa-check"></i><b>8.27</b> Identifying Leverage Points<span></span></a></li>
<li class="chapter" data-level="8.28" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-the-hat-matrix-and-leverage-values-for-savings-model"><i class="fa fa-check"></i><b>8.28</b> Computing the Hat Matrix and Leverage Values for Savings Model<span></span></a></li>
<li class="chapter" data-level="8.29" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#half-normal-plots"><i class="fa fa-check"></i><b>8.29</b> Half-Normal Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.29.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#creating-a-half-normal-plot"><i class="fa fa-check"></i><b>8.29.1</b> Creating a Half-Normal Plot<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.30" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-complete-the-code-cell-below-to-create-a-half-normal-plot"><i class="fa fa-check"></i><b>8.30</b> Question 1: Complete the code cell below to create a half-normal plot<span></span></a>
<ul>
<li class="chapter" data-level="8.30.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#example-with-no-leverage-points"><i class="fa fa-check"></i><b>8.30.1</b> Example With No Leverage Points<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.31" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots"><i class="fa fa-check"></i><b>8.31</b> Index Plots<span></span></a></li>
<li class="chapter" data-level="8.32" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#identifying-outliers"><i class="fa fa-check"></i><b>8.32</b> Identifying outliers<span></span></a></li>
<li class="chapter" data-level="8.33" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#examples-of-outliers"><i class="fa fa-check"></i><b>8.33</b> Examples of Outliers<span></span></a></li>
<li class="chapter" data-level="8.34" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leave-one-out-statistics"><i class="fa fa-check"></i><b>8.34</b> Leave-One-Out Statistics<span></span></a></li>
<li class="chapter" data-level="8.35" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#studentized-residuals"><i class="fa fa-check"></i><b>8.35</b> Studentized Residuals<span></span></a>
<ul>
<li class="chapter" data-level="8.35.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#bonferonni-correction"><i class="fa fa-check"></i><b>8.35.1</b> Bonferonni correction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.36" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-why-do-we-divide-by-2n-and-not-just-n"><i class="fa fa-check"></i><b>8.36</b> Question 3: Why do we divide by <span class="math inline">\(2n\)</span> and not just <span class="math inline">\(n\)</span>?<span></span></a>
<ul>
<li class="chapter" data-level="8.36.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-studentized-residuals"><i class="fa fa-check"></i><b>8.36.1</b> Computing Studentized Residuals<span></span></a></li>
<li class="chapter" data-level="8.36.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-correction-p-value"><i class="fa fa-check"></i><b>8.36.2</b> Computing Bonferonni correction P-value<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.37" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-based-on-the-output-above-does-this-model-have-any-outliers"><i class="fa fa-check"></i><b>8.37</b> Question 4: Based on the output above, does this model have any outliers?<span></span></a>
<ul>
<li class="chapter" data-level="8.37.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#using-the-outliertest-function"><i class="fa fa-check"></i><b>8.37.1</b> Using the outlierTest Function<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.38" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots-of-studentized-residuals-and-bonferroni-p-values"><i class="fa fa-check"></i><b>8.38</b> Index Plots of Studentized Residuals and Bonferroni p-values<span></span></a></li>
<li class="chapter" data-level="8.39" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-unusual-observations"><i class="fa fa-check"></i><b>8.39</b> Summary of Unusual Observations<span></span></a></li>
<li class="chapter" data-level="8.40" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>8.40</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.41" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leave-one-out-statistics-1"><i class="fa fa-check"></i><b>8.41</b> Leave-One-Out Statistics<span></span></a></li>
<li class="chapter" data-level="8.42" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-what-is-the-value-of-the-leave-one-out-residual-corresponding-to-the-obervation-from-zambia"><i class="fa fa-check"></i><b>8.42</b> Question 1: What is the value of the leave-one-out residual corresponding to the obervation from Zambia?<span></span></a></li>
<li class="chapter" data-level="8.43" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-in-analyzing-another-marketing-dataset-the-largest-leave-one-out-residual-value-is-12.48.-do-you-believe-this-observation-is-more-of-an-outlier-in-marketing-model-compared-to-zambia-in-the-savings-model"><i class="fa fa-check"></i><b>8.43</b> Question 2: In analyzing another marketing dataset, the largest leave-one-out residual value is <span class="math inline">\(12.48\)</span>. Do you believe this observation is more of an outlier in marketing model compared to Zambia in the savings model?<span></span></a></li>
<li class="chapter" data-level="8.44" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#studentized-residuals-1"><i class="fa fa-check"></i><b>8.44</b> Studentized Residuals<span></span></a>
<ul>
<li class="chapter" data-level="8.44.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-studentized-residuals-1"><i class="fa fa-check"></i><b>8.44.1</b> Computing Studentized Residuals<span></span></a></li>
<li class="chapter" data-level="8.44.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#plotting-studentized-residuals"><i class="fa fa-check"></i><b>8.44.2</b> Plotting Studentized Residuals<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.45" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#testing-for-outliers"><i class="fa fa-check"></i><b>8.45</b> Testing for Outliers<span></span></a>
<ul>
<li class="chapter" data-level="8.45.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#types-of-errors-and-the-significance-level"><i class="fa fa-check"></i><b>8.45.1</b> Types of Errors and the Significance Level<span></span></a></li>
<li class="chapter" data-level="8.45.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#bonferonni-correction-1"><i class="fa fa-check"></i><b>8.45.2</b> Bonferonni correction<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.46" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-what-is-the-maximum-value-for-the-probability-of-making-a-type-i-error-if-the-results-of-the-two-individual-tests-are-significant."><i class="fa fa-check"></i><b>8.46</b> Question 3: What is the maximum value for the probability of making a Type I Error if the results of the two individual tests are significant.<span></span></a>
<ul>
<li class="chapter" data-level="8.46.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-critical-values"><i class="fa fa-check"></i><b>8.46.1</b> Computing Bonferonni Critical Values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.47" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-based-on-the-output-above-does-this-model-have-any-outliers-1"><i class="fa fa-check"></i><b>8.47</b> Question 4: Based on the output above, does this model have any outliers?<span></span></a>
<ul>
<li class="chapter" data-level="8.47.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-bonferonni-p-values-with-outliertest-function"><i class="fa fa-check"></i><b>8.47.1</b> Computing Bonferonni p-Values with outlierTest Function<span></span></a></li>
<li class="chapter" data-level="8.47.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#index-plots-of-bonferroni-p-values"><i class="fa fa-check"></i><b>8.47.2</b> Index Plots of Bonferroni p-values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.48" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#caution-with-outliers"><i class="fa fa-check"></i><b>8.48</b> Caution With Outliers:<span></span></a></li>
<li class="chapter" data-level="8.49" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations"><i class="fa fa-check"></i><b>8.49</b> Influential Observations<span></span></a></li>
<li class="chapter" data-level="8.50" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#change-in-regression-coefficients-dfbeta"><i class="fa fa-check"></i><b>8.50</b> Change in Regression Coefficients, DFBETA<span></span></a></li>
<li class="chapter" data-level="8.51" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-5-how-does-the-model-fit-change-when-we-remove-libya-from-the-data-which-coefficients-seem-most-influenced"><i class="fa fa-check"></i><b>8.51</b> Question 5: How does the model fit change when we remove Libya from the data? Which coefficients seem most influenced?<span></span></a>
<ul>
<li class="chapter" data-level="8.51.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#ploting-dfbeta-and-dfbetas"><i class="fa fa-check"></i><b>8.51.1</b> Ploting DFBETA and DFBETAs<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.52" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#cooks-distance"><i class="fa fa-check"></i><b>8.52</b> Cook’s Distance<span></span></a>
<ul>
<li class="chapter" data-level="8.52.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#computing-and-plotting-cooks-distance"><i class="fa fa-check"></i><b>8.52.1</b> Computing and Plotting Cook’s Distance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.53" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influence-plots"><i class="fa fa-check"></i><b>8.53</b> Influence Plots<span></span></a></li>
<li class="chapter" data-level="8.54" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#what-should-we-do-about-outliers-and-influential-observations"><i class="fa fa-check"></i><b>8.54</b> What should we do about outliers and influential observations?<span></span></a></li>
<li class="chapter" data-level="8.55" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#should-we-correct-or-delete-the-observations"><i class="fa fa-check"></i><b>8.55</b> Should we correct or delete the observation(s)?<span></span></a></li>
<li class="chapter" data-level="8.56" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#should-we-keep-them-and-fit-a-different-model"><i class="fa fa-check"></i><b>8.56</b> Should we keep them and fit a different model?<span></span></a></li>
<li class="chapter" data-level="8.57" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-1"><i class="fa fa-check"></i><b>8.57</b> Summary<span></span></a></li>
<li class="chapter" data-level="8.58" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#practice-sat-example"><i class="fa fa-check"></i><b>8.58</b> Practice: SAT Example<span></span></a></li>
<li class="chapter" data-level="8.59" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#part-1-fit-a-model-with-the-total-sat-score-as-the-response-and-expend-salary-ratio-and-takers-as-regressors."><i class="fa fa-check"></i><b>8.59</b> Part 1: Fit a model with the total SAT score as the response and expend, salary, ratio, and takers as regressors.<span></span></a></li>
<li class="chapter" data-level="8.60" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#part-2-perform-regression-diagnostics-on-this-model-to-answer-the-following-questions."><i class="fa fa-check"></i><b>8.60</b> Part 2: Perform regression diagnostics on this model to answer the following questions.<span></span></a></li>
<li class="chapter" data-level="8.61" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#glossary"><i class="fa fa-check"></i><b>8.61</b> Glossary<span></span></a></li>
<li class="chapter" data-level="8.62" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-methods-for-identifying-unusual-observations"><i class="fa fa-check"></i><b>8.62</b> Summary of methods for identifying unusual observations<span></span></a>
<ul>
<li class="chapter" data-level="8.62.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leverage-points"><i class="fa fa-check"></i><b>8.62.1</b> Leverage points:<span></span></a></li>
<li class="chapter" data-level="8.62.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers-1"><i class="fa fa-check"></i><b>8.62.2</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.62.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations-1"><i class="fa fa-check"></i><b>8.62.3</b> Influential observations:<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.63" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#summary-of-useful-r-functions-for-identifying-unusual-observations"><i class="fa fa-check"></i><b>8.63</b> Summary of useful R functions for identifying unusual observations<span></span></a>
<ul>
<li class="chapter" data-level="8.63.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#leverage-points-1"><i class="fa fa-check"></i><b>8.63.1</b> Leverage points<span></span></a></li>
<li class="chapter" data-level="8.63.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#outliers-2"><i class="fa fa-check"></i><b>8.63.2</b> Outliers<span></span></a></li>
<li class="chapter" data-level="8.63.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#influential-observations-2"><i class="fa fa-check"></i><b>8.63.3</b> Influential observations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.64" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#model-structure"><i class="fa fa-check"></i><b>8.64</b> Model Structure<span></span></a>
<ul>
<li class="chapter" data-level="8.64.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#residual-plots"><i class="fa fa-check"></i><b>8.64.1</b> Residual Plots<span></span></a></li>
<li class="chapter" data-level="8.64.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#example"><i class="fa fa-check"></i><b>8.64.2</b> Example<span></span></a></li>
<li class="chapter" data-level="8.64.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-1-comment-on-any-patterns-you-observe-in-the-residual-plots-above."><i class="fa fa-check"></i><b>8.64.3</b> Question 1: Comment on any patterns you observe in the residual plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.65" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#tests-to-determine-nonlinearity"><i class="fa fa-check"></i><b>8.65</b> Tests to Determine Nonlinearity<span></span></a>
<ul>
<li class="chapter" data-level="8.65.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#lack-of-fit-test"><i class="fa fa-check"></i><b>8.65.1</b> Lack of Fit Test<span></span></a></li>
<li class="chapter" data-level="8.65.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-2-does-the-output-above-support-the-claim-that-the-income-variable-has-a-somewhat-nonlinear-pattern-explain-why-or-why-not."><i class="fa fa-check"></i><b>8.65.2</b> Question 2: Does the output above support the claim that the income variable has a somewhat nonlinear pattern? Explain why or why not.<span></span></a></li>
<li class="chapter" data-level="8.65.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#tukeys-test-for-nonadditivity"><i class="fa fa-check"></i><b>8.65.3</b> Tukey’s Test for Nonadditivity<span></span></a></li>
<li class="chapter" data-level="8.65.4" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-3-summarize-the-output-above-in-practical-terms."><i class="fa fa-check"></i><b>8.65.4</b> Question 3: Summarize the output above in practical terms.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.66" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#marginal-model-plot"><i class="fa fa-check"></i><b>8.66</b> Marginal Model Plot<span></span></a>
<ul>
<li class="chapter" data-level="8.66.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-4-interpret-the-output-of-the-marginal-model-plots-above."><i class="fa fa-check"></i><b>8.66.1</b> Question 4: Interpret the output of the marginal model plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.67" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#added-variable-plots"><i class="fa fa-check"></i><b>8.67</b> Added Variable Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.67.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#creating-added-variable-plots"><i class="fa fa-check"></i><b>8.67.1</b> Creating Added Variable Plots<span></span></a></li>
<li class="chapter" data-level="8.67.2" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#interpreting-added-variable-plots"><i class="fa fa-check"></i><b>8.67.2</b> Interpreting Added Variable Plots<span></span></a></li>
<li class="chapter" data-level="8.67.3" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-5-interpret-the-output-of-the-added-value-plots-above."><i class="fa fa-check"></i><b>8.67.3</b> Question 5: Interpret the output of the added value plots above.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.68" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#component-plus-residual-plots"><i class="fa fa-check"></i><b>8.68</b> Component Plus Residual Plots<span></span></a>
<ul>
<li class="chapter" data-level="8.68.1" data-path="basic-regression-diagnostics.html"><a href="basic-regression-diagnostics.html#question-6-do-the-cr-plots-provide-evidence-of-clear-nonlinear-relationships-for-any-of-the-variables"><i class="fa fa-check"></i><b>8.68.1</b> Question 6: Do the cr plots provide evidence of clear nonlinear relationships for any of the variables?<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html"><i class="fa fa-check"></i><b>9</b> Assessing and addressing collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#motivating-example-driver-seat-settings"><i class="fa fa-check"></i><b>9.1</b> Motivating Example: Driver Seat Settings<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-1-does-r2-value-of-the-model-seem-consistent-with-the-p-values-for-each-coefficient-why-or-why-not"><i class="fa fa-check"></i><b>9.1.1</b> Question 1: Does <span class="math inline">\(R^2\)</span> value of the model seem consistent with the p-values for each coefficient? Why or why not?<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-2-what-are-some-methods-we-have-use-to-check-for-correlations-between-regressors"><i class="fa fa-check"></i><b>9.1.2</b> Question 2: What are some methods we have use to check for correlations between regressors?<span></span></a></li>
<li class="chapter" data-level="9.1.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-3-do-you-see-any-large-pairwise-correlations-do-these-make-practical-sense-in-this-context"><i class="fa fa-check"></i><b>9.1.3</b> Question 3: Do you see any large pairwise correlations? Do these make practical sense in this context?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#collinearity"><i class="fa fa-check"></i><b>9.2</b> Collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#why-is-collinearity-problematic"><i class="fa fa-check"></i><b>9.2.1</b> Why is Collinearity Problematic?<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-4-comment-on-the-changes-to-the-coefficients-from-model-1-and-model-2."><i class="fa fa-check"></i><b>9.2.2</b> Question 4: Comment on the changes to the coefficients from Model 1 and Model 2.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#methods-for-detecting-collinearity"><i class="fa fa-check"></i><b>9.3</b> Methods for Detecting Collinearity<span></span></a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#examine-the-correlation-matrix"><i class="fa fa-check"></i><b>9.3.1</b> Examine the Correlation Matrix<span></span></a></li>
<li class="chapter" data-level="9.3.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#regress-on-predictors"><i class="fa fa-check"></i><b>9.3.2</b> Regress on Predictor(s)<span></span></a></li>
<li class="chapter" data-level="9.3.3" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#the-variance-inflation-factor-vif"><i class="fa fa-check"></i><b>9.3.3</b> The Variance Inflation Factor (VIF)<span></span></a></li>
<li class="chapter" data-level="9.3.4" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-5-which-predictors-seem-to-have-a-problem-with-collinearity"><i class="fa fa-check"></i><b>9.3.4</b> Question 5: Which predictors seem to have a problem with collinearity?<span></span></a></li>
<li class="chapter" data-level="9.3.5" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#interpreting-vif"><i class="fa fa-check"></i><b>9.3.5</b> Interpreting VIF<span></span></a></li>
<li class="chapter" data-level="9.3.6" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-6-play-around-with-model-and-see-what-happens-when-your-remove-correlated-variables."><i class="fa fa-check"></i><b>9.3.6</b> Question 6: Play around with model and see what happens when your remove correlated variables.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#examine-the-eigenvalues-of-xtx."><i class="fa fa-check"></i><b>9.4</b> Examine the eigenvalues of <span class="math inline">\(X^TX\)</span>.<span></span></a></li>
<li class="chapter" data-level="9.5" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#including-the-intercept-and-scaling-regressors"><i class="fa fa-check"></i><b>9.5</b> Including the Intercept and Scaling Regressors<span></span></a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#which-regressors-are-leading-to-large-condition-indices"><i class="fa fa-check"></i><b>9.5.1</b> Which Regressors are Leading to Large Condition Indices?<span></span></a></li>
<li class="chapter" data-level="9.5.2" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-7-based-on-the-output-above-which-regressors-should-be-removed-which-should-we-remove-first"><i class="fa fa-check"></i><b>9.5.2</b> Question 7: Based on the output above, which regressors should be removed? Which should we remove first?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#dropping-regressors-from-our-model"><i class="fa fa-check"></i><b>9.6</b> Dropping Regressors from our model<span></span></a></li>
<li class="chapter" data-level="9.7" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#wrap-up"><i class="fa fa-check"></i><b>9.7</b> Wrap-Up<span></span></a></li>
<li class="chapter" data-level="9.8" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#note-on-predictions"><i class="fa fa-check"></i><b>9.8</b> Note on predictions<span></span></a></li>
<li class="chapter" data-level="9.9" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html#question-8-what-methods-can-be-utilized-to-look-for-and-fix-problems-with-collinearity"><i class="fa fa-check"></i><b>9.9</b> Question 8: What methods can be utilized to look for and fix problems with collinearity?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>10</b> Variable Selection<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="variable-selection.html"><a href="variable-selection.html#loading-the-data"><i class="fa fa-check"></i><b>10.1</b> Loading the Data<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="variable-selection.html"><a href="variable-selection.html#question-1-why-might-it-be-a-bad-a-idea-to-simply-include-all-regressors"><i class="fa fa-check"></i><b>10.2</b> Question 1: Why might it be a bad a idea to simply include all regressors?<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="variable-selection.html"><a href="variable-selection.html#question-2-what-metricsmethods-have-we-used-to-compare-models-thus-far"><i class="fa fa-check"></i><b>10.3</b> Question 2: What metrics/methods have we used to compare models thus far?<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="variable-selection.html"><a href="variable-selection.html#testing-based-procedures"><i class="fa fa-check"></i><b>10.4</b> Testing-Based Procedures<span></span></a></li>
<li class="chapter" data-level="10.5" data-path="variable-selection.html"><a href="variable-selection.html#backward-elimination"><i class="fa fa-check"></i><b>10.5</b> Backward elimination<span></span></a></li>
<li class="chapter" data-level="10.6" data-path="variable-selection.html"><a href="variable-selection.html#question-3-which-of-the-predictors-would-you-remove-from-the-full-model-what-criteria-did-you-use-to-make-that-decision"><i class="fa fa-check"></i><b>10.6</b> Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision?<span></span></a></li>
<li class="chapter" data-level="10.7" data-path="variable-selection.html"><a href="variable-selection.html#forward-selection"><i class="fa fa-check"></i><b>10.7</b> Forward Selection<span></span></a></li>
<li class="chapter" data-level="10.8" data-path="variable-selection.html"><a href="variable-selection.html#stepwise-regression"><i class="fa fa-check"></i><b>10.8</b> Stepwise Regression<span></span></a></li>
<li class="chapter" data-level="10.9" data-path="variable-selection.html"><a href="variable-selection.html#model-hierarchy"><i class="fa fa-check"></i><b>10.9</b> Model Hierarchy<span></span></a></li>
<li class="chapter" data-level="10.10" data-path="variable-selection.html"><a href="variable-selection.html#criterion-based-procedures"><i class="fa fa-check"></i><b>10.10</b> Criterion-Based Procedures<span></span></a></li>
<li class="chapter" data-level="10.11" data-path="variable-selection.html"><a href="variable-selection.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>10.11</b> Akaike’s Information Criterion (AIC)<span></span></a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="variable-selection.html"><a href="variable-selection.html#interpreting-aic"><i class="fa fa-check"></i><b>10.11.1</b> Interpreting AIC<span></span></a></li>
<li class="chapter" data-level="10.11.2" data-path="variable-selection.html"><a href="variable-selection.html#exhaustive-model-searches"><i class="fa fa-check"></i><b>10.11.2</b> Exhaustive Model Searches<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="variable-selection.html"><a href="variable-selection.html#question-4-interpret-the-output-from-the-code-above.-is-this-consistent-with-the-model-we-obtained-using-backward-elimination"><i class="fa fa-check"></i><b>10.12</b> Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination?<span></span></a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="variable-selection.html"><a href="variable-selection.html#computing-the-aic"><i class="fa fa-check"></i><b>10.12.1</b> Computing the AIC<span></span></a></li>
<li class="chapter" data-level="10.12.2" data-path="variable-selection.html"><a href="variable-selection.html#question-5-interpret-the-output-from-the-aic-plots-above.-what-is-the-best-model-according-to-this-metric"><i class="fa fa-check"></i><b>10.12.2</b> Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="variable-selection.html"><a href="variable-selection.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>10.13</b> Bayesian Information Criterion (BIC)<span></span></a></li>
<li class="chapter" data-level="10.14" data-path="variable-selection.html"><a href="variable-selection.html#summary-2"><i class="fa fa-check"></i><b>10.14</b> Summary<span></span></a></li>
<li class="chapter" data-level="10.15" data-path="variable-selection.html"><a href="variable-selection.html#exercise"><i class="fa fa-check"></i><b>10.15</b> Exercise<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="transformations.html"><a href="transformations.html"><i class="fa fa-check"></i><b>11</b> Transformations<span></span></a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="transformations.html"><a href="transformations.html#question-7-compare-the-graphic-below-with-the-type-of-bulge-seen-in-your-data-move-along-the-ladder-of-transformations-for-your-response-or-predictors-to-determine-a-helpful-transformation."><i class="fa fa-check"></i><b>11.0.1</b> Question 7: Compare the graphic below with the type of “bulge” seen in your data; move along the “ladder of transformations” for your response or predictors to determine a helpful transformation.<span></span></a></li>
<li class="chapter" data-level="11.0.2" data-path="transformations.html"><a href="transformations.html#question-8-which-transformation-of-income-seems-like-a-better-fit"><i class="fa fa-check"></i><b>11.0.2</b> Question 8: Which transformation of income seems like a better fit?<span></span></a></li>
<li class="chapter" data-level="11.0.3" data-path="transformations.html"><a href="transformations.html#polynomial-transformations"><i class="fa fa-check"></i><b>11.0.3</b> Polynomial Transformations<span></span></a></li>
<li class="chapter" data-level="11.0.4" data-path="transformations.html"><a href="transformations.html#logistic-transforation"><i class="fa fa-check"></i><b>11.0.4</b> Logistic Transforation<span></span></a></li>
<li class="chapter" data-level="11.0.5" data-path="transformations.html"><a href="transformations.html#closing-comments-on-transformations"><i class="fa fa-check"></i><b>11.0.5</b> Closing Comments on Transformations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="advanced-regressor-variables.html"><a href="advanced-regressor-variables.html"><i class="fa fa-check"></i><b>12</b> Advanced regressor variables<span></span></a></li>
<li class="chapter" data-level="13" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html"><i class="fa fa-check"></i><b>13</b> More on categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#indicatordummy-variables"><i class="fa fa-check"></i><b>13.1</b> Indicator/dummy variables<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#common-of-linear-models-with-categorical-predictors"><i class="fa fa-check"></i><b>13.2</b> Common of linear models with categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#one-way-anova"><i class="fa fa-check"></i><b>13.2.1</b> One-way ANOVA<span></span></a></li>
<li class="chapter" data-level="13.2.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#main-effects-models"><i class="fa fa-check"></i><b>13.2.2</b> Main effects models<span></span></a></li>
<li class="chapter" data-level="13.2.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interaction-models"><i class="fa fa-check"></i><b>13.2.3</b> Interaction models<span></span></a></li>
<li class="chapter" data-level="13.2.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#extensions-1"><i class="fa fa-check"></i><b>13.2.4</b> Extensions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploring-data-for-one-factor-models-example-with-united-nations-dataset"><i class="fa fa-check"></i><b>13.3</b> Exploring Data for One-Factor Models: Example with United Nations Dataset<span></span></a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#load-the-un11-dataset-below-and-read-the-help-documentation-to-familiarize-yourself-with-the-data.-how-many-observations-are-in-the-dataset-how-many-variables"><i class="fa fa-check"></i><b>13.3.1</b> 1. Load the <code>UN11</code> dataset below and read the help documentation to familiarize yourself with the data. How many observations are in the dataset? How many variables?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>13.4</b> Exploratory Data Analysis<span></span></a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#create-side-by-side-boxplots-for-the-variable-lifeexpf-for-each-of-the-categories-of-the-group-variable-and-describe-any-interesting-observations-about-the-data."><i class="fa fa-check"></i><b>13.4.1</b> 2. Create side-by-side boxplots for the variable <strong>lifeExpF</strong> for each of the categories of the <strong>group</strong> variable and describe any interesting observations about the data.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#how-do-we-include-factor-variables-as-regressors"><i class="fa fa-check"></i><b>13.5</b> How do we include factor variables as regressors?<span></span></a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#explain-what-the-code-below-is-doing-what-happens-if-we-remove-the-0-from-the-commands"><i class="fa fa-check"></i><b>13.5.1</b> 3. Explain what the code below is doing? What happens if we remove the <span class="math inline">\(+ 0\)</span> from the commands?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#the-one-factor-model"><i class="fa fa-check"></i><b>13.6</b> The one-factor model<span></span></a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#let-y-denote-the-response-variable-lifeexpf.-what-would-be-problematic-about-using-the-linear-model-proposed-below"><i class="fa fa-check"></i><b>13.6.1</b> 4. Let <span class="math inline">\(Y\)</span> denote the response variable <code>lifeExpF</code>. What would be problematic about using the linear model proposed below?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-ggplot2"><i class="fa fa-check"></i><b>13.7</b> Exploratory Data Analsis with <code>ggplot2</code><span></span></a></li>
<li class="chapter" data-level="13.8" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-lattice"><i class="fa fa-check"></i><b>13.8</b> Exploratory Data Analsis with <code>lattice</code><span></span></a></li>
<li class="chapter" data-level="13.9" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#exploratory-data-analsis-with-base-graphics"><i class="fa fa-check"></i><b>13.9</b> Exploratory Data Analsis with Base Graphics<span></span></a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#what-are-some-interesting-observationspatterns-you-can-see-in-the-plots-above"><i class="fa fa-check"></i><b>13.9.1</b> 1. What are some interesting observations/patterns you can see in the plots above?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#comparing-different-models-1"><i class="fa fa-check"></i><b>13.10</b> Comparing Different Models<span></span></a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-simple-linear-model-with-logppgdp-as-the-regressor-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.1</b> 2. Enter code to fit a simple linear model with <code>log(ppgdp)</code> as the regressor and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-one-way-model-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.2</b> 3. Enter code to fit a one-way model with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.3" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-a-main-effects-model-with-logppgdp-as-a-predictor-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response."><i class="fa fa-check"></i><b>13.10.3</b> 4. Enter code to fit a main effects model with <code>log(ppgdp)</code> as a predictor with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response.<span></span></a></li>
<li class="chapter" data-level="13.10.4" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#enter-code-to-fit-an-interaction-model-with-logppgdp-as-a-predictor-with-u2-and-u3-as-the-treatment-levels-and-lifeexpf-as-the-response"><i class="fa fa-check"></i><b>13.10.4</b> 5. Enter code to fit an interaction model with <code>log(ppgdp)</code> as a predictor with <code>U2</code> and <code>U3</code> as the treatment levels and <code>lifeExpF</code> as the response<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.11" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#model-matrix-for-the-interaction-model"><i class="fa fa-check"></i><b>13.11</b> Model Matrix for the Interaction Model<span></span></a>
<ul>
<li class="chapter" data-level="13.11.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#how-are-the-last-two-columns-of-x-computed"><i class="fa fa-check"></i><b>13.11.1</b> 6. How are the last two columns of <span class="math inline">\(X\)</span> computed?<span></span></a></li>
<li class="chapter" data-level="13.11.2" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#find-formulas-for-elifeexpf-vert-ppgdp-x-groupoecd-elifeexpf-vert-ppgdp-x-group-other-and-elifeexpf-vert-ppgdp-x-group-africa."><i class="fa fa-check"></i><b>13.11.2</b> 7. Find formulas for E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group=oecd), E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = other) and E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = africa).<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.12" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#plotting-the-interaction-model"><i class="fa fa-check"></i><b>13.12</b> Plotting the Interaction Model<span></span></a>
<ul>
<li class="chapter" data-level="13.12.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interpret-each-coefficient-in-your-previous-formulas-in-the-context-of-this-dataset."><i class="fa fa-check"></i><b>13.12.1</b> 8. Interpret each coefficient in your previous formulas in the context of this dataset.<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.13" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#interpreting-coefficients-in-the-interaction-model"><i class="fa fa-check"></i><b>13.13</b> Interpreting Coefficients in the Interaction Model<span></span></a></li>
<li class="chapter" data-level="13.14" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-interaction-model"><i class="fa fa-check"></i><b>13.14</b> Effects Plot for Interaction Model<span></span></a></li>
<li class="chapter" data-level="13.15" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-interaction-model-with-log-scale"><i class="fa fa-check"></i><b>13.15</b> Effects Plot for Interaction Model with Log Scale<span></span></a>
<ul>
<li class="chapter" data-level="13.15.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#what-are-some-interesting-observations-we-can-infer-from-the-plot-above"><i class="fa fa-check"></i><b>13.15.1</b> 9. What are some interesting observations we can infer from the plot above?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.16" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#main-effects-model"><i class="fa fa-check"></i><b>13.16</b> Main Effects Model<span></span></a>
<ul>
<li class="chapter" data-level="13.16.1" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#find-formulas-for-elifeexpf-vert-ppgdp-x-groupoecd-elifeexpf-vert-ppgdp-x-group-other-and-elifeexpf-vert-ppgdp-x-group-africa.-1"><i class="fa fa-check"></i><b>13.16.1</b> 10. Find formulas for E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group=oecd), E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = other) and E(lifeExpF <span class="math inline">\(\vert\)</span> ppgdp = x, group = africa).<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.17" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#plotting-the-main-effects-model"><i class="fa fa-check"></i><b>13.17</b> Plotting the Main Effects Model<span></span></a></li>
<li class="chapter" data-level="13.18" data-path="more-on-categorical-predictors.html"><a href="more-on-categorical-predictors.html#effects-plot-for-the-main-effects-model"><i class="fa fa-check"></i><b>13.18</b> Effects Plot for the Main Effects Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="advanced-model-structure.html"><a href="advanced-model-structure.html"><i class="fa fa-check"></i><b>14</b> Advanced model structure<span></span></a></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html"><i class="fa fa-check"></i><b>A</b> Overview of matrix facts<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#notation"><i class="fa fa-check"></i><b>A.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="A.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>A.2</b> Basic mathematical operations<span></span></a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>A.2.1</b> Addition and subtraction<span></span></a></li>
<li class="chapter" data-level="A.2.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>A.2.2</b> Scalar multiplication<span></span></a></li>
<li class="chapter" data-level="A.2.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>A.2.3</b> Matrix multiplication<span></span></a></li>
<li class="chapter" data-level="A.2.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose"><i class="fa fa-check"></i><b>A.2.4</b> Transpose<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>A.3</b> Basic mathematical properties<span></span></a>
<ul>
<li class="chapter" data-level="A.3.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>A.3.1</b> Associative property<span></span></a></li>
<li class="chapter" data-level="A.3.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>A.3.2</b> Distributive property<span></span></a></li>
<li class="chapter" data-level="A.3.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>A.3.3</b> No commutative property<span></span></a></li>
<li class="chapter" data-level="A.3.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#transpose-related-properties"><i class="fa fa-check"></i><b>A.3.4</b> Transpose-related properties<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>A.4</b> Special matrices<span></span></a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>A.4.1</b> Square matrices<span></span></a></li>
<li class="chapter" data-level="A.4.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>A.4.2</b> Identity matrix<span></span></a></li>
<li class="chapter" data-level="A.4.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#diagonal-matrices"><i class="fa fa-check"></i><b>A.4.3</b> Diagonal matrices<span></span></a></li>
<li class="chapter" data-level="A.4.4" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#symmetric-matrices"><i class="fa fa-check"></i><b>A.4.4</b> Symmetric matrices<span></span></a></li>
<li class="chapter" data-level="A.4.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#idempotent-matrices"><i class="fa fa-check"></i><b>A.4.5</b> Idempotent matrices<span></span></a></li>
<li class="chapter" data-level="A.4.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#positive-definite-matrices"><i class="fa fa-check"></i><b>A.4.6</b> Positive definite matrices<span></span></a></li>
<li class="chapter" data-level="A.4.7" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#inverse-matrix"><i class="fa fa-check"></i><b>A.4.7</b> Inverse matrix<span></span></a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>A.5</b> Matrix derivatives<span></span></a></li>
<li class="chapter" data-level="A.6" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#additional-topics"><i class="fa fa-check"></i><b>A.6</b> Additional topics<span></span></a>
<ul>
<li class="chapter" data-level="A.6.1" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#determinant"><i class="fa fa-check"></i><b>A.6.1</b> Determinant<span></span></a></li>
<li class="chapter" data-level="A.6.2" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#linearly-independent-vectors"><i class="fa fa-check"></i><b>A.6.2</b> Linearly independent vectors<span></span></a></li>
<li class="chapter" data-level="A.6.3" data-path="overview-of-matrix-facts.html"><a href="overview-of-matrix-facts.html#rank"><i class="fa fa-check"></i><b>A.6.3</b> Rank<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="prob-review.html"><a href="prob-review.html"><i class="fa fa-check"></i><b>B</b> Overview of probability, random variables, and random vectors<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="prob-review.html"><a href="prob-review.html#probability-basics"><i class="fa fa-check"></i><b>B.1</b> Probability Basics<span></span></a></li>
<li class="chapter" data-level="B.2" data-path="prob-review.html"><a href="prob-review.html#random-variables"><i class="fa fa-check"></i><b>B.2</b> Random Variables<span></span></a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="prob-review.html"><a href="prob-review.html#discrete-random-variables"><i class="fa fa-check"></i><b>B.2.1</b> Discrete random variables<span></span></a></li>
<li class="chapter" data-level="B.2.2" data-path="prob-review.html"><a href="prob-review.html#continuous-random-variables"><i class="fa fa-check"></i><b>B.2.2</b> Continuous random variables<span></span></a></li>
<li class="chapter" data-level="B.2.3" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-random-variables"><i class="fa fa-check"></i><b>B.2.3</b> Useful facts for transformations of random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="prob-review.html"><a href="prob-review.html#multivariate-distributions"><i class="fa fa-check"></i><b>B.3</b> Multivariate distributions<span></span></a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="prob-review.html"><a href="prob-review.html#basic-properties"><i class="fa fa-check"></i><b>B.3.1</b> Basic properties<span></span></a></li>
<li class="chapter" data-level="B.3.2" data-path="prob-review.html"><a href="prob-review.html#marginal-distributions"><i class="fa fa-check"></i><b>B.3.2</b> Marginal distributions<span></span></a></li>
<li class="chapter" data-level="B.3.3" data-path="prob-review.html"><a href="prob-review.html#independence-of-random-variables"><i class="fa fa-check"></i><b>B.3.3</b> Independence of random variables<span></span></a></li>
<li class="chapter" data-level="B.3.4" data-path="prob-review.html"><a href="prob-review.html#conditional-distributions"><i class="fa fa-check"></i><b>B.3.4</b> Conditional distributions<span></span></a></li>
<li class="chapter" data-level="B.3.5" data-path="prob-review.html"><a href="prob-review.html#covariance"><i class="fa fa-check"></i><b>B.3.5</b> Covariance<span></span></a></li>
<li class="chapter" data-level="B.3.6" data-path="prob-review.html"><a href="prob-review.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>B.3.6</b> Useful facts for transformations of multiple random variables<span></span></a></li>
<li class="chapter" data-level="B.3.7" data-path="prob-review.html"><a href="prob-review.html#binomial-distribution-example"><i class="fa fa-check"></i><b>B.3.7</b> Binomial distribution example<span></span></a></li>
<li class="chapter" data-level="B.3.8" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example"><i class="fa fa-check"></i><b>B.3.8</b> Continuous bivariate distribution example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="prob-review.html"><a href="prob-review.html#random-vectors"><i class="fa fa-check"></i><b>B.4</b> Random vectors<span></span></a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="prob-review.html"><a href="prob-review.html#definition-1"><i class="fa fa-check"></i><b>B.4.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="B.4.2" data-path="prob-review.html"><a href="prob-review.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>B.4.2</b> Mean, variance, and covariance<span></span></a></li>
<li class="chapter" data-level="B.4.3" data-path="prob-review.html"><a href="prob-review.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>B.4.3</b> Properties of transformations of random vectors<span></span></a></li>
<li class="chapter" data-level="B.4.4" data-path="prob-review.html"><a href="prob-review.html#continuous-bivariate-distribution-example-continued"><i class="fa fa-check"></i><b>B.4.4</b> Continuous bivariate distribution example continued<span></span></a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="prob-review.html"><a href="prob-review.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>B.5</b> Multivariate normal (Gaussian) distribution<span></span></a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="prob-review.html"><a href="prob-review.html#definition-2"><i class="fa fa-check"></i><b>B.5.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="B.5.2" data-path="prob-review.html"><a href="prob-review.html#linear-functions-of-a-multivariate-normal-random-vector"><i class="fa fa-check"></i><b>B.5.2</b> Linear functions of a multivariate normal random vector<span></span></a></li>
<li class="chapter" data-level="B.5.3" data-path="prob-review.html"><a href="prob-review.html#ols-example"><i class="fa fa-check"></i><b>B.5.3</b> OLS example<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="chapter" data-level="C" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html"><i class="fa fa-check"></i><b>C</b> Defining and fitting a linear model<span></span></a>
<ul>
<li class="chapter" data-level="C.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#background-and-terminology"><i class="fa fa-check"></i><b>C.1</b> Background and terminology<span></span></a></li>
<li class="chapter" data-level="C.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#goals-of-regression"><i class="fa fa-check"></i><b>C.2</b> Goals of regression<span></span></a></li>
<li class="chapter" data-level="C.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>C.3</b> Definition of a linear model<span></span></a>
<ul>
<li class="chapter" data-level="C.3.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#basic-construction-and-relationships"><i class="fa fa-check"></i><b>C.3.1</b> Basic construction and relationships<span></span></a></li>
<li class="chapter" data-level="C.3.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#as-a-system-of-equations"><i class="fa fa-check"></i><b>C.3.2</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="C.3.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#using-matrix-notation"><i class="fa fa-check"></i><b>C.3.3</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#summarizing-the-components-of-a-linear-model"><i class="fa fa-check"></i><b>C.4</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="C.5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#types-of-regression-models"><i class="fa fa-check"></i><b>C.5</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="C.6" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#standard-linear-model-assumptions-and-implications"><i class="fa fa-check"></i><b>C.6</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="C.7" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#parameter-estimation-for-linear-models"><i class="fa fa-check"></i><b>C.7</b> Parameter estimation for linear models<span></span></a></li>
<li class="chapter" data-level="C.8" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#ols-estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>C.8</b> OLS estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#visualizing-the-rss-as-a-function-of-the-estimated-coefficients"><i class="fa fa-check"></i><b>C.8.1</b> Visualizing the RSS as a function of the estimated coefficients<span></span></a></li>
<li class="chapter" data-level="C.8.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#ols-estimators-of-the-simple-linear-regression-parameters-1"><i class="fa fa-check"></i><b>C.8.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="C.9" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>C.9</b> Penguins simple linear regression example<span></span></a>
<ul>
<li class="chapter" data-level="C.9.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#derivation-of-ols-simple-linear-regression-estimators"><i class="fa fa-check"></i><b>C.9.1</b> Derivation of OLS simple linear regression estimators<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Joshua French</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-estimation" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Linear model estimation<a href="linear-model-estimation.html#linear-model-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="a-simple-motivating-example" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> A simple motivating example<a href="linear-model-estimation.html#a-simple-motivating-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose you observe data related to the heights of 5 mothers and their adult daughters. The observed heights (measured in inches) are provided in Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>. Figure <a href="linear-model-estimation.html#fig:mdheights-plot">3.1</a> displays a scatter plot of the height data provided in Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>. Would it be reasonable to use a mother’s height to predict the height of her adult daughter?</p>
<table>
<caption><span id="tab:mdheights">Table 3.1: </span>Heights of mothers and their adult daughters (in).</caption>
<thead>
<tr class="header">
<th align="right">observation</th>
<th align="right">mother’s height (in)</th>
<th align="right">daughter’s height (in)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">57.5</td>
<td align="right">61.5</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">60.5</td>
<td align="right">63.5</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">63.5</td>
<td align="right">63.5</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">66.5</td>
<td align="right">66.5</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">69.5</td>
<td align="right">66.5</td>
</tr>
</tbody>
</table>
<div class="figure"><span style="display:block;" id="fig:mdheights-plot"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/mdheights-plot-1.png" alt="A scatter plot displaying pairs of heights for a mother and her adult daughter." width="672" />
<p class="caption">
Figure 3.1: A scatter plot displaying pairs of heights for a mother and her adult daughter.
</p>
</div>
<p>A <strong>regression analysis</strong> is the process of building a model describing the typical relationship between a set of observed variables. In the present context, we want to model the height of adult daughters using their height of their mothers. The model we build is known as a <strong>regression model</strong>.</p>
<p>The variables in a regression analysis may be divided into two types: the response variable and the predictor variables.</p>
<p>The outcome variable we are trying to predict is known as the <strong>response variable</strong>. Response variables are also known as <strong>outcome</strong>, <strong>output</strong>, or <strong>dependent</strong> variables. The response variable is denoted by <span class="math inline">\(Y\)</span>.</p>
<p>The variables available to model the response variable are known as <strong>predictors variables</strong>. Predictor variables are also known as <strong>explanatory</strong>, <strong>regressor</strong>, <strong>input</strong>, or <strong>dependent</strong> variables or simply as <strong>features</strong>. Following the convention of <span class="citation"><a href="#ref-alr4" role="doc-biblioref">Weisberg</a> (<a href="#ref-alr4" role="doc-biblioref">2014</a>)</span>, we use the term <strong>regressor</strong> to refer to the variables used in our regression model, whether that is the original predictor variable, some transformation of a predictor, some combination of predictors, etc. Thus, every predictor can be a regressor but not all regressors are a predictor. The regressor variables are denoted as <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span>.</p>
<p>A regression analysis assumes that we have observed variables <span class="math inline">\(X_1, X_2, X_3, \ldots, X_{p-1}, Y\)</span> for each of <span class="math inline">\(n\)</span> subjects from some population, with <span class="math inline">\(x_{i,j}\)</span> denoting the value of <span class="math inline">\(X_j\)</span> for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> denoting the value of <span class="math inline">\(Y\)</span> for observation <span class="math inline">\(i\)</span>. Note: if there is only a single regressor, we can denote the regressor as <span class="math inline">\(X\)</span> and the observed values as <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p>
<p>For the height data, the 5 pairs of observed data are denoted
<span class="math display">\[(x_1, Y_1), (x_2, Y_2), \ldots, (x_5, Y_5),\]</span>
with <span class="math inline">\((x_i, Y_i)\)</span> denoting the data for observation <span class="math inline">\(i\)</span>. <span class="math inline">\(x_i\)</span> denotes the mother’s height for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> denotes the daughter’s height for
observation <span class="math inline">\(i\)</span>. Referring to Table <a href="linear-model-estimation.html#tab:mdheights">3.1</a>, we see that, e.g., <span class="math inline">\(x_3 = 63.5\)</span> and <span class="math inline">\(Y_5= 66.5\)</span>.</p>
<!-- The $x_1,x_2,\ldots,x_5$ are observed values of a random variable $X$, -->
<!-- while $Y_1, Y_2, \ldots, Y_5$ are observed values of a random variable -->
<!-- $Y$. Thus, $X$ denotes the height a mother and $Y$ denotes the height of -->
<!-- their adult daughter. We want to model variable $Y$ using -->
<!-- variable $X$. -->
<!-- formally, the **regression model** for $Y$ as a function of $X$, denoted -->
<!-- $E(Y|X)$ is the expected value of $Y$ conditional on the regressor $X$. -->
<!-- The regression model specifically refers to the expected relationship -->
<!-- between the response and regressors. -->
<p>The regression model for <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(E(Y|X)\)</span> is the expected value of <span class="math inline">\(Y\)</span> conditional on the regressor <span class="math inline">\(X\)</span>. Thus, a regression model specifically refers to the expected relationship
between the response and regressors.</p>
<p>A <strong>simple linear regression model</strong> assumes the regression model between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is a straight line using the equation
<span class="math display">\[E(Y\mid X)=\beta_0 + \beta_1 X.\]</span>
The <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> terms of our equation are known as the intercept and slope. In general, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are known as <strong>regression coefficients</strong> and are model parameters that we estimate from our data.</p>
<p>The estimated regression model is denoted by
<span class="math display">\[\hat{E}(Y|X)=\hat{\beta}_0 + \hat{\beta}_1 X,\]</span> where <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that we estimate from the data. A <span class="math inline">\(\hat{}\)</span> above a term indicates it is an estimate. We will refer to <span class="math inline">\(\hat{E}(Y|X)\)</span> as the <strong>fitted model</strong> or <strong>estimated regression model</strong>.</p>
<p>How do we determine the “best fitting” model? Consider Figure <a href="linear-model-estimation.html#fig:three-fitted-lines">3.2</a>, in which 2 potential “best fitting” models are drawn on the scatter plot of the height data. Which one is best?</p>
<div class="figure"><span style="display:block;" id="fig:three-fitted-lines"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/three-fitted-lines-1.png" alt="Comparison of three potential fitted models to some observed data. The fitted models are shown in grey." width="672" />
<p class="caption">
Figure 3.2: Comparison of three potential fitted models to some observed data. The fitted models are shown in grey.
</p>
</div>
<p>The rest of this chapter focuses on defining and estimating the
parameters of a <em>linear</em> regression model.</p>
</div>
<div id="defining-a-linear-model" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Defining a linear model<a href="linear-model-estimation.html#defining-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ss-necessary-components" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Necessary components and notation<a href="linear-model-estimation.html#ss-necessary-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin by defining notation for the components of a linear model and some of their important properties. We repeat some of the previous discussion for clarity.</p>
<ul>
<li><span class="math inline">\(Y\)</span> denotes the response variable.
<ul>
<li>The response variable is treated as a random variable.</li>
<li>We will observe realizations of this random variable for each
observation in our data set.</li>
</ul></li>
<li><span class="math inline">\(X\)</span> denotes a single regressor variable. <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span> denote distinct regressor variables if we are performing multiple regression.
<ul>
<li>The regressor variables are treated as non-random variables.</li>
<li>The observed values of the regressor variables are treated as fixed, known values.</li>
</ul></li>
<li><span class="math inline">\(\mathbb{X}=\{X_0, X_1,\ldots,X_{p-1}\}\)</span> denotes the collection of all regressors under consideration, though this notation is really only needed in the context of multiple regression. <span class="math inline">\(X_0\)</span> is usually the constant regressor 1, which is needed to include an intercept in the multiple regression model.</li>
<li><span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_{p-1}\)</span> denote <strong>regression coefficients</strong>.
<ul>
<li>Regression coefficients are statistical parameters that we will estimate from our data.</li>
<li>The regression coefficients are treated as fixed (non-random) but unknown values.</li>
<li>Regression coefficients are not observable.</li>
</ul></li>
<li><span class="math inline">\(\epsilon\)</span> denotes model <strong>error</strong>.
<ul>
<li>The model error is more accurately described as random variation of each observation from the regression model <span class="math inline">\(E(Y\mid\mathbb{X})\)</span>.</li>
<li>The error is treated as a random variable.</li>
<li>The error is assumed to have mean 0 for all values of the regressors, i.e., <span class="math inline">\(E(\epsilon\mid\mathbb{X}) = 0\)</span>.</li>
<li>The variance of the errors is assumed to be a constant value for all values of the regressors, i.e., <span class="math inline">\(\mathrm{var}(\epsilon\mid\mathbb{X})=\sigma^2\)</span>.</li>
<li>The error is never observable (except in the context of a simulation study where the experimenter literally defines the true model).</li>
</ul></li>
</ul>
</div>
<div id="standard-definition-of-linear-model" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Standard definition of linear model<a href="linear-model-estimation.html#standard-definition-of-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>linear model</strong> for <span class="math inline">\(Y\)</span> is defined by the equation <span class="math display" id="eq:lmdef">\[\begin{align}
Y &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1}
X_{p-1} + \epsilon \\
&amp;= E(Y \mid \mathbb{X}) + \epsilon. \tag{3.1}
\end{align}\]</span></p>
<p>We write the model using the form in Equation <a href="linear-model-estimation.html#eq:lmdef">(3.1)</a> to emphasize the fact <em>the response value equals the expected response for that combination of regressor values plus some error</em>. It should be clear from comparing Equation <a href="linear-model-estimation.html#eq:lmdef">(3.1)</a> with the previous line that
<span class="math display">\[
E(Y \mid \mathbb{X}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1},
\]</span>
which we will prove later.</p>
<p>More generally, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values created from our regressor variables, i.e.,
<span class="math display" id="eq:lmdef-cj">\[\begin{equation}
E(Y \mid X_1, X_2, \ldots, X_{p-1}) = \sum_{j=0}^{p-1} c_j \beta_j, \tag{3.2}
\end{equation}\]</span>
where <span class="math inline">\(c_0, c_1, \ldots, c_{p-1}\)</span> are known functions of
the regressor variables, e.g., <span class="math inline">\(c_1 = X_1 X_2 X_3\)</span>, <span class="math inline">\(c_3 = X_2^2\)</span>,
<span class="math inline">\(c_8 = \ln(X_1)/X_2^2\)</span>, etc. Thus, if <span class="math inline">\(g_0,\ldots,g_{p-1}\)</span> are functions of <span class="math inline">\(\mathbb{X}\)</span>, then we can say that the regression model is linear if
it can be written as
<span class="math display">\[
E(Y\mid \mathbb{X}) = \sum_{j=0}^{p-1} g_j(\mathbb{X})\beta_j.
\]</span></p>
<p>Some examples of linear regression models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + +\beta_1 X + \beta_2 X^2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2^{-1}\)</span>.</li>
<li><span class="math inline">\(E(\ln(Y)|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y^{-1}|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
</ul>
<p>Some examples of non-linear regression models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0 + e^{\beta_1 X}\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + \beta_1 X/(\beta_2 + X)\)</span>.</li>
</ul>
<p>The latter regression models are non-linear models because there is no way to express them using the expression in Equation <a href="linear-model-estimation.html#eq:lmdef-cj">(3.2)</a>.</p>
<p>There are many different methods of parameter estimation in statistics: method-of-moments, maximum likelihood, Bayesian, etc. The most common parameter estimation method for linear models is the <strong>least squares method</strong>, which is commonly called <strong>Ordinary Least Squares (OLS)</strong> estimation. OLS estimation estimates the regression coefficients with the values that minimize the residual sum of squares (RSS), which we will define shortly.</p>
</div>
</div>
<div id="estimation-of-the-simple-linear-regression-model" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Estimation of the simple linear regression model<a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ss:fv-resid-rss" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Fitted values, residuals, and RSS<a href="linear-model-estimation.html#ss:fv-resid-rss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that a simple linear regression model is defined by the equation
<span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon = E(Y|X) + \epsilon\]</span>
where
<span class="math display">\[E(Y|X) = \beta_0 + \beta_1 X.\]</span>
In a simple linear regression context, we have <span class="math inline">\(n\)</span> observed responses <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> and <span class="math inline">\(n\)</span> regressor values <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>.</p>
<p>Let <span class="math inline">\(\hat{\beta}_j\)</span> denote the estimated value of <span class="math inline">\(\beta_j\)</span> and
<span class="math inline">\(\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X\)</span> denote the estimated regression model.</p>
<p>The <span class="math inline">\(i\)</span>th <strong>fitted value</strong> is defined as
<span class="math display" id="eq:def-fitted-value-slr">\[\begin{equation}
\hat{Y}_i = \hat{E}(Y|X = x_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i. \tag{3.3}
\end{equation}\]</span>
Thus, the <span class="math inline">\(i\)</span>th fitted value is the estimated mean of <span class="math inline">\(Y\)</span> when the regressor <span class="math inline">\(X=x_i\)</span>. More specifically, the <span class="math inline">\(i\)</span>th fitted value is the estimated mean response for the combination of regressor values observed for the <span class="math inline">\(i\)</span>th observation.</p>
<p>The <span class="math inline">\(i\)</span>th <strong>residual</strong> is defined as
<span class="math display" id="eq:def-residual-slr">\[\begin{equation}
\hat{\epsilon}_i = Y_i - \hat{Y}_i. \tag{3.4}
\end{equation}\]</span>
The <span class="math inline">\(i\)</span>th residual is the difference between the response and estimated
mean response of observation <span class="math inline">\(i\)</span>.</p>
<p>The <strong>residual sum of squares (RSS)</strong> of a regression model is the sum of its squared residuals. The RSS for a simple linear regression model, as a function of the estimated regression coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, is defined as
<span class="math display" id="eq:def-rss-slr">\[\begin{equation}
RSS(\hat{\beta}_0, \hat{\beta}_1) = \sum_{i=1}^n \hat{\epsilon}_i^2. \tag{3.5}
\end{equation}\]</span></p>
<p>Using the various objects defined above, there are many equivalent expressions for the RSS. Notably, Equation <a href="linear-model-estimation.html#eq:def-rss-slr">(3.5)</a> can be rewritten using Equations <a href="linear-model-estimation.html#eq:def-residual-slr">(3.4)</a> and <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(3.3)</a> as
<span class="math display">\[\begin{align*}
RSS(\hat{\beta}_0, \hat{\beta}_1) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 &amp; \\
&amp;= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
&amp;= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{align*}\]</span></p>
<p>The <strong>fitted model</strong> is the estimated model that minimizes the RSS, i.e., the fitted model (in the context of simple linear regression) is defined as
<span class="math display" id="eq:def-fitted-model-slr">\[\begin{equation}
\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X. \tag{3.6}
\end{equation}\]</span>
In a simple linear regression context, the fitted model is known as the <strong>line of best fit</strong>.</p>
<p>In Figure <a href="linear-model-estimation.html#fig:rss-viz2">3.3</a>, we visualize the response values, fitted values, residuals, and fitted model in a simple linear regression context. Note that:</p>
<ul>
<li>The fitted model is shown as the dashed grey line and minimizes the RSS.</li>
<li>The fitted values, shown as blue x’s, are the values returned by evaluating the fitted
model at the observed regressor values.</li>
<li>The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative.</li>
<li>The RSS is the sum of the squared vertical distances between the
response and fitted values.</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:rss-viz2"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/rss-viz2-1.png" alt="Visualization of the response values, fitted values, residuals, and fitted model." width="672" />
<p class="caption">
Figure 3.3: Visualization of the response values, fitted values, residuals, and fitted model.
</p>
</div>
</div>
<div id="ols-estimators-of-the-simple-linear-regression-parameters" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> OLS estimators of the simple linear regression parameters<a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the RSS for a simple linear regression model can be obtained analytically using basic calculus under minimal assumptions. Specifically, the optimal analytical solutions for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are valid as long as the regressor values are not a constant value, i.e, <span class="math inline">\(x_i \neq x_j\)</span> for at least some <span class="math inline">\(i,j\in \{1,2,\ldots,n\}\)</span>.</p>
<p>Define <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i\)</span> and
<span class="math inline">\(\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i\)</span>. The OLS estimators of the
simple linear regression coefficients are</p>
<p><span class="math display" id="eq:slr-beta1hat">\[\begin{align}
\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n x_i Y_i - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)\biggl(\sum_{i=1}^n Y_i\biggr)}{\sum_{i=1}^n x_i^2 - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)^2} \notag \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \notag \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})Y_i}{\sum_{i=1}^n (x_i - \bar{x})x_i} \tag{3.7}
\end{align}\]</span>
and
<span class="math display" id="eq:slr-beta0hat">\[\begin{equation}
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}. \tag{3.8}
\end{equation}\]</span>
A derivation of the estimators for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> is provided in Section <a href="linear-model-estimation.html#slr-derivation">3.11.1</a>.</p>
<p>We emphasize once again that the OLS estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the estimators that minimize the RSS.</p>
<p>In edition to the regression coefficients, the other parameter we discussed in Section <a href="linear-model-estimation.html#ss-necessary-components">3.2.1</a> is the error
variance, <span class="math inline">\(\sigma^2\)</span>. The most common estimator of the error variance is
<span class="math display" id="eq:sigmasq-hat">\[\begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-p}, \tag{3.9}
\end{equation}\]</span>
where <span class="math inline">\(p\)</span> is the number of estimated regression coefficients. In general, <span class="math inline">\(n-p\)</span> is the degrees of freedom of the RSS. In a simple linear
regression context, the denominator of Equation <a href="linear-model-estimation.html#eq:sigmasq-hat">(3.9)</a> is <span class="math inline">\(n-2\)</span>.</p>
</div>
</div>
<div id="s:penguins-slr" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Penguins simple linear regression example<a href="linear-model-estimation.html#s:penguins-slr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use the <code>penguins</code> data set in the <strong>palmerpenguins</strong> package <span class="citation">(<a href="#ref-R-palmerpenguins" role="doc-biblioref">Horst, Hill, and Gorman 2020</a>)</span> to illustrate a very basic simple linear regression analysis.</p>
<p>The <code>penguins</code> data set provides data related to various penguin species measured in the Palmer Archipelago (Antarctica), originally provided by <span class="citation"><a href="#ref-GormanEtAl2014" role="doc-biblioref">Gorman, Williams, and Fraser</a> (<a href="#ref-GormanEtAl2014" role="doc-biblioref">2014</a>)</span>. We start by loading the data into memory.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="linear-model-estimation.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span></code></pre></div>
<p>The data set includes 344 observations of
8 variables. The variables are:</p>
<ul>
<li><code>species</code>: a <code>factor</code> indicating the penguin species.</li>
<li><code>island</code>: a <code>factor</code> indicating the island the penguin was observed.</li>
<li><code>bill_length_mm</code>: a <code>numeric</code> variable indicating the bill length in millimeters.</li>
<li><code>bill_depth_mm</code>: a <code>numeric</code> variable indicating the bill depth in millimeters.</li>
<li><code>flipper_length_mm</code>: an <code>integer</code> variable indicating the flipper
length in millimeters</li>
<li><code>body_mass_g</code>: an <code>integer</code> variable indicating the body mass in grams.</li>
<li><code>sex</code>: a <code>factor</code> indicating the penguin sex (<code>female</code>, <code>male</code>).</li>
<li><code>year</code>: an integer denoting the study year the penguin was observed (<code>2007</code>, <code>2008</code>, or <code>2009</code>).</li>
</ul>
<p>We begin by creating a scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code> (y-axis versus x-axis) in Figure <a href="linear-model-estimation.html#fig:penguin-plot-2">3.4</a>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="linear-model-estimation.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins,</span>
<span id="cb2-2"><a href="linear-model-estimation.html#cb2-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>,</span>
<span id="cb2-3"><a href="linear-model-estimation.html#cb2-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:penguin-plot-2"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/penguin-plot-2-1.png" alt="A scatter plot of penguin bill length (mm) versus body mass (g)" width="672" />
<p class="caption">
Figure 3.4: A scatter plot of penguin bill length (mm) versus body mass (g)
</p>
</div>
<p>We see a clear positive association between body mass and bill length: as the body mass increases, the bill length tends to increase. The pattern is linear, i.e., roughly a straight line. We will build a simple linear regression model that regresses <code>bill_length_mm</code> on <code>body_mass_g</code>. More specifically, we want to estimate the parameters of the regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1\,X\)</span>, with <span class="math inline">\(Y=\mathtt{bill\_length\_mm}\)</span> and <span class="math inline">\(X=\mathtt{body\_mass\_g}\)</span>, i.e., we want to estimate the parameters of the model
<span class="math display">\[
E(\mathtt{bill\_length\_mm}|\mathtt{body\_mass\_g})=\beta_0+\beta_1\,\mathtt{body\_mass\_g}.
\]</span></p>
<p>The <code>lm</code> function uses OLS to fit a linear model to data. The function has two main arguments:</p>
<ul>
<li><code>data</code>: the data frame in which the model variables are stored. This can be omitted if the variables are already stored in memory.</li>
<li><code>formula</code>: a <span class="citation"><a href="#ref-wilkinsonrogers1973" role="doc-biblioref">Wilkinson and Rogers</a> (<a href="#ref-wilkinsonrogers1973" role="doc-biblioref">1973</a>)</span> style formula describing the linear regression model. For complete details, run <code>?stats:formula</code> in the Console. For a basic understanding of <code>formula</code>, assume <code>y</code> is the response variable and <code>x</code>, <code>x1</code>, <code>x2</code>, <code>x3</code> are available numeric predictors. Then:
<ul>
<li><code>y ~ x</code> describes the simple linear regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1 X\)</span>.</li>
<li><code>y ~ x1 + x2</code> describes the multiple linear regression model <span class="math inline">\(E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><code>y ~ x1 + x2 + x1:x2</code> and <code>y ~ x1 * x2</code> describe the multiple linear regression model
<span class="math inline">\(E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</li>
<li><code>y ~ -1 + x1 + x2</code> describe a multiple linear regression model without an intercept, in this case,
<span class="math inline">\(E(Y|X_1, X_2)=\beta_1 X_1 + \beta_2 X_2\)</span>. The <code>-1</code> tells R not to include an intercept in the fitted model.</li>
<li><code>y ~ x + I(x^2)</code> describe the multiple linear regression model <span class="math inline">\(E(Y|X)=\beta_0+\beta_1 X + \beta_2 X^2\)</span>. The <code>I()</code> function is a special function that tells R to create a regressor based on the syntax inside the <code>()</code> and include that regressor in the model.</li>
</ul></li>
</ul>
<p>We use the code below to fit a linear model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> using the <code>penguins</code> data frame and assign the result the name <code>lmod</code>. <code>lmod</code> is an object of class <code>lm</code>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="linear-model-estimation.html#cb3-1" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins) <span class="co"># fit model</span></span>
<span id="cb3-2"><a href="linear-model-estimation.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(lmod) <span class="co"># class of lmod</span></span>
<span id="cb3-3"><a href="linear-model-estimation.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] &quot;lm&quot;</span></span></code></pre></div>
<p>The <code>summary</code> function is commonly used to summarize the results of our fitted model. When an <code>lm</code> object is supplied to the <code>summary</code> function, it returns:</p>
<ul>
<li><code>Call</code>: the function call used to fit the model.</li>
<li><code>Residuals</code>: A 5-number summary of the <span class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span>.</li>
<li><code>Coefficients</code>: A table that lists:
<ul>
<li>The regressors in the fitted model.</li>
<li><code>Estimate</code>: the estimated coefficient for each regressor.</li>
<li><code>Std. Error</code>: the <em>estimated</em> standard error of the estimated coefficients.</li>
<li><code>t value</code>: the computed test statistic associated with testing <span class="math inline">\(H_0: \beta_j = 0\)</span> versus <span class="math inline">\(H_a: \beta_j \neq 0\)</span> for <span class="math inline">\(j=0,1,\ldots,p-1\)</span>.</li>
<li><code>Pr(&gt;|t|)</code>: the associated p-value of each test.</li>
</ul></li>
<li>Various summary statistics:
<ul>
<li><code>Residual standard error</code> is the value of <span class="math inline">\(\hat{\sigma}\)</span>, the estimate of the error standard deviation. The degrees of freedom is <code>n-p</code>, i.e., the number of observations used in the model fit minus the number of estimated regression coefficients.</li>
<li><code>Multiple R-squared</code> is an estimate of model fit discussed in Section <a href="linear-model-estimation.html#evaluating-model-fit">3.9</a>.</li>
<li><code>Adjusted R-squared</code> is a modified version of <code>Multiple R-squared</code>.</li>
<li><code>F-statistic</code> is the test statistic for the test that compares the model with an only an intercept to the fitted model. The <code>DF</code> (degrees of freedom) values relate to the statistic under the null hypothesis, and the <code>p-value</code> is the p-value for the test.</li>
</ul></li>
</ul>
<p>We use the <code>summary</code> function on <code>lmod</code> to produce the output below.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="linear-model-estimation.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize results stored in lmod</span></span>
<span id="cb4-2"><a href="linear-model-estimation.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod)</span>
<span id="cb4-3"><a href="linear-model-estimation.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb4-4"><a href="linear-model-estimation.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb4-5"><a href="linear-model-estimation.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lm(formula = bill_length_mm ~ body_mass_g, data = penguins)</span></span>
<span id="cb4-6"><a href="linear-model-estimation.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb4-7"><a href="linear-model-estimation.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb4-8"><a href="linear-model-estimation.html#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="do">##      Min       1Q   Median       3Q      Max </span></span>
<span id="cb4-9"><a href="linear-model-estimation.html#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="do">## -10.1251  -3.0434  -0.8089   2.0711  16.1109 </span></span>
<span id="cb4-10"><a href="linear-model-estimation.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb4-11"><a href="linear-model-estimation.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb4-12"><a href="linear-model-estimation.html#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb4-13"><a href="linear-model-estimation.html#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept) 2.690e+01  1.269e+00   21.19   &lt;2e-16 ***</span></span>
<span id="cb4-14"><a href="linear-model-estimation.html#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="do">## body_mass_g 4.051e-03  2.967e-04   13.65   &lt;2e-16 ***</span></span>
<span id="cb4-15"><a href="linear-model-estimation.html#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb4-16"><a href="linear-model-estimation.html#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb4-17"><a href="linear-model-estimation.html#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb4-18"><a href="linear-model-estimation.html#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Residual standard error: 4.394 on 340 degrees of freedom</span></span>
<span id="cb4-19"><a href="linear-model-estimation.html#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="do">##   (2 observations deleted due to missingness)</span></span>
<span id="cb4-20"><a href="linear-model-estimation.html#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.3542, Adjusted R-squared:  0.3523 </span></span>
<span id="cb4-21"><a href="linear-model-estimation.html#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="do">## F-statistic: 186.4 on 1 and 340 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Using the output above, we see that the estimated parameters are <span class="math inline">\(\hat{\beta}_0=26.9\)</span> and <span class="math inline">\(\hat{\beta}_1=0.004\)</span>. Thus, our fitted model is
<span class="math display">\[
\widehat{\mathtt{bill\_length\_mm}}=26.9+0.004 \,\mathtt{body\_mass\_g}.
\]</span></p>
<p>In the context of a simple linear regression model, the intercerpt term is the expected response when the value of the regressor is zero, while the slope is the expected change in the response when the regressor increases by 1 unit. Thus, based on the model we fit to the <code>penguin</code> data, we can make the following interpretations:</p>
<ul>
<li><span class="math inline">\(\hat{\beta}_1\)</span>: If a penguin has a body mass 1 gram larger than another penguin, we expect the larger penguin’s bill length to be 0.004 millimeters longer.</li>
<li><span class="math inline">\(\hat{\beta}_0\)</span>: A penguin with a body mass of 0 grams is expected to have a bill length of 26.9 millimeters.</li>
</ul>
<p>The latter interpretation is clearly nonsensical and is caused by the fact that we are extrapolating far outside the observed body mass values. The relationship between body mass and bill length is different for penguin chicks versus adults.</p>
<p>The <code>abline</code> function can be used to automatically overlay the fitted model on the observed data. We run the code below to produce Figure <a href="linear-model-estimation.html#fig:slr-penguin-fit">3.5</a>. The fit of the model to our observed data seems reasonable.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="linear-model-estimation.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins,</span>
<span id="cb5-2"><a href="linear-model-estimation.html#cb5-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>,</span>
<span id="cb5-3"><a href="linear-model-estimation.html#cb5-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>)</span>
<span id="cb5-4"><a href="linear-model-estimation.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># draw fitted line of plot</span></span>
<span id="cb5-5"><a href="linear-model-estimation.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lmod)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:slr-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/slr-penguin-fit-1.png" alt="The fitted model overlaid on the penguin data." width="672" />
<p class="caption">
Figure 3.5: The fitted model overlaid on the penguin data.
</p>
</div>
<p>R provides many additional methods (generic functions that do something specific when applied to a certain type of object) for <code>lm</code> objects. Commonly used ones include:</p>
<ul>
<li><code>residuals</code>: extracts <span class="math inline">\(\hat{\boldsymbol{\epsilon}}\)</span> from an <code>lm</code> object.</li>
<li><code>fitted</code>: extracts <span class="math inline">\(\hat{\mathbf{y}}\)</span> from an <code>lm</code> object.</li>
<li><code>predict</code>: by default, computes <span class="math inline">\(\hat{\mathbf{y}}\)</span> from an <code>lm</code> object. It can also be used to make arbitrary predictions for the <code>lm</code> object.</li>
<li><code>coef</code> or <code>coefficients</code>: extracts <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> from an <code>lm</code> object.</li>
<li><code>deviance</code>: extracts the RSS from an <code>lm</code> object.</li>
<li><code>df.residual</code>: extracts <span class="math inline">\(n-p\)</span>, the degrees of freedom for the RSS, from an <code>lm</code> object.</li>
<li><code>sigma</code>: extracts <span class="math inline">\(\hat{\sigma}\)</span> from an <code>lm</code> object.</li>
</ul>
<p>We now use some of the methods to extract important characteristics of our fitted model.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="linear-model-estimation.html#cb6-1" aria-hidden="true" tabindex="-1"></a>(coeffs <span class="ot">&lt;-</span> <span class="fu">coef</span>(lmod)) <span class="co"># extract, assign, and print coefficients</span></span>
<span id="cb6-2"><a href="linear-model-estimation.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">##  (Intercept)  body_mass_g </span></span>
<span id="cb6-3"><a href="linear-model-estimation.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## 26.898872424  0.004051417</span></span>
<span id="cb6-4"><a href="linear-model-estimation.html#cb6-4" aria-hidden="true" tabindex="-1"></a>ehat <span class="ot">&lt;-</span> <span class="fu">residuals</span>(lmod) <span class="co"># extract and assign residuals</span></span>
<span id="cb6-5"><a href="linear-model-estimation.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ehat) <span class="co"># first few residuals</span></span>
<span id="cb6-6"><a href="linear-model-estimation.html#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="do">##          1          2          3          5          6          7 </span></span>
<span id="cb6-7"><a href="linear-model-estimation.html#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="do">## -2.9916846 -2.7942554  0.2340237 -4.1762596 -2.3865430 -2.6852575</span></span>
<span id="cb6-8"><a href="linear-model-estimation.html#cb6-8" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(lmod) <span class="co"># extract and assign fitted values</span></span>
<span id="cb6-9"><a href="linear-model-estimation.html#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(yhat) <span class="co"># first few fitted values</span></span>
<span id="cb6-10"><a href="linear-model-estimation.html#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="do">##        1        2        3        5        6        7 </span></span>
<span id="cb6-11"><a href="linear-model-estimation.html#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 42.09168 42.29426 40.06598 40.87626 41.68654 41.58526</span></span>
<span id="cb6-12"><a href="linear-model-estimation.html#cb6-12" aria-hidden="true" tabindex="-1"></a>yhat2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(lmod) <span class="co"># compute and assign fitted values</span></span>
<span id="cb6-13"><a href="linear-model-estimation.html#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(yhat2) <span class="co"># first few fitted values</span></span>
<span id="cb6-14"><a href="linear-model-estimation.html#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="do">##        1        2        3        5        6        7 </span></span>
<span id="cb6-15"><a href="linear-model-estimation.html#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 42.09168 42.29426 40.06598 40.87626 41.68654 41.58526</span></span>
<span id="cb6-16"><a href="linear-model-estimation.html#cb6-16" aria-hidden="true" tabindex="-1"></a>(rss <span class="ot">&lt;-</span> <span class="fu">deviance</span>(lmod)) <span class="co"># extract, assign, and print rss</span></span>
<span id="cb6-17"><a href="linear-model-estimation.html#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6564.494</span></span>
<span id="cb6-18"><a href="linear-model-estimation.html#cb6-18" aria-hidden="true" tabindex="-1"></a>(dfr <span class="ot">&lt;-</span> <span class="fu">df.residual</span>(lmod)) <span class="co"># extract n - p</span></span>
<span id="cb6-19"><a href="linear-model-estimation.html#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 340</span></span>
<span id="cb6-20"><a href="linear-model-estimation.html#cb6-20" aria-hidden="true" tabindex="-1"></a>(sigmasqhat <span class="ot">&lt;-</span> <span class="fu">sigma</span>(lmod)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># estimated error variance</span></span>
<span id="cb6-21"><a href="linear-model-estimation.html#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 19.30734</span></span></code></pre></div>
<p>From the output above, we that the the first 3 residuals are -2.99, -2.79, and 0.23. The first 3 fitted values are 42.09, 42.29, and 40.07. The RSS for the fitted model is 6564.49 with 340 degrees of freedom. The estimated error variance, <span class="math inline">\(\hat{\sigma}^2\)</span>, is 19.31.</p>
<p>We use the <code>methods</code> function to obtain a full list of methods available for <code>lm</code> objects using the code below.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="linear-model-estimation.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">methods</span>(<span class="at">class =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb7-2"><a href="linear-model-estimation.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] add1           alias          anova          case.names     coerce        </span></span>
<span id="cb7-3"><a href="linear-model-estimation.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  [6] confint        cooks.distance deviance       dfbeta         dfbetas       </span></span>
<span id="cb7-4"><a href="linear-model-estimation.html#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [11] drop1          dummy.coef     effects        extractAIC     family        </span></span>
<span id="cb7-5"><a href="linear-model-estimation.html#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [16] formula        hatvalues      influence      initialize     kappa         </span></span>
<span id="cb7-6"><a href="linear-model-estimation.html#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [21] labels         logLik         model.frame    model.matrix   nobs          </span></span>
<span id="cb7-7"><a href="linear-model-estimation.html#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [26] plot           predict        print          proj           qr            </span></span>
<span id="cb7-8"><a href="linear-model-estimation.html#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [31] residuals      rstandard      rstudent       show           simulate      </span></span>
<span id="cb7-9"><a href="linear-model-estimation.html#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [36] slotsFromS3    summary        variable.names vcov          </span></span>
<span id="cb7-10"><a href="linear-model-estimation.html#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="do">## see &#39;?methods&#39; for accessing help and source code</span></span></code></pre></div>
</div>
<div id="estimation-of-the-multiple-linear-regression-model" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Estimation of the multiple linear regression model<a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now consider the context where we want to estimate the parameters of a linear model with 1 or more regressors, i.e., when we use the model
<span class="math display">\[
Y=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1} + \epsilon.
\]</span></p>
<p>The multiple linear regression model relating the responses, the regressors, and the errors for all <span class="math inline">\(n\)</span> observations is defined by the system of equations
<span class="math display" id="eq:lmSystem">\[\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
\tag{3.10}
\end{equation}\]</span></p>
<div id="using-matrix-notation-to-represent-a-linear-model" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Using matrix notation to represent a linear model<a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can simplify the linear model described in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.10)</a> using matrix notation. It may be useful to refer to Appendix <a href="overview-of-matrix-facts.html#overview-of-matrix-facts">A</a> for a brief overview of matrix-related information.</p>
<p>We use the following notation:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]\)</span> denotes the column vector containing the <span class="math inline">\(n\)</span> responses.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> denotes the matrix containing a column of 1s and the observed regressor values for <span class="math inline">\(X_1, X_2, \ldots, X_{p-1}\)</span>, specifically,
<span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p-1} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p-1}
\end{bmatrix}.\]</span></li>
<li><span class="math inline">\(\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]\)</span> denotes the column vector containing the <span class="math inline">\(p\)</span> regression coefficients.</li>
<li><span class="math inline">\(\boldsymbol{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]\)</span> denotes the column vector contained the <span class="math inline">\(n\)</span> errors.</li>
</ul>
<p>Then the system of equations defining the linear model in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.10)</a> can be written as
<span class="math display">\[
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.
\]</span>
Thus, matrix notation can be used to represent a system of linear equations. A model that cannot be represented as a system of linear equations using matrices is not a linear model.</p>
</div>
<div id="ss:fv-resid-rss-mlr" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Residuals, fitted values, and RSS for multiple linear regression<a href="linear-model-estimation.html#ss:fv-resid-rss-mlr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now discuss of residuals, fitted values, and RSS for the multiple linear regression context using matrix notation.</p>
<p>The vector of estimated values for the coefficients contained in <span class="math inline">\(\boldsymbol{\beta}\)</span> is denoted
<span class="math display" id="eq:def-beta-matrix">\[\begin{equation}
\hat{\boldsymbol{\beta}}=[\hat{\beta}_0,\hat{\beta}_1,\ldots,\hat{\beta}_{p-1}]. \tag{3.11}
\end{equation}\]</span></p>
<p>The vector of regressor values for the <span class="math inline">\(i\)</span>th observation is denoted
<span class="math display" id="eq:def-ith-regressor-matrix">\[\begin{equation}
\mathbf{x}_i=[1,x_{i,1},\ldots,x_{i,p-1}], \tag{3.12}
\end{equation}\]</span>
where the 1 is needed to account for the intercept in our model.</p>
<p>Extending the original definition of a fitted value in Equation <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(3.3)</a>, the <span class="math inline">\(i\)</span>th <strong>fitted value</strong> in the context of multiple linear regression is defined as
<span class="math display" id="eq:def-fitted-value-matrix">\[\begin{align}
\hat{Y}_i &amp;= \hat{E}(Y|\mathbb{X} = \mathbf{x}_i) \notag \\
&amp;= \hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \cdots + \hat{\beta}_{p-1} x_{i,p-1} \notag \\
&amp;= \mathbf{x}_i^T\hat{\boldsymbol{\beta}}. \tag{3.13}
\end{align}\]</span></p>
<p>The column vector of fitted values is defined as
<span class="math display" id="eq:def-fitted-values-matrix">\[\begin{equation}
\hat{\mathbf{y}} = [\hat{Y}_1,\ldots,\hat{Y}_n] = \mathbf{X}\hat{\boldsymbol{\beta}}. \tag{3.14}
\end{equation}\]</span></p>
<p>Extending the original definition of a residual in Equation <a href="linear-model-estimation.html#eq:def-residual-slr">(3.4)</a>,
the <span class="math inline">\(i\)</span>th <strong>residual</strong> in the context of multiple linear regression can be written as
<span class="math display">\[\begin{align}
\hat{\epsilon}_i = Y_i - \hat{Y}_i=Y_i-\mathbf{x}_i^T\hat{\boldsymbol{\beta}},
\end{align}\]</span>
using Equation <a href="linear-model-estimation.html#eq:def-fitted-value-matrix">(3.13)</a>.</p>
<p>The column vector of residuals is defined as
<span class="math display" id="eq:def-residuals-matrix">\[\begin{equation}
\hat{\epsilon} = [\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n]. \tag{3.15}
\end{equation}\]</span>
Using Equation <a href="linear-model-estimation.html#eq:def-fitted-values-matrix">(3.14)</a>, equivalent expressions for the residual vector are
<span class="math display">\[
\hat{\boldsymbol{\epsilon}}=\mathbf{y}-\hat{\mathbf{y}}=\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}.
\]</span></p>
<p>The RSS for a multiple linear regression model, as a function of the estimated regression coefficients, is
<span class="math display" id="eq:def-rss-matrix">\[\begin{equation}
\begin{aligned}
RSS(\hat{\boldsymbol{\beta}}) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \hat{\boldsymbol{\epsilon}}^T \hat{\boldsymbol{\epsilon}} \\
&amp;= (\mathbf{y} - \hat{\mathbf{y}})^T (\mathbf{y} - \hat{\mathbf{y}}) \\
&amp; = (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T (\mathbf{y} - \hat{\boldsymbol{\beta}}).
\end{aligned} \tag{3.16}
\end{equation}\]</span>
The various expressions in Equation <a href="linear-model-estimation.html#eq:def-rss-matrix">(3.16)</a> are equivalent.</p>
</div>
<div id="ols-estimator-of-the-regression-coefficients" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> OLS estimator of the regression coefficients<a href="linear-model-estimation.html#ols-estimator-of-the-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The OLS estimator of the regression coefficient vector, <span class="math inline">\(\boldsymbol{\beta}\)</span>, is
<span class="math display" id="eq:betahat">\[\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}. \tag{3.17}
\end{equation}\]</span>
A derivation of the estimator for <span class="math inline">\(\beta\)</span> is provided in Section <a href="linear-model-estimation.html#mlr-derivation">3.11.3</a>.</p>
</div>
</div>
<div id="s:penguins-mlr" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Penguins multiple linear regression example<a href="linear-model-estimation.html#s:penguins-mlr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We continue our analysis of the <code>penguins</code> data introduced in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>. We will fit a multiple linear regression model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>.
In the code below, we fit this multiple linear regression model to the data and then use the <code>coef</code> and <code>deviance</code> functions to extract the estimated coefficients and RSS of the fitted model, respectively.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="linear-model-estimation.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb8-2"><a href="linear-model-estimation.html#cb8-2" aria-hidden="true" tabindex="-1"></a>mlmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> flipper_length_mm,</span>
<span id="cb8-3"><a href="linear-model-estimation.html#cb8-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> penguins)</span>
<span id="cb8-4"><a href="linear-model-estimation.html#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb8-5"><a href="linear-model-estimation.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mlmod)</span>
<span id="cb8-6"><a href="linear-model-estimation.html#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="do">##       (Intercept)       body_mass_g flipper_length_mm </span></span>
<span id="cb8-7"><a href="linear-model-estimation.html#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     -3.4366939266      0.0006622186      0.2218654584</span></span>
<span id="cb8-8"><a href="linear-model-estimation.html#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># extract RSS</span></span>
<span id="cb8-9"><a href="linear-model-estimation.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(mlmod)</span>
<span id="cb8-10"><a href="linear-model-estimation.html#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 5764.585</span></span></code></pre></div>
<p>The fitted model is
<span class="math display">\[
\widehat{\mathtt{bill\_length\_mm}}=-3.44+0.0007 \,\mathtt{body\_mass\_g}+0.22\,\mathtt{flipper\_length\_mm}.
\]</span> We discuss basic interpretation of multiple linear regression model in Chapter @ref(interpreting-a-fitted linear-model).</p>
<p>The estimated error variance for the fitted model is 5764.59, which is substantially less then the simple linear regression model in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>.</p>
<p>It is trivial to add additional numeric regressors to our linear regression model using the <code>lm</code> function. But what if we have a categorical predictor? The next section discusses how to transform a categorical predictors into 1 or more numeric regressors that can be included in our linear model.</p>
</div>
<div id="categorical-predictors" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Categorical predictors<a href="linear-model-estimation.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the variables. Categorical predictors can be used in many ways, but we will only discuss basic usage of categorical predictors in this section. We will briefly introduce the:</p>
<ul>
<li><strong>parallel line regression model</strong>, which is a regression model that produces parallel lines for each level of the categorical variable.</li>
<li><strong>separate lines regression model</strong>, which is a regression model that produces separate lines for each level of a categorical variable.
Chapter <a href="more-on-categorical-predictors.html#more-on-categorical-predictors">13</a> provides more information about using categorical predictors in a regression model.</li>
</ul>
<div id="indicator-variables" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Indicator variables<a href="linear-model-estimation.html#indicator-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to compute <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using Equation <a href="linear-model-estimation.html#eq:betahat">(3.17)</a>, both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> must contain numeric values. How can we use a categorical predictor in our regression model when its values are not numeric? To do so, we must transform the categorical predictor into one or more <strong>indicator</strong> or <strong>dummy variables</strong>, which we explain in more detail below.</p>
<p>An <strong>indicator function</strong> is a function that takes the value 1 if a certain property is true and 0 otherwise. An <strong>indicator variable</strong> is the variable that results from applying an indicator function to each observation of a variable. Many notations exist for indicator functions. We use the notation
<span class="math display">\[\begin{equation*}
I_S(x) = \begin{cases}
1 &amp; \textrm{if}\;x \in S\\
0 &amp; \textrm{if}\;x \notin S
\end{cases},
\end{equation*}\]</span>
which is shorthand for a function that returns 1 if <span class="math inline">\(x\)</span> is in the set <span class="math inline">\(S\)</span> and 0 otherwise.</p>
<p>Let <span class="math inline">\(C\)</span> denote a categorical predictor with levels <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span>. The <span class="math inline">\(C\)</span> stands for “categorical,” while the <span class="math inline">\(L\)</span> stands for “level.” Let <span class="math inline">\(c_i\)</span> denote the value of <span class="math inline">\(C\)</span> for observation <span class="math inline">\(i\)</span>.</p>
<p>Let <span class="math inline">\(D_j\)</span> denote the indicator (dummy) variable for factor level <span class="math inline">\(L_j\)</span> of <span class="math inline">\(C\)</span>. The value of <span class="math inline">\(D_j\)</span> for observation <span class="math inline">\(i\)</span> is denoted <span class="math inline">\(d_{i,j}\)</span>, with
<span class="math display">\[
d_{i,j} = I_{\{L_j\}}(c_i),
\]</span>
i.e., <span class="math inline">\(d_{i,j}\)</span> is 1 if <span class="math inline">\(c_i\)</span> has factor level <span class="math inline">\(L_j\)</span> and 0 otherwise.</p>
</div>
<div id="parallel-and-separate-lines-models" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Parallel and separate lines models<a href="linear-model-estimation.html#parallel-and-separate-lines-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assume we want to build a linear regression model using a single numeric regressor <span class="math inline">\(X\)</span> and a two-level categorical predictor <span class="math inline">\(C\)</span>.</p>
<p>The standard simple linear regression model is
<span class="math display">\[E(Y\mid X)=\beta_0 + \beta_1 X.\]</span></p>
<p>To create a parallel lines regression model, we add regressor <span class="math inline">\(D_2\)</span> to the simple linear regression model. Thus, the parallel lines regression model is
<span class="math display" id="eq:parallel-lines-model">\[\begin{equation}
E(Y\mid X,C)=\beta_{0}+\beta_1 X+\beta_2 D_2. \tag{3.18}
\end{equation}\]</span>
Since <span class="math inline">\(D_2=0\)</span> when <span class="math inline">\(C=L_1\)</span> and <span class="math inline">\(D_2=1\)</span> when <span class="math inline">\(C=L_2\)</span>, we see that the model in Equation <a href="linear-model-estimation.html#eq:parallel-lines-model">(3.18)</a> simplifies to
<span class="math display">\[
E(Y\mid X, C) =
\begin{cases}
  \beta_0+\beta_1 X &amp; \mathrm{if}\;C = L_1 \\
  (\beta_0 + \beta_2) +\beta_X X &amp; \mathrm{if}\;C = L_2
\end{cases}.
\]</span></p>
<p>To create a separate lines regression model, we add regressors <span class="math inline">\(D_2\)</span> and <span class="math inline">\(D_2 X\)</span> to our simple linear regression model. Thus, the separate lines regression model is
<span class="math display">\[
E(Y\mid X,C)=\beta_0+\beta_1 X+\beta_2 D_2 + \beta_{3} (D_2X),
\]</span>
which, similar to the previous model, simplifies to
<span class="math display">\[
E(Y\mid X, C) =
\begin{cases}
  \beta_{int}+\beta_X X &amp; \mathrm{if}\;C = L_1 \\
  (\beta_{0} + \beta_{2}) +(\beta_1 + \beta_{3}) X &amp; \mathrm{if}\;C = L_2
\end{cases}.
\]</span></p>
<p>We provides examples of the parallel lines and separate lines regression models in the next section.</p>
</div>
<div id="extensions" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Extensions<a href="linear-model-estimation.html#extensions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have presented the most basic regression models that include a categorical predictor. If you had a categorical predictor <span class="math inline">\(C\)</span> with <span class="math inline">\(K\)</span> levels <span class="math inline">\(L_1, L_2, \ldots, L_K\)</span>, then you could add indicator variables <span class="math inline">\(D_2, D_3, \ldots, L_K\)</span> to a simple linear regression model to create a parallel lines model for each level of <span class="math inline">\(C\)</span>. Similarly, you could add regressors <span class="math inline">\(D_2, D_3, \ldots, D_K, D_2 X, D_3 X, \ldots, D_K X\)</span> to a simple linear regression model to create a separate lines model for each level of <span class="math inline">\(C\)</span>.</p>
<p>It is easy to imagine using multiple categorical predictors in a model, interacting one or more categorical predictors with one or more numeric regressors in model, etc. These models can be fit easily using R (as we’ll see below), but you must carefully keep track of reference levels to interpret the results.</p>
</div>
<div id="avoiding-an-easy-mistake" class="section level3 hasAnchor" number="3.7.4">
<h3><span class="header-section-number">3.7.4</span> Avoiding an easy mistake<a href="linear-model-estimation.html#avoiding-an-easy-mistake" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Should you also consider adding <span class="math inline">\(D_1\)</span> to your parallel lines model? Or <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_1 X\)</span> to your separate lines model? No, you should not, because you will create linear dependencies in the columns of the regressor matrix <span class="math inline">\(\mathbf{X}\)</span>. Let <span class="math inline">\(\mathbf{d}_1=[d_{1,1}, d_{2,1}, \ldots, d_{n,1}]\)</span> be the column vector of observed values for indicator variable <span class="math inline">\(D_1\)</span> and <span class="math inline">\(\mathbf{d}_2\)</span> be the column vector for <span class="math inline">\(D_2\)</span>. Then for a two-level categorical variable, <span class="math inline">\(\mathbf{d}_1 + \mathbf{d}_2\)</span> is an <span class="math inline">\(n\times 1\)</span> vector of 1s, meaning that <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> will be linearly dependent with the intercept column of our <span class="math inline">\(\mathbf{X}\)</span> matrix. Thus, adding <span class="math inline">\(D_1\)</span> to parallel lines model would result in <span class="math inline">\(\mathbf{X}\)</span> having linearly dependent columns, which creates estimation problems. To solve this problem, we use all but 1 of the levels of our categorical predictor to create indicator variables to use in our model. The level without an indicator variable in the regression model is known as the <strong>reference level</strong>, which will be explained in Chapter <a href="interpreting-a-fitted-linear-model.html#interpreting-a-fitted-linear-model">4</a>.</p>
<!-- , D_2, \ldots, D_K, X\,D_2, X\,D_3,\ldots, X\,D_K$ -->
<!-- For simplicity, assume that categorical predictor $F$ has only two levels, $L_1$ and $L_2$. It is common to use notation like $E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 +\beta_2 X_2$ when discussing linear regression models. That notation is generally simple and convenient, but can be unclear. Asking a researcher what the estimate of $\beta_2$ is in a model is ambiguous because it will depend on the order the researcher added the variables to the model. To more closely connect each coefficient with the regressor to which it is related, we will use the notation $\beta_X$ to denote the coefficient for regressor $X$ and $\beta_{D_j}$ to denote the coefficient for regressor $D_j$. Similarly, $\beta_{int}$ denotes the intercept included in our model. -->
</div>
</div>
<div id="s:penguins-mlr2" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Penguins example with categorical predictor<a href="linear-model-estimation.html#s:penguins-mlr2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We return once again to the <code>penguins</code> data previously introduced. We use the code below to produce Figure <a href="linear-model-estimation.html#fig:penguins-grouped-scatter">3.6</a>, which displays the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code> that distinguishes the <code>species</code> of each observation. It is very clear that the relationship between bill length and body mass changes depending on the species of penguin.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="linear-model-estimation.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb9-2"><a href="linear-model-estimation.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span></span>
<span id="cb9-3"><a href="linear-model-estimation.html#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm, <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb9-4"><a href="linear-model-estimation.html#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>)</span>
<span id="cb9-5"><a href="linear-model-estimation.html#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: Removed 2 rows containing missing values (geom_point).</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:penguins-grouped-scatter"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/penguins-grouped-scatter-1.png" alt="A grouped scatter plot of body mass versus bill length that distinguishes penguin species." width="672" />
<p class="caption">
Figure 3.6: A grouped scatter plot of body mass versus bill length that distinguishes penguin species.
</p>
</div>
<p>Recall that you should represent your categorical variables is a <code>factor</code> in R.</p>
<p>Helpfully, the <code>lm</code> function will automatically convert a <code>factor</code> variable to the correct number of indicator variables when you include the <code>factor</code> variable in your <code>formula</code> argument. E.g., if <code>c</code> is a <code>factor</code> variable and <code>x</code> is a <code>numeric</code> variable, you can use the notation <code>c:x</code> in your <code>formula</code> to get all the interactions between <code>c</code> and <code>x</code>. R will automatically choose the reference level to be the first level of the <code>factor</code> variable.</p>
<p>In our present context, the categorical predictor is <code>species</code>, which has the levels <code>Adelie</code>, <code>Chinstrap</code>, and <code>Gentoo</code>. The <code>species</code> variable is already a <code>factor</code>. Since the variable has 3 levels, it will be transformed into 2 indicator variables by R. The first level of species is <code>Adelie</code>, so R will treat that level as the reference level, and automatically create indicator variables for the levels <code>Chinstrap</code> and <code>Gentoo</code>. (Reminder: to determine the level order of a <code>factor</code> variable <code>c</code>, run the commend <code>levels(c)</code>, or in this case <code>levels(penguins$species)</code>.)</p>
<p>Let <span class="math inline">\(D_C\)</span> denote the indicator variable for the <code>Chinstrap</code> level and <span class="math inline">\(D_G\)</span> denote the indicator variable for the <code>Gentoo</code> level. To fit the parallel lines regression model
<span class="math display">\[E(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) = \beta_{0} + \beta_1 \mathtt{body\_mass\_g} + \beta_2 D_C + \beta_3 D_G,\]</span>
we run the code below. The <code>coef</code> function is used to extract the estimated coefficients from our fitted model in <code>lmodp</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="linear-model-estimation.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit parallel lines model</span></span>
<span id="cb10-2"><a href="linear-model-estimation.html#cb10-2" aria-hidden="true" tabindex="-1"></a>lmodp <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species, <span class="at">data =</span> penguins)</span>
<span id="cb10-3"><a href="linear-model-estimation.html#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract coefficients</span></span>
<span id="cb10-4"><a href="linear-model-estimation.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmodp)</span>
<span id="cb10-5"><a href="linear-model-estimation.html#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="do">##      (Intercept)      body_mass_g speciesChinstrap    speciesGentoo </span></span>
<span id="cb10-6"><a href="linear-model-estimation.html#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="do">##     24.919470977      0.003748497      9.920884113      3.557977539</span></span></code></pre></div>
<p>Thus, the fitted parallel lines model is
<span class="math display" id="eq:pl-model-penguins">\[\begin{equation}
\widehat{\mathtt{bill\_length\_mm}} = 24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 D_C + 3.56 D_G. \tag{3.19}
\end{equation}\]</span>
Note that <code>speciesChinstrap</code> and <code>speciesGentoo</code> are the indicator variables related to the <code>Chinstrap</code> and <code>Gentoo</code> levels of <code>species</code>, respectively. When an observation has <code>species</code> level <code>Adelie</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.19)</a> simplifies to
<span class="math display">\[\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 0 + 3.56 \cdot 0 \\
&amp;= 24.92 + 0.004 \mathtt{body\_mass\_g}.
\end{align}\]</span>
When an observation has <code>species</code> level <code>Chinstrap</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.19)</a> simplifies to
<span class="math display">\[\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 1 + 3.56 \cdot 0 \\
&amp;= 34.84 + 0.004 \mathtt{body\_mass\_g}.
\end{align}\]</span>
Lastly, when an observation has <code>species</code> level <code>Gentoo</code>, then Equation <a href="linear-model-estimation.html#eq:pl-model-penguins">(3.19)</a> simplifies to
<span class="math display">\[\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Gentoo}) \\
&amp;=24.92 + 0.004 \mathtt{body\_mass\_g} + 9.92 \cdot 0 + 3.56 \cdot 1 \\
&amp;= 28.48 + 0.004 \mathtt{body\_mass\_g}.
\end{align}\]</span></p>
<p>Adding fitted lines for each <code>species</code> level to the scatter plot in Figure <a href="linear-model-estimation.html#fig:penguins-grouped-scatter">3.6</a> is a bit more difficult than before. One technique is to use <code>predict</code> to get the fitted values of each observation, use the <code>transform</code> function to add those values as a column to the original the data frame, then use <code>geom_line</code> to connect the fitted values from each group.</p>
<p>We start by adding our fitted values to the <code>penguins</code> data frame. We use the <code>predict</code> function to obtained the fitted values of our fitted model and then use the <code>transform</code> function to add those values as the <code>pl_fitted</code> variable in the <code>penguins</code> data frame.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="linear-model-estimation.html#cb11-1" aria-hidden="true" tabindex="-1"></a>penguins <span class="ot">&lt;-</span> </span>
<span id="cb11-2"><a href="linear-model-estimation.html#cb11-2" aria-hidden="true" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb11-3"><a href="linear-model-estimation.html#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">pl_fitted =</span> <span class="fu">predict</span>(lmodp))</span>
<span id="cb11-4"><a href="linear-model-estimation.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Error in data.frame(structure(list(species = structure(c(1L, 1L, 1L, 1L, : arguments imply differing number of rows: 344, 342</span></span></code></pre></div>
<p>We just received a nasty error. What is going on? The original <code>penguins</code> data frame has 344 rows. However, two rows had <code>NA</code> observations such that when we used the <code>lm</code> function to fit our parallel lines model, those observations were removed prior to fitting. The <code>predict</code> function produces fitted values for the observations used in the fitting process, so there are only 342 predicted values. There is a mismatch between the number of rows in <code>penguins</code> and the number of values we attempt to add in the new column <code>pl_fitted</code>, so we get an error.</p>
<p>To handle this error, we refit our model while setting the <code>na.action</code> argument to <code>na.exclude</code>. As stated Details section of the documention for the <code>lm</code> function (run <code>?lm</code> in the Console):</p>
<blockquote>
<p><span class="math inline">\(\ldots\)</span> when <code>na.exclude</code> is used the residuals and predictions are padded to the correct length by inserting <code>NA</code>s for cases omitted by <code>na.exclude</code>.</p>
</blockquote>
<p>We refit the parallel lines model below with <code>na.action = na.exclude</code>, then use the <code>predict</code> function to add the fitted values to the <code>penguins</code> data frame via the <code>transform</code> function.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="linear-model-estimation.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># refit parallel lines model with new na.action behavior</span></span>
<span id="cb12-2"><a href="linear-model-estimation.html#cb12-2" aria-hidden="true" tabindex="-1"></a>lmodp <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species,</span>
<span id="cb12-3"><a href="linear-model-estimation.html#cb12-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> penguins, <span class="at">na.action =</span> na.exclude)</span>
<span id="cb12-4"><a href="linear-model-estimation.html#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># add fitted values to penguins data frame</span></span>
<span id="cb12-5"><a href="linear-model-estimation.html#cb12-5" aria-hidden="true" tabindex="-1"></a>penguins <span class="ot">&lt;-</span> </span>
<span id="cb12-6"><a href="linear-model-estimation.html#cb12-6" aria-hidden="true" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb12-7"><a href="linear-model-estimation.html#cb12-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">pl_fitted =</span> <span class="fu">predict</span>(lmodp))</span></code></pre></div>
<p>We now use the <code>geom_line</code> function to add the fitted lines for each <code>species</code> level to our scatter plot. The results of the code below are shown in Figure <a href="linear-model-estimation.html#fig:pl-penguin-fit">3.7</a>. The parallel lines model shown in Figure <a href="linear-model-estimation.html#fig:pl-penguin-fit">3.7</a> fits the <code>penguins</code> data better than the simple linear regression model shown in Figure <a href="linear-model-estimation.html#fig:slr-penguin-fit">3.5</a>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="linear-model-estimation.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span> </span>
<span id="cb13-2"><a href="linear-model-estimation.html#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm, <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb13-3"><a href="linear-model-estimation.html#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb13-4"><a href="linear-model-estimation.html#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> pl_fitted, <span class="at">col =</span> species))</span>
<span id="cb13-5"><a href="linear-model-estimation.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: Removed 2 rows containing missing values (geom_point).</span></span>
<span id="cb13-6"><a href="linear-model-estimation.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: Removed 2 row(s) containing missing values (geom_path).</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pl-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/pl-penguin-fit-1.png" alt="The fitted lines from the separate lines model for each level of `species` is added to the grouped scatter plot of `bill_length_mm` versus `body_mass_g`." width="672" />
<p class="caption">
Figure 3.7: The fitted lines from the separate lines model for each level of <code>species</code> is added to the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code>.
</p>
</div>
<p>We now fit a separate lines regression model to the <code>penguins</code> data. Specifically, we fit the model
<span class="math display">\[
\begin{align}
&amp;E(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= \beta_{0} + \beta_1 \mathtt{body\_mass\_g} + \beta_2 D_C + \beta_3 D_G + \beta_4 D_C \mathtt{body\_mass\_g} + \beta_5 D_G \mathtt{body\_mass\_g},
\end{align}
\]</span>
using the code below, using the <code>coef</code> function to extract the estimated coefficients. The terms with <code>:</code> are interaction variables, e.g., <code>body_mass_g:speciesChinstrap</code> is <span class="math inline">\(\hat{\beta}_4\)</span>, the coefficient for the interaction between regressors <span class="math inline">\(D_C\)</span> (the indicator variable for the <code>Chinstrap</code> species) and <span class="math inline">\(\mathtt{body\_mass\_g}\)</span>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear-model-estimation.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit separate lines model</span></span>
<span id="cb14-2"><a href="linear-model-estimation.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># na.omit = na.exclude used to change predict behavior</span></span>
<span id="cb14-3"><a href="linear-model-estimation.html#cb14-3" aria-hidden="true" tabindex="-1"></a>lmods <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> species <span class="sc">+</span> body_mass_g<span class="sc">:</span>species,</span>
<span id="cb14-4"><a href="linear-model-estimation.html#cb14-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> penguins, <span class="at">na.action =</span> na.exclude)</span>
<span id="cb14-5"><a href="linear-model-estimation.html#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># extract estimated coefficients</span></span>
<span id="cb14-6"><a href="linear-model-estimation.html#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmods)</span>
<span id="cb14-7"><a href="linear-model-estimation.html#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                  (Intercept)                  body_mass_g </span></span>
<span id="cb14-8"><a href="linear-model-estimation.html#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                26.9941391367                 0.0031878758 </span></span>
<span id="cb14-9"><a href="linear-model-estimation.html#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="do">##             speciesChinstrap                speciesGentoo </span></span>
<span id="cb14-10"><a href="linear-model-estimation.html#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="do">##                 5.1800537287                -0.2545906615 </span></span>
<span id="cb14-11"><a href="linear-model-estimation.html#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="do">## body_mass_g:speciesChinstrap    body_mass_g:speciesGentoo </span></span>
<span id="cb14-12"><a href="linear-model-estimation.html#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                 0.0012748183                 0.0009029956</span></span></code></pre></div>
<p>Thus, the fitted separate lines model is
<span class="math display" id="eq:sl-model-penguins">\[
\begin{align}
&amp;\hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}) \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 D_C - 0.25 D_G \\
&amp;\quad + 0.001 D_C \mathtt{body\_mass\_g} + 0.0009 D_G \mathtt{body\_mass\_g}, \tag{3.20}
\end{align} 
\]</span></p>
<p>When an observation has <code>species</code> level <code>Adelie</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.20)</a> simplifies to
<span class="math display">\[
\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Adelie}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 0 - 0.25 \cdot 0\\
&amp;\quad + 0.001 \cdot 0 \mathtt{body\_mass\_g} + 0.0009 \cdot 0 \mathtt{body\_mass\_g} \\
&amp;= 26.99 + 0.003 \mathtt{body\_mass\_g}.
\end{align}
\]</span>
When an observation has <code>species</code> level <code>Chinstrap</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.20)</a> simplifies to
<span class="math display">\[
\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 0 - 0.25 \cdot 1 \\
&amp;\quad + 0.001 \cdot 0 \mathtt{body\_mass\_g} + 0.0009 \cdot 1 \mathtt{body\_mass\_g} \\
&amp;= 31.17 + 0.004 \mathtt{body\_mass\_g}.
\end{align}
\]</span>
When an observation has <code>species</code> level <code>Gentoo</code>, then Equation <a href="linear-model-estimation.html#eq:sl-model-penguins">(3.20)</a> simplifies to
<span class="math display">\[
\begin{align}
\widehat{\mathtt{bill\_length\_mm}} &amp;= \hat{E}(\mathtt{bill\_length\_mm} \mid \mathtt{body\_mass\_g}, \mathtt{species}=\mathtt{Chinstrap}) \\
&amp;=26.99 + 0.003 \mathtt{body\_mass\_g} + 5.18 \cdot 0 - 0.25 \cdot 1 \\
&amp;\quad + 0.001 \cdot 0 \mathtt{body\_mass\_g} + 0.0009 \cdot 1 \mathtt{body\_mass\_g} \\
&amp;= 26.74 + 0.004 \mathtt{body\_mass\_g}.
\end{align}
\]</span></p>
<p>We use the code below to display the fitted lines for the separate lines model on the <code>penguins</code> data. Figure <a href="linear-model-estimation.html#fig:sl-penguin-fit">3.8</a> shows the results. The fitted lines match the observed data behavior reasonably well.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="linear-model-estimation.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add separate lines fitted values to penguins data frame</span></span>
<span id="cb15-2"><a href="linear-model-estimation.html#cb15-2" aria-hidden="true" tabindex="-1"></a>penguins <span class="ot">&lt;-</span> </span>
<span id="cb15-3"><a href="linear-model-estimation.html#cb15-3" aria-hidden="true" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb15-4"><a href="linear-model-estimation.html#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transform</span>(<span class="at">sl_fitted =</span> <span class="fu">predict</span>(lmods))</span>
<span id="cb15-5"><a href="linear-model-estimation.html#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># use geom_line to add fitted lines to plot</span></span>
<span id="cb15-6"><a href="linear-model-estimation.html#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> penguins) <span class="sc">+</span> </span>
<span id="cb15-7"><a href="linear-model-estimation.html#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> bill_length_mm, <span class="at">shape =</span> species, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb15-8"><a href="linear-model-estimation.html#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;body mass (g)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;bill length (mm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb15-9"><a href="linear-model-estimation.html#cb15-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> body_mass_g, <span class="at">y =</span> sl_fitted, <span class="at">col =</span> species))</span>
<span id="cb15-10"><a href="linear-model-estimation.html#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: Removed 2 rows containing missing values (geom_point).</span></span>
<span id="cb15-11"><a href="linear-model-estimation.html#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="do">## Warning: Removed 2 row(s) containing missing values (geom_path).</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:sl-penguin-fit"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/sl-penguin-fit-1.png" alt="The fitted model for each level of `species` is added to the grouped scatter plot of `bill_length_mm` versus `body_mass_g`." width="672" />
<p class="caption">
Figure 3.8: The fitted model for each level of <code>species</code> is added to the grouped scatter plot of <code>bill_length_mm</code> versus <code>body_mass_g</code>.
</p>
</div>
<p>Having fit several models for the <code>penguins</code> data, we may be wondering how to evaluate how well the models fit the data. We discuss that in the next section.</p>
</div>
<div id="evaluating-model-fit" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Evaluating model fit<a href="linear-model-estimation.html#evaluating-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most basic statistic measuring the fit of a regression model is the <strong>coefficient of determination</strong>, which is denoted
<span class="math display" id="eq:rsquared">\[\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^n (Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2},\tag{3.21}
\end{equation}\]</span>
where <span class="math inline">\(\bar{Y}\)</span> is the sample mean of the observed response values.</p>
<p>To interpret this statistic, we need to introduce some new “sum-of-squares” statistics similar to the RSS.</p>
<p>The <strong>total sum of squares</strong> (corrected for the mean) is computed as
<span class="math display" id="eq:tss">\[\begin{equation}
TSS = \sum_{i=1}^n(Y_i-\bar{Y})^2. \tag{3.22}
\end{equation}\]</span>
The TSS is the sum of the squared deviations from the sample mean of the response values. However, it has a more insightful interpretation. Consider the <strong>constant mean model</strong>, which is the model
<span class="math display" id="eq:constant-mean-model">\[\begin{equation}
E(Y)=\beta_0. \tag{3.23}
\end{equation}\]</span>
Using basic calculus, you can show that the OLS estimator of <span class="math inline">\(\beta_0\)</span> for the model in Equation <a href="linear-model-estimation.html#eq:constant-mean-model">(3.23)</a> if <span class="math inline">\(\hat{\beta}_0\)</span> is <span class="math inline">\(\bar{Y}\)</span>. For the constant mean model, the fitted value of every observation is <span class="math inline">\(\hat{\beta}_0\)</span>, i.e., <span class="math inline">\(\hat{Y}_i=\hat{\beta}_0\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>. Thus, the RSS of the constant mean model is <span class="math inline">\(\sum_{i=1}^n(Y_i-\hat{Y}_i)^2=\sum_{i=1}^n(Y_i-\bar{Y})^2\)</span>. Thus, <em>the TSS is the RSS for the constant mean model</em>.</p>
<p>The <strong>regression sum-of-squares</strong> or <strong>model sum-of-squares</strong> is defined as
<span class="math display" id="eq:ssreg">\[\begin{equation}
SS_{reg} = \sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2. \tag{3.24}
\end{equation}\]</span>
Thus, SS<sub>reg</sub> is the sum of the squared deviations between the fitted values of a model and the fitted values of the constant mean model. More helpfully, we have the following equation relating TSS, RSS, and SS<sub>reg</sub>:
<span class="math display" id="eq:ss-equality">\[\begin{equation}
TSS = RSS + SS_{reg}.\tag{3.25}
\end{equation}\]</span>
Thus, <span class="math inline">\(SS_{reg}=TSS-RSS\)</span>. This means that <em>SS<sub>reg</sub> measures the reduction in RSS when comparing the fitted model to the constant mean model</em>.</p>
<p>Comparing Equations <a href="linear-model-estimation.html#eq:def-rss-slr">(3.5)</a>, <a href="linear-model-estimation.html#eq:rsquared">(3.21)</a>, <a href="linear-model-estimation.html#eq:tss">(3.22)</a>, and <a href="linear-model-estimation.html#eq:ssreg">(3.24)</a>, we can express <span class="math inline">\(R^2\)</span> as:
<span class="math display" id="eq:rsquared2">\[\begin{align}
R^2 &amp;= 1-\frac{\sum_{i=1}^n (Y_i-\hat{Y}_i)^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} \\
&amp;= 1 - \frac{RSS}{TSS} \\
&amp;= \frac{TSS - RSS}{TSS} \tag{3.26}\\
&amp;= \frac{SS_{reg}}{TSS} \\
&amp;= [\mathrm{cor}(\mathbf{y}, \hat{\mathbf{y}})]^2.
\end{align}\]</span>
The last expression is the squared sample correlation between the observed and fitted values, and is a helpful way to express the coefficient of determination as it extends to regression models that are not linear.</p>
<p>Looking at Equation <a href="linear-model-estimation.html#eq:rsquared2">(3.26)</a> in particular, we can say that <em>the coefficient of determination is the proportional reduction in RSS when comparing the fitted model to the constant mean model</em>.</p>
<p>Some comments about the coefficient of determination:</p>
<ul>
<li><span class="math inline">\(0\leq R^2 \leq 1\)</span>.</li>
<li><span class="math inline">\(R^2=0\)</span> for the constant mean model.</li>
<li><span class="math inline">\(R^2=1\)</span> for a fitted model that perfectly fits the data (the fitted values match the observed response values).</li>
<li>Generally, larger values of <span class="math inline">\(R^2\)</span> suggest that the model explains a lot of the variation in the response variable. Smaller <span class="math inline">\(R^2\)</span> values suggest the fitted model does not explain a lot of the response variation.</li>
<li>The <code>Multiple R-squared</code> value printed by the <code>summary</code> of an <code>lm</code> objects is <span class="math inline">\(R^2\)</span>.</li>
<li>To extract <span class="math inline">\(R^2\)</span> from a fitted model, you can use the syntax <code>summary(lmod)$r.squared</code>, where <code>lmod</code> is your fitted model.</li>
</ul>
<p>Figure <a href="linear-model-estimation.html#fig:rsquared-examples">3.9</a> provides examples of the <span class="math inline">\(R^2\)</span> value for various fitted models.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="linear-model-estimation.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rsquared-examples"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/rsquared-examples-1.png" alt="The coefficient of determination values for 4 different data sets." width="672" />
<p class="caption">
Figure 3.9: The coefficient of determination values for 4 different data sets.
</p>
</div>
<p>The coefficient of determination for the parallel lines model fit to the <code>penguins</code> data in Section <a href="linear-model-estimation.html#s:penguins-mlr2">3.8</a> is 0.81, as shown in the R output below. By adding the <code>body_mass_g</code> regressor and <code>species</code> predictor to the constant mean model of <code>bill_length_mm</code>, we reduced the RSS by 81%.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="linear-model-estimation.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmodp)<span class="sc">$</span>r.squared</span>
<span id="cb17-2"><a href="linear-model-estimation.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8079566</span></span></code></pre></div>
<p>It may seem sensible to choose between models based on the value of <span class="math inline">\(R^2\)</span>. This is unwise for two reasons:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(R^2\)</span> never decreases as regressors are added to an existing model. Basically, you can increase <span class="math inline">\(R^2\)</span> by simply adding regressors to your existing model, even if they are non-sensical.</li>
<li><span class="math inline">\(R^2\)</span> doesn’t tell you whether a model adequately describes the pattern of the observed data. <span class="math inline">\(R^2\)</span> is a useful statistic for measuring model fit when there is approximately a linear relationship between the response values and fitted values.</li>
</ol>
<p>Regarding point 1, consider the following code where we add 344 values randomly drawn from a <span class="math inline">\(\mathsf{N}(0,1)\)</span> to the parallel lines regression model in <code>lmodp</code>, then extract the <span class="math inline">\(R^2\)</span> value. We use the <code>update</code> method to update our existing model. The <code>update</code> functions takes an existing model as its first argument and then the <code>formula</code> for the updated model. The syntax <code>. ~ .</code> means “keep the same response (on the left) and the same regressors (on the right).” We can then add or subtract regressors using the typical <code>formula</code> syntax. We use this approach to add the regressor of random values below.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="linear-model-estimation.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add random noise as regressor to lmodp</span></span>
<span id="cb18-2"><a href="linear-model-estimation.html#cb18-2" aria-hidden="true" tabindex="-1"></a>lmod_silly <span class="ot">&lt;-</span> <span class="fu">update</span>(lmodp, . <span class="sc">~</span> . <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">344</span>))</span>
<span id="cb18-3"><a href="linear-model-estimation.html#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract R^2 from fitted model</span></span>
<span id="cb18-4"><a href="linear-model-estimation.html#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod_silly)<span class="sc">$</span>r.squared</span>
<span id="cb18-5"><a href="linear-model-estimation.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8080559</span></span></code></pre></div>
<p>The <span class="math inline">\(R^2\)</span> value increased from 0.8080 to 0.8085! So clearly, choosing the model with the largest <span class="math inline">\(R^2\)</span> can be a mistake, as it will tend to favor models with more regressors.</p>
<p>Regarding point 2, <span class="math inline">\(R^2\)</span> can mislead you into thinking an inappropriate model fits better than it actually does. <span class="citation"><a href="#ref-anscombe1973graphs" role="doc-biblioref">Anscombe</a> (<a href="#ref-anscombe1973graphs" role="doc-biblioref">1973</a>)</span> provided a canonical data set known as “Anscombe’s quartet” that illustrates this point. The data set is comprised of 4 different data sets. When a simple linear regression model is fit to each data set, we find that <span class="math inline">\(\hat{\beta}_0=3\)</span> and <span class="math inline">\(\hat{\beta}_1=0.5\)</span> and that <span class="math inline">\(R^2=0.67\)</span>. However, as we will see, not all models describe the data particularly well!</p>
<p>Anscombe’s quartet is available as the <code>anscombe</code> data set in the <strong>datasets</strong> package. The data set includes 11 observations of
8 variables. The variables are:</p>
<ul>
<li><code>x1</code>, <code>x2</code>, <code>x3</code> <code>x4</code>: the regressor variable for each individual data set.</li>
<li><code>y1</code>, <code>y2</code>, <code>y3</code> <code>y4</code>: the response variable for each individual data set.</li>
</ul>
<p>We fit the simple linear regression model to the four data sets in the code below, then extract the coefficients and <span class="math inline">\(R^2\)</span> to verify the information provided above.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="linear-model-estimation.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model to first data set</span></span>
<span id="cb19-2"><a href="linear-model-estimation.html#cb19-2" aria-hidden="true" tabindex="-1"></a>lmod_a1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x1, <span class="at">data =</span> anscombe)</span>
<span id="cb19-3"><a href="linear-model-estimation.html#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract coefficients from fitted model</span></span>
<span id="cb19-4"><a href="linear-model-estimation.html#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod_a1)</span>
<span id="cb19-5"><a href="linear-model-estimation.html#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          x1 </span></span>
<span id="cb19-6"><a href="linear-model-estimation.html#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="do">##   3.0000909   0.5000909</span></span>
<span id="cb19-7"><a href="linear-model-estimation.html#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># extract R^2 from fitted model</span></span>
<span id="cb19-8"><a href="linear-model-estimation.html#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod_a1)<span class="sc">$</span>r.squared</span>
<span id="cb19-9"><a href="linear-model-estimation.html#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6665425</span></span>
<span id="cb19-10"><a href="linear-model-estimation.html#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model to second data set</span></span>
<span id="cb19-11"><a href="linear-model-estimation.html#cb19-11" aria-hidden="true" tabindex="-1"></a>lmod_a2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> x2, <span class="at">data =</span> anscombe)</span>
<span id="cb19-12"><a href="linear-model-estimation.html#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod_a2)</span>
<span id="cb19-13"><a href="linear-model-estimation.html#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          x2 </span></span>
<span id="cb19-14"><a href="linear-model-estimation.html#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="do">##    3.000909    0.500000</span></span>
<span id="cb19-15"><a href="linear-model-estimation.html#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod_a2)<span class="sc">$</span>r.squared</span>
<span id="cb19-16"><a href="linear-model-estimation.html#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.666242</span></span>
<span id="cb19-17"><a href="linear-model-estimation.html#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model to third data set</span></span>
<span id="cb19-18"><a href="linear-model-estimation.html#cb19-18" aria-hidden="true" tabindex="-1"></a>lmod_a3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y3 <span class="sc">~</span> x3, <span class="at">data =</span> anscombe)</span>
<span id="cb19-19"><a href="linear-model-estimation.html#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod_a3)</span>
<span id="cb19-20"><a href="linear-model-estimation.html#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          x3 </span></span>
<span id="cb19-21"><a href="linear-model-estimation.html#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="do">##   3.0024545   0.4997273</span></span>
<span id="cb19-22"><a href="linear-model-estimation.html#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod_a3)<span class="sc">$</span>r.squared</span>
<span id="cb19-23"><a href="linear-model-estimation.html#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.666324</span></span>
<span id="cb19-24"><a href="linear-model-estimation.html#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model to fourth data set</span></span>
<span id="cb19-25"><a href="linear-model-estimation.html#cb19-25" aria-hidden="true" tabindex="-1"></a>lmod_a4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y4 <span class="sc">~</span> x4, <span class="at">data =</span> anscombe)</span>
<span id="cb19-26"><a href="linear-model-estimation.html#cb19-26" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(lmod_a4)</span>
<span id="cb19-27"><a href="linear-model-estimation.html#cb19-27" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)          x4 </span></span>
<span id="cb19-28"><a href="linear-model-estimation.html#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="do">##   3.0017273   0.4999091</span></span>
<span id="cb19-29"><a href="linear-model-estimation.html#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod_a4)<span class="sc">$</span>r.squared</span>
<span id="cb19-30"><a href="linear-model-estimation.html#cb19-30" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6667073</span></span></code></pre></div>
<p>Figure <a href="linear-model-estimation.html#fig:anscombe-plots">3.10</a> provides a scatter plot each data set and overlays their fitted models.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="linear-model-estimation.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># par(mfrow = c(1, 4))</span></span>
<span id="cb20-2"><a href="linear-model-estimation.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(y1 ~ x1, data = anscombe)</span></span>
<span id="cb20-3"><a href="linear-model-estimation.html#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># title(&quot;data set 1&quot;)</span></span>
<span id="cb20-4"><a href="linear-model-estimation.html#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># abline(lmod_a1)</span></span>
<span id="cb20-5"><a href="linear-model-estimation.html#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(y2 ~ x2, data = anscombe)</span></span>
<span id="cb20-6"><a href="linear-model-estimation.html#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># title(&quot;data set 2&quot;)</span></span>
<span id="cb20-7"><a href="linear-model-estimation.html#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># abline(lmod_a2)</span></span>
<span id="cb20-8"><a href="linear-model-estimation.html#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(y3 ~ x3, data = anscombe)</span></span>
<span id="cb20-9"><a href="linear-model-estimation.html#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># title(&quot;data set 3&quot;)</span></span>
<span id="cb20-10"><a href="linear-model-estimation.html#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># abline(lmod_a3)</span></span>
<span id="cb20-11"><a href="linear-model-estimation.html#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot(y4 ~ x4, data = anscombe)</span></span>
<span id="cb20-12"><a href="linear-model-estimation.html#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># title(&quot;data set 4&quot;)</span></span>
<span id="cb20-13"><a href="linear-model-estimation.html#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># abline(lmod_a4)</span></span>
<span id="cb20-14"><a href="linear-model-estimation.html#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># par(mfrow = c(1, 1))</span></span>
<span id="cb20-15"><a href="linear-model-estimation.html#cb20-15" aria-hidden="true" tabindex="-1"></a>adf <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">unlist</span>(anscombe[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]),</span>
<span id="cb20-16"><a href="linear-model-estimation.html#cb20-16" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">unlist</span>(anscombe[,<span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>]),</span>
<span id="cb20-17"><a href="linear-model-estimation.html#cb20-17" aria-hidden="true" tabindex="-1"></a>                 <span class="at">set =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">each =</span> <span class="dv">11</span>)))</span>
<span id="cb20-18"><a href="linear-model-estimation.html#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(adf, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">group =</span> set, <span class="at">col =</span> set)) <span class="sc">+</span></span>
<span id="cb20-19"><a href="linear-model-estimation.html#cb20-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb20-20"><a href="linear-model-estimation.html#cb20-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">group =</span> set), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb20-21"><a href="linear-model-estimation.html#cb20-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(. <span class="sc">~</span> set) <span class="sc">+</span></span>
<span id="cb20-22"><a href="linear-model-estimation.html#cb20-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb20-23"><a href="linear-model-estimation.html#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="do">## `geom_smooth()` using formula &#39;y ~ x&#39;</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:anscombe-plots"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/anscombe-plots-1.png" alt="Scatter plots of the four Anscombe (1973) data sets along with their line of best fit." width="672" />
<p class="caption">
Figure 3.10: Scatter plots of the four Anscombe (1973) data sets along with their line of best fit.
</p>
</div>
<p>While the fitted model and <span class="math inline">\(R^2\)</span> value is essentially the same for each model, the fitted model is only appropriate for data set 1. The fitted model for the second data set fails to model the curve of the data. The third fitted model doesn’t handle the outlier in the data. Lastly, the fourth data set has a single point on the far right side driving the model fit, so the fitted model is highly questionable.</p>
<p>To address the problem with <span class="math inline">\(R^2\)</span> that it cannot decrease as regressors are added to a model, <span class="citation"><a href="#ref-ezekiel1930methods" role="doc-biblioref">Ezekiel</a> (<a href="#ref-ezekiel1930methods" role="doc-biblioref">1930</a>)</span> proposed the adjusted R-squared statistic for measuring model fit. The adjusted <span class="math inline">\(R^2\)</span> statistic is defined as
<span class="math display">\[
R^2_a=1-(1-R^2)\frac{n-1}{n-p}=1-\frac{RSS/(n-p)}{TSS/(n-1)}.
\]</span>
Practically speaking, <span class="math inline">\(R^2_a\)</span> will only increase when a regressors substantively improves the fit of the model to the observed data. We favor models with larger values of <span class="math inline">\(R^2_a\)</span>. To extract the adjusted R-squared from a fitted model, we can use the syntax <code>summary(lmod)$adj.R.squared</code>, where <code>lmod</code> is the fitted model.</p>
<p>We extract the <span class="math inline">\(R^2_a\)</span> for the 4 models we previously fit to the <code>penguins</code> data. Specifically, we extract <span class="math inline">\(R_a^2\)</span> for the simple linear regression model fit in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>, the multiple linear regression model in Section <a href="linear-model-estimation.html#s:penguins-mlr">3.6</a>, and the parallel and separate lines models fit in Section <a href="#s:s:penguins-mlr2"><strong>??</strong></a>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="linear-model-estimation.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simple linear regression model</span></span>
<span id="cb21-2"><a href="linear-model-estimation.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb21-3"><a href="linear-model-estimation.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.3522562</span></span>
<span id="cb21-4"><a href="linear-model-estimation.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># multiple linear regression model</span></span>
<span id="cb21-5"><a href="linear-model-estimation.html#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mlmod)<span class="sc">$</span>adj.r.squared</span>
<span id="cb21-6"><a href="linear-model-estimation.html#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.4295084</span></span>
<span id="cb21-7"><a href="linear-model-estimation.html#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># parallel lines model</span></span>
<span id="cb21-8"><a href="linear-model-estimation.html#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmodp)<span class="sc">$</span>adj.r.squared</span>
<span id="cb21-9"><a href="linear-model-estimation.html#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8062521</span></span>
<span id="cb21-10"><a href="linear-model-estimation.html#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># separate lines model</span></span>
<span id="cb21-11"><a href="linear-model-estimation.html#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmods)<span class="sc">$</span>adj.r.squared</span>
<span id="cb21-12"><a href="linear-model-estimation.html#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8069556</span></span></code></pre></div>
<p>With an <span class="math inline">\(R_a^2\)</span> of 0.8070, the separate lines regression model appears to be slightly favored over the other 4 models fit to the penguin data. To confirm that this statistic is meaningful (i.e., that the model provides a reasonable fit to the data), we use the code below to create a scatter plot of the response versus fitted values shown in Figure <a href="linear-model-estimation.html#fig:y-vs-yhat-penguins">3.11</a>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="linear-model-estimation.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(penguins<span class="sc">$</span>bill_length_mm <span class="sc">~</span> <span class="fu">fitted</span>(lmods),</span>
<span id="cb22-2"><a href="linear-model-estimation.html#cb22-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;fitted values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:y-vs-yhat-penguins"></span>
<img src="A-Progessive-Introduction-to-Linear-Models_files/figure-html/y-vs-yhat-penguins-1.png" alt="A scatter plot of the observed bill length versus the fitted values of the separate lines model for the `penguins` data." width="672" />
<p class="caption">
Figure 3.11: A scatter plot of the observed bill length versus the fitted values of the separate lines model for the <code>penguins</code> data.
</p>
</div>
<p>The points in Figure <a href="linear-model-estimation.html#fig:y-vs-yhat-penguins">3.11</a> follow a linear pattern, so the separate lines model seems to be a reasonable model for the <code>penguins</code> data.</p>
</div>
<div id="summary" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Summary<a href="linear-model-estimation.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learned:</p>
<ul>
<li>What a linear model is.</li>
<li>What various objects are, such as coefficients, residuals, fitted values, etc.</li>
<li>How to estimate the coefficients of a linear model using ordinary least squares estimation.</li>
<li>How to fit a linear model using R.</li>
<li>How to include a categorical predictor in a linear model.</li>
<li>How to evaluate the fit of a model.</li>
</ul>
<div id="summary-of-terms" class="section level3 hasAnchor" number="3.10.1">
<h3><span class="header-section-number">3.10.1</span> Summary of terms<a href="linear-model-estimation.html#summary-of-terms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have introduced many terms to define a linear model. It can be difficult to keep track of their notation, their purpose, whether they are observable, and whether they are treated as random variables or vectors. We discuss various terms below, and then summarize the discussion in Table <a href="linear-model-estimation.html#tab:term-df">3.2</a>.</p>
<p>We’ve already talked about observing the response variable and the predictor/regressor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. One way to distinguish observable versus non-observable variables is that observable variables are denoted using Phoenician letters (e.g., <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>) while non-observable variables are denoted using Greek letters (e.g., <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(\sigma^2\)</span>).</p>
<p>We treat the response variable as a random variable. Perhaps surprisingly, we treat the predictor and regressor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the regressor variables and the regression coefficients are non-random, the only way for the responses in Equation <a href="linear-model-estimation.html#eq:lmSystem">(3.10)</a> to be random variables is for the errors to be random.</p>
<table>
<caption><span id="tab:term-df">Table 3.2: </span>An overview of terms used to define a linear model.</caption>
<colgroup>
<col width="19%" />
<col width="62%" />
<col width="10%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="left">Description</th>
<th align="left">Observable?</th>
<th align="left">Random?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(Y\)</span></td>
<td align="left">response variable</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(Y_i\)</span></td>
<td align="left">response value for the <span class="math inline">\(i\)</span>th observation</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\mathbf{y}\)</span></td>
<td align="left">the <span class="math inline">\(n\times 1\)</span> column vector of response values</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(X\)</span></td>
<td align="left">regressor variable</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(X_j\)</span></td>
<td align="left">the <span class="math inline">\(j\)</span>th regressor variable</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(x_{i,j}\)</span></td>
<td align="left">the value of the <span class="math inline">\(j\)</span>th regressor variable for the <span class="math inline">\(i\)</span>th observation</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\mathbf{X}\)</span></td>
<td align="left">the <span class="math inline">\(n\times p\)</span> matrix of regressor values</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\mathbf{x}_i\)</span></td>
<td align="left">the <span class="math inline">\(p\times 1\)</span> vector of regressor values for the <span class="math inline">\(i\)</span>th observation</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\beta_j\)</span></td>
<td align="left">the coefficient associated with the <span class="math inline">\(j\)</span>th regressor variable</td>
<td align="left">No</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\boldsymbol{\beta}\)</span></td>
<td align="left">the <span class="math inline">\(p\times 1\)</span> column vector of regression coefficients</td>
<td align="left">No</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\epsilon\)</span></td>
<td align="left">the model error</td>
<td align="left">No</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\epsilon_i\)</span></td>
<td align="left">the error for the <span class="math inline">\(i\)</span>th observation</td>
<td align="left">No</td>
<td align="left">Yes</td>
</tr>
</tbody>
</table>
</div>
<div id="summary-of-functions" class="section level3 hasAnchor" number="3.10.2">
<h3><span class="header-section-number">3.10.2</span> Summary of functions<a href="linear-model-estimation.html#summary-of-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have used many functions in this Chapter. We summarize some of the most important ones in Table <a href="linear-model-estimation.html#tab:function-df">3.3</a>.</p>
<table>
<caption><span id="tab:function-df">Table 3.3: </span>An overview of important functions discussed in this chapter.</caption>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Function</th>
<th align="left">Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>lm</code></td>
<td align="left">Fits a linear model based on a provided <code>formula</code></td>
</tr>
<tr class="even">
<td align="left"><code>summary</code></td>
<td align="left">Provides summary information about the fitted model</td>
</tr>
<tr class="odd">
<td align="left"><code>coef</code></td>
<td align="left">Extracts the vector of estimated regression coefficients from the fitted model</td>
</tr>
<tr class="even">
<td align="left"><code>residuals</code></td>
<td align="left">Extracts the vector of residuals from the fitted model</td>
</tr>
<tr class="odd">
<td align="left"><code>fitted</code></td>
<td align="left">Extracts the vector of fitted values from the fitted model</td>
</tr>
<tr class="even">
<td align="left"><code>predict</code></td>
<td align="left">Computes the fitted values (or arbitrary predictions) based on a fitted model</td>
</tr>
<tr class="odd">
<td align="left"><code>deviance</code></td>
<td align="left">Extracts the RSS of a fitted model</td>
</tr>
<tr class="even">
<td align="left"><code>sigma</code></td>
<td align="left">Extracts <span class="math inline">\(\hat{\sigma}\)</span> from the fitted model</td>
</tr>
<tr class="odd">
<td align="left"><code>update</code></td>
<td align="left">Updates a fitted model to remove or add regressors</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="going-deeper" class="section level2 hasAnchor" number="3.11">
<h2><span class="header-section-number">3.11</span> Going Deeper<a href="linear-model-estimation.html#going-deeper" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="slr-derivation" class="section level3 hasAnchor" number="3.11.1">
<h3><span class="header-section-number">3.11.1</span> Derivation of the OLS estimator for the simple linear regression model coefficients<a href="linear-model-estimation.html#slr-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="manual-calculation-penguins-simple-linear-regression-example" class="section level3 hasAnchor" number="3.11.2">
<h3><span class="header-section-number">3.11.2</span> Manual calculation Penguins simple linear regression example<a href="linear-model-estimation.html#manual-calculation-penguins-simple-linear-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we manually produce (i.e., without the <code>lm</code> function) the <code>penguins</code> simple linear regression example in Section <a href="linear-model-estimation.html#s:penguins-slr">3.4</a>.</p>
<p>First, we will manually fit a simple linear regression model that regresses <code>bill_length_mm</code> on <code>body_mass_g</code>.</p>
<p>Using the <code>summary</code> function on the <code>penguins</code> data frame, we see that both <code>bill_length_mm</code> and <code>body_mass_g</code> have <code>NA</code> values.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="linear-model-estimation.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(penguins)</span>
<span id="cb23-2"><a href="linear-model-estimation.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="do">##       species          island    bill_length_mm  bill_depth_mm  </span></span>
<span id="cb23-3"><a href="linear-model-estimation.html#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="do">##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  </span></span>
<span id="cb23-4"><a href="linear-model-estimation.html#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  </span></span>
<span id="cb23-5"><a href="linear-model-estimation.html#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  </span></span>
<span id="cb23-6"><a href="linear-model-estimation.html#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="do">##                                  Mean   :43.92   Mean   :17.15  </span></span>
<span id="cb23-7"><a href="linear-model-estimation.html#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="do">##                                  3rd Qu.:48.50   3rd Qu.:18.70  </span></span>
<span id="cb23-8"><a href="linear-model-estimation.html#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                                  Max.   :59.60   Max.   :21.50  </span></span>
<span id="cb23-9"><a href="linear-model-estimation.html#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="do">##                                  NA&#39;s   :2       NA&#39;s   :2      </span></span>
<span id="cb23-10"><a href="linear-model-estimation.html#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="do">##  flipper_length_mm  body_mass_g       sex           year        pl_fitted    </span></span>
<span id="cb23-11"><a href="linear-model-estimation.html#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="do">##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007   Min.   :35.60  </span></span>
<span id="cb23-12"><a href="linear-model-estimation.html#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="do">##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007   1st Qu.:38.98  </span></span>
<span id="cb23-13"><a href="linear-model-estimation.html#cb23-13" aria-hidden="true" tabindex="-1"></a><span class="do">##  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008   Median :45.67  </span></span>
<span id="cb23-14"><a href="linear-model-estimation.html#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mean   :200.9     Mean   :4202                Mean   :2008   Mean   :43.92  </span></span>
<span id="cb23-15"><a href="linear-model-estimation.html#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="do">##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009   3rd Qu.:48.34  </span></span>
<span id="cb23-16"><a href="linear-model-estimation.html#cb23-16" aria-hidden="true" tabindex="-1"></a><span class="do">##  Max.   :231.0     Max.   :6300                Max.   :2009   Max.   :52.83  </span></span>
<span id="cb23-17"><a href="linear-model-estimation.html#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  NA&#39;s   :2         NA&#39;s   :2                                  NA&#39;s   :2      </span></span>
<span id="cb23-18"><a href="linear-model-estimation.html#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="do">##    sl_fitted    </span></span>
<span id="cb23-19"><a href="linear-model-estimation.html#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="do">##  Min.   :36.08  </span></span>
<span id="cb23-20"><a href="linear-model-estimation.html#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="do">##  1st Qu.:38.95  </span></span>
<span id="cb23-21"><a href="linear-model-estimation.html#cb23-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  Median :45.40  </span></span>
<span id="cb23-22"><a href="linear-model-estimation.html#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="do">##  Mean   :43.92  </span></span>
<span id="cb23-23"><a href="linear-model-estimation.html#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="do">##  3rd Qu.:48.42  </span></span>
<span id="cb23-24"><a href="linear-model-estimation.html#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="do">##  Max.   :53.60  </span></span>
<span id="cb23-25"><a href="linear-model-estimation.html#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="do">##  NA&#39;s   :2</span></span></code></pre></div>
<p>This is important to note because the <code>lm</code> function automatically removes any observation with <code>NA</code> values for any of the variables specified in the <code>formula</code> argument. In order to replicate our results, we must remove the same observations.</p>
<p>We want to remove the rows of <code>penguins</code> where either <code>body_mass_g</code> or <code>bill_length_mm</code> have <code>NA</code> values. We do that below using the <code>na.omit</code> function (selecting only the relevant variables) and assign the cleaned
object the name <code>penguins_clean</code>.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="linear-model-estimation.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># remove rows of penguins where bill_length_mm or body_mass_g have NA values </span></span>
<span id="cb24-2"><a href="linear-model-estimation.html#cb24-2" aria-hidden="true" tabindex="-1"></a>penguins_clean <span class="ot">&lt;-</span></span>
<span id="cb24-3"><a href="linear-model-estimation.html#cb24-3" aria-hidden="true" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb24-4"><a href="linear-model-estimation.html#cb24-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">subset</span>(<span class="at">select =</span> <span class="fu">c</span>(<span class="st">&quot;bill_length_mm&quot;</span>, <span class="st">&quot;body_mass_g&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb24-5"><a href="linear-model-estimation.html#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span></code></pre></div>
<p>We extract the <code>bill_length_mm</code> variable from the <code>penguins</code> data frame and assign it the name <code>y</code> since it will be the response variable. We extract the <code>body_mass_g</code> variable from the <code>penguins</code> data frame and
assign it the name <code>x</code> since it will be the regressor variable. We also determine the number of observations and assign that value the name <code>n</code>.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="linear-model-estimation.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract response and regressor from penguins_clean</span></span>
<span id="cb25-2"><a href="linear-model-estimation.html#cb25-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>bill_length_mm</span>
<span id="cb25-3"><a href="linear-model-estimation.html#cb25-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>body_mass_g</span>
<span id="cb25-4"><a href="linear-model-estimation.html#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># determine number of observations</span></span>
<span id="cb25-5"><a href="linear-model-estimation.html#cb25-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span></code></pre></div>
<p>We now compute <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> using Equations <a href="linear-model-estimation.html#eq:slr-beta1hat">(3.7)</a> and <a href="linear-model-estimation.html#eq:slr-beta0hat">(3.8)</a>. Note that placing <code>()</code> around the assignment operations will both perform the assignment and print the results.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="linear-model-estimation.html#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute OLS estimate of beta1</span></span>
<span id="cb26-2"><a href="linear-model-estimation.html#cb26-2" aria-hidden="true" tabindex="-1"></a>(b1 <span class="ot">&lt;-</span> (<span class="fu">sum</span>(x <span class="sc">*</span> y) <span class="sc">-</span> <span class="fu">sum</span>(x) <span class="sc">*</span> <span class="fu">sum</span>(y) <span class="sc">/</span> n)<span class="sc">/</span>(<span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">sum</span>(x)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n))</span>
<span id="cb26-3"><a href="linear-model-estimation.html#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.004051417</span></span>
<span id="cb26-4"><a href="linear-model-estimation.html#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># compute OLS estimate of beta0</span></span>
<span id="cb26-5"><a href="linear-model-estimation.html#cb26-5" aria-hidden="true" tabindex="-1"></a>(b0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> b1 <span class="sc">*</span> <span class="fu">mean</span>(x))        </span>
<span id="cb26-6"><a href="linear-model-estimation.html#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 26.89887</span></span></code></pre></div>
<p>The estimated value of <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(\hat{\beta}_0=26.90\)</span> and the estimated value of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat{\beta}_1=0.004\)</span>.</p>
<p>We can also compute the residuals, the fitted values, the RSS, and the estimated error variance. Using the code below, the RSS for our model is 6564.49 and the estimated error variance if <span class="math inline">\(\hat{\sigma}^2=19.31\)</span>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="linear-model-estimation.html#cb27-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="co"># compute fitted values</span></span>
<span id="cb27-2"><a href="linear-model-estimation.html#cb27-2" aria-hidden="true" tabindex="-1"></a>ehat <span class="ot">&lt;-</span> y <span class="sc">-</span> yhat <span class="co"># compute residuals</span></span>
<span id="cb27-3"><a href="linear-model-estimation.html#cb27-3" aria-hidden="true" tabindex="-1"></a>(rss <span class="ot">&lt;-</span> <span class="fu">sum</span>(ehat<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># sum of the squared residuals</span></span>
<span id="cb27-4"><a href="linear-model-estimation.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6564.494</span></span>
<span id="cb27-5"><a href="linear-model-estimation.html#cb27-5" aria-hidden="true" tabindex="-1"></a>(sigmasqhat <span class="ot">&lt;-</span> rss<span class="sc">/</span>(n<span class="dv">-2</span>)) <span class="co"># estimated error variance</span></span>
<span id="cb27-6"><a href="linear-model-estimation.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 19.30734</span></span></code></pre></div>
</div>
<div id="mlr-derivation" class="section level3 hasAnchor" number="3.11.3">
<h3><span class="header-section-number">3.11.3</span> Derivation of the OLS estimator for the multiple linear regression model coefficients<a href="linear-model-estimation.html#mlr-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to determine the value of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that will minimize
<span class="math display">\[
\begin{align}
RSS(\hat{\boldsymbol{\beta}}) &amp;=\sum_{i=1}^n \hat{\epsilon_i}^2 \\
&amp;= \hat{\boldsymbol{\epsilon}}^T\hat{\boldsymbol{\epsilon}} \\
&amp;= (\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}) \\
&amp;= \mathbf{y}^T\mathbf{y}-2\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}+\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}},
\end{align}
\]</span>
where the second term in the last line comes from the fact that <span class="math inline">\(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}\)</span> is a <span class="math inline">\(1\times 1\)</span> matrix, and is thus symmetric. Consequently, <span class="math inline">\(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y}=(\hat{\boldsymbol{\beta}}^T\mathbf{X}^T\mathbf{y})^T=\mathbf{y}^T\mathbf{X}\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>To find the local extrema of <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, we set its derivative with respect to <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> equal to 0, and solve for <span class="math inline">\(\hat{\boldsymbol{\beta}})\)</span>.</p>
<p>Using the results in Appendix <a href="overview-of-matrix-facts.html#matrix-derivatives">A.5</a>,
we see that
<span class="math display">\[
\frac{\partial RSS(\hat{\boldsymbol{\beta}})}{\partial\hat{\boldsymbol{\beta}}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}.
\]</span>
Setting <span class="math inline">\(\partial RSS(\hat{\boldsymbol{\beta}})/\partial\hat{\boldsymbol{\beta}}=0\)</span> and using some simple algebra, we derive the <strong>normal equations</strong>
<span class="math display">\[\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}^T\mathbf{y}.\]</span>
Assuming the <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is invertible, which it will be when <span class="math inline">\(\mathbf{X}\)</span> is full-rank, our solution is
<span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>To show that the OLS estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> minimizes <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, we technically need to show that the Hessian matrix of <span class="math inline">\(RSS(\hat{\boldsymbol{\beta}})\)</span>, the matrix of second-order partial derivatives, is positive definite. In our context, the Hessian matrix is
<span class="math display">\[
\begin{align}
\frac{\partial^2 RSS(\hat{\boldsymbol{\beta}})}{\partial \hat{\boldsymbol{\beta}}^2} &amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}(-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}) \\
&amp;= 2\mathbf{X}^T\mathbf{X}.
\end{align}
\]</span>
The <span class="math inline">\(p\times p\)</span> matrix <span class="math inline">\(2\mathbf{X}^T\mathbf{X}\)</span> is positive definite, but it is beyond the scope of the course to prove this.</p>
<p>Therefore, the OLS estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>,
<span class="math display">\[\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\]</span>
minimizes the RSS.</p>
</div>
<div id="manual-calculation-of-penguins-multiple-linear-regression-example" class="section level3 hasAnchor" number="3.11.4">
<h3><span class="header-section-number">3.11.4</span> Manual calculation of Penguins multiple linear regression example<a href="linear-model-estimation.html#manual-calculation-of-penguins-multiple-linear-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We manually verify the calculations for the <code>penguins</code> example given in Section @ref{s:penguins-mlr}, where we fit the multiple linear regression model regressing <code>bill_length_mm</code> on <code>body_mass_g</code> and <code>flipper_length_mm</code>. We refit the model below, specifying the argument <code>y = TRUE</code> so we can get the response vector used in the model.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="linear-model-estimation.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit regression model, retaining the y vector</span></span>
<span id="cb28-2"><a href="linear-model-estimation.html#cb28-2" aria-hidden="true" tabindex="-1"></a>mlmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(bill_length_mm <span class="sc">~</span> body_mass_g <span class="sc">+</span> flipper_length_mm,</span>
<span id="cb28-3"><a href="linear-model-estimation.html#cb28-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">data =</span> penguins, <span class="at">y =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>We can use <code>model.matrix</code> to extract the <span class="math inline">\(\mathbf{X}\)</span> matrix from our fitted model. And because we specified <code>y = TRUE</code> in our call to <code>lm</code>, we can also extract <code>y</code> from the fitted model using the code below.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="linear-model-estimation.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract X matrix from fitted model</span></span>
<span id="cb29-2"><a href="linear-model-estimation.html#cb29-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(mlmod)</span>
<span id="cb29-3"><a href="linear-model-estimation.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract y vector from fitted model</span></span>
<span id="cb29-4"><a href="linear-model-estimation.html#cb29-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> mlmod<span class="sc">$</span>y</span></code></pre></div>
<p>We’ll need to learn a few new commands in R to do the calculations:</p>
<ul>
<li><code>t</code> is the transpose of a matrix.</li>
<li><code>%*%</code> is the multiplication operator for two matrices.</li>
<li><code>solve(A, b)</code> computes <span class="math inline">\(\mathbf{A}^{-1}\mathbf{b}\)</span>.</li>
</ul>
<p>Thus, we compute <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> using the code below, which matches the estimate from the <code>lm</code> function.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="linear-model-estimation.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># manually calculate betahat</span></span>
<span id="cb30-2"><a href="linear-model-estimation.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span>
<span id="cb30-3"><a href="linear-model-estimation.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                            [,1]</span></span>
<span id="cb30-4"><a href="linear-model-estimation.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)       -3.4366939266</span></span>
<span id="cb30-5"><a href="linear-model-estimation.html#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="do">## body_mass_g        0.0006622186</span></span>
<span id="cb30-6"><a href="linear-model-estimation.html#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="do">## flipper_length_mm  0.2218654584</span></span>
<span id="cb30-7"><a href="linear-model-estimation.html#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># betahat from lm function</span></span>
<span id="cb30-8"><a href="linear-model-estimation.html#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mlmod)</span>
<span id="cb30-9"><a href="linear-model-estimation.html#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="do">##       (Intercept)       body_mass_g flipper_length_mm </span></span>
<span id="cb30-10"><a href="linear-model-estimation.html#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="do">##     -3.4366939266      0.0006622186      0.2218654584</span></span></code></pre></div>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anscombe1973graphs" class="csl-entry">
Anscombe, Francis J. 1973. <span>“Graphs in Statistical Analysis.”</span> <em>The American Statistician</em> 27 (1): 17–21.
</div>
<div id="ref-ezekiel1930methods" class="csl-entry">
Ezekiel, Mordecai. 1930. <span>“Methods of Correlation Analysis.”</span>
</div>
<div id="ref-GormanEtAl2014" class="csl-entry">
Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. <span>“Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).”</span> <em>PLOS ONE</em> 9 (3): 1–14. <a href="https://doi.org/10.1371/journal.pone.0090081">https://doi.org/10.1371/journal.pone.0090081</a>.
</div>
<div id="ref-R-palmerpenguins" class="csl-entry">
Horst, Allison, Alison Hill, and Kristen Gorman. 2020. <em>Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data</em>. <a href="https://CRAN.R-project.org/package=palmerpenguins">https://CRAN.R-project.org/package=palmerpenguins</a>.
</div>
<div id="ref-alr4" class="csl-entry">
Weisberg, Sanford. 2014. <em>Applied Linear Regression</em>. Fourth. Hoboken <span>NJ</span>: Wiley. <a href="http://z.umn.edu/alr4ed">http://z.umn.edu/alr4ed</a>.
</div>
<div id="ref-wilkinsonrogers1973" class="csl-entry">
Wilkinson, GN, and CE Rogers. 1973. <span>“Symbolic Description of Factorial Models for Analysis of Variance.”</span> <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 22 (3): 392–99.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-cleaning-and-exploration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interpreting-a-fitted-linear-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["A-Progessive-Introduction-to-Linear-Models.pdf", "A-Progessive-Introduction-to-Linear-Models.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
