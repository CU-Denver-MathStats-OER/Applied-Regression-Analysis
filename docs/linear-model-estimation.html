<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear model estimation | Joshua French</title>
  <meta name="description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear model estimation | Joshua French" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear model estimation | Joshua French" />
  
  <meta name="twitter:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

<meta name="author" content="Chapter 6 Linear model estimation | Joshua French" />


<meta name="date" content="2022-06-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="defining-and-fitting-a-linear-model.html"/>
<link rel="next" href="linear-model-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with Linear Regression</a></li>

<li class="divider"></li>
<li><a href="index.html#preliminaries">Preliminaries<span></span></a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#what-is-r"><i class="fa fa-check"></i><b>1.1</b> What is R?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#where-to-get-r-and-r-studio-desktop"><i class="fa fa-check"></i><b>1.2</b> Where to get R (and R Studio Desktop)<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#r-studio-layout"><i class="fa fa-check"></i><b>1.3</b> R Studio Layout<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.4</b> Running code, scripts, and comments<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-foundations.html"><a href="r-foundations.html#example"><i class="fa fa-check"></i><b>1.4.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages<span></span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="r-foundations.html"><a href="r-foundations.html#example-1"><i class="fa fa-check"></i><b>1.5.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help<span></span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="r-foundations.html"><a href="r-foundations.html#example-2"><i class="fa fa-check"></i><b>1.6.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.8</b> Assignment<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#example-3"><i class="fa fa-check"></i><b>1.8.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.9</b> Vectors<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.9.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#creating-patterned-vectors"><i class="fa fa-check"></i><b>1.9.2</b> Creating patterned vectors<span></span></a></li>
<li class="chapter" data-level="1.9.3" data-path="r-foundations.html"><a href="r-foundations.html#example-4"><i class="fa fa-check"></i><b>1.9.3</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.4" data-path="r-foundations.html"><a href="r-foundations.html#example-5"><i class="fa fa-check"></i><b>1.9.4</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.5" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.9.5</b> Categorical vectors<span></span></a></li>
<li class="chapter" data-level="1.9.6" data-path="r-foundations.html"><a href="r-foundations.html#example-6"><i class="fa fa-check"></i><b>1.9.6</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.7" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.9.7</b> Extracting parts of a vector<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.10</b> Helpful functions<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.10.1</b> General functions<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#example-7"><i class="fa fa-check"></i><b>1.10.2</b> Example<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.10.3</b> Functions related to statistical distributions<span></span></a></li>
<li class="chapter" data-level="1.10.4" data-path="r-foundations.html"><a href="r-foundations.html#example-8"><i class="fa fa-check"></i><b>1.10.4</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.11</b> Data Frames<span></span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="r-foundations.html"><a href="r-foundations.html#creation-1"><i class="fa fa-check"></i><b>1.11.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.11.2" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.11.2</b> Extracting parts of a data frame<span></span></a></li>
<li class="chapter" data-level="1.11.3" data-path="r-foundations.html"><a href="r-foundations.html#example-9"><i class="fa fa-check"></i><b>1.11.3</b> Example<span></span></a></li>
<li class="chapter" data-level="1.11.4" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.11.4</b> Importing Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#logical-statements"><i class="fa fa-check"></i><b>1.12</b> Logical statements<span></span></a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-comparisons"><i class="fa fa-check"></i><b>1.12.1</b> Basic comparisons<span></span></a></li>
<li class="chapter" data-level="1.12.2" data-path="r-foundations.html"><a href="r-foundations.html#example-10"><i class="fa fa-check"></i><b>1.12.2</b> Example<span></span></a></li>
<li class="chapter" data-level="1.12.3" data-path="r-foundations.html"><a href="r-foundations.html#and-and-or-statements"><i class="fa fa-check"></i><b>1.12.3</b> And and Or statements<span></span></a></li>
<li class="chapter" data-level="1.12.4" data-path="r-foundations.html"><a href="r-foundations.html#example-11"><i class="fa fa-check"></i><b>1.12.4</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#subsetting-with-logical-statements"><i class="fa fa-check"></i><b>1.13</b> Subsetting with logical statements<span></span></a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="r-foundations.html"><a href="r-foundations.html#example-12"><i class="fa fa-check"></i><b>1.13.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.14</b> Ecosystem debate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>2</b> Data exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-exploration.html"><a href="data-exploration.html#data-analysis-process"><i class="fa fa-check"></i><b>2.1</b> Data analysis process<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-exploration.html"><a href="data-exploration.html#problem-formulation"><i class="fa fa-check"></i><b>2.1.1</b> Problem Formulation<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="data-exploration.html"><a href="data-exploration.html#data-collection"><i class="fa fa-check"></i><b>2.1.2</b> Data collection<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-exploration.html"><a href="data-exploration.html#data-exploration-1"><i class="fa fa-check"></i><b>2.2</b> Data exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-exploration.html"><a href="data-exploration.html#numerical-summaries-of-data"><i class="fa fa-check"></i><b>2.2.1</b> Numerical summaries of data<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="data-exploration.html"><a href="data-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.2.2</b> Visual summaries of data<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="data-exploration.html"><a href="data-exploration.html#what-to-look-for"><i class="fa fa-check"></i><b>2.2.3</b> What to look for<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-exploration.html"><a href="data-exploration.html#kidney-example"><i class="fa fa-check"></i><b>2.3</b> Kidney Example<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-exploration.html"><a href="data-exploration.html#numerically-summarizing-the-data"><i class="fa fa-check"></i><b>2.3.1</b> Numerically summarizing the data<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="data-exploration.html"><a href="data-exploration.html#cleaning-the-data"><i class="fa fa-check"></i><b>2.3.2</b> Cleaning the data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-exploration.html"><a href="data-exploration.html#visualizing-data-with-base-graphics"><i class="fa fa-check"></i><b>2.4</b> Visualizing data with <strong>base</strong> graphics<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-exploration.html"><a href="data-exploration.html#histograms"><i class="fa fa-check"></i><b>2.4.1</b> Histograms<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-exploration.html"><a href="data-exploration.html#density-plots"><i class="fa fa-check"></i><b>2.4.2</b> Density plots<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-exploration.html"><a href="data-exploration.html#index-plots"><i class="fa fa-check"></i><b>2.4.3</b> Index plots<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-exploration.html"><a href="data-exploration.html#bivariate-scatter-plots"><i class="fa fa-check"></i><b>2.4.4</b> Bivariate scatter plots<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-exploration.html"><a href="data-exploration.html#bivariate-boxplots"><i class="fa fa-check"></i><b>2.4.5</b> Bivariate boxplots<span></span></a></li>
<li class="chapter" data-level="2.4.6" data-path="data-exploration.html"><a href="data-exploration.html#multiple-plots-in-one-figure"><i class="fa fa-check"></i><b>2.4.6</b> Multiple plots in one figure<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-exploration.html"><a href="data-exploration.html#visualizing-data-with-ggplot2"><i class="fa fa-check"></i><b>2.5</b> Visualizing data with <strong>ggplot2</strong><span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-histogram"><i class="fa fa-check"></i><b>2.5.1</b> A <strong>ggplot2</strong> histogram<span></span></a></li>
<li class="chapter" data-level="2.5.2" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-density-plot"><i class="fa fa-check"></i><b>2.5.2</b> A <strong>ggplot2</strong> density plot<span></span></a></li>
<li class="chapter" data-level="2.5.3" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-scatter-plot"><i class="fa fa-check"></i><b>2.5.3</b> A <strong>ggplot2</strong> scatter plot<span></span></a></li>
<li class="chapter" data-level="2.5.4" data-path="data-exploration.html"><a href="data-exploration.html#scaling-ggplot2-plots"><i class="fa fa-check"></i><b>2.5.4</b> Scaling <strong>ggplot2</strong> plots<span></span></a></li>
<li class="chapter" data-level="2.5.5" data-path="data-exploration.html"><a href="data-exploration.html#facetting-in-ggplot2"><i class="fa fa-check"></i><b>2.5.5</b> Facetting in <code>ggplot2</code><span></span></a></li>
<li class="chapter" data-level="2.5.6" data-path="data-exploration.html"><a href="data-exploration.html#summary-of-ggplot2"><i class="fa fa-check"></i><b>2.5.6</b> Summary of <strong>ggplot2</strong><span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data-exploration.html"><a href="data-exploration.html#summary-of-data-exploration"><i class="fa fa-check"></i><b>2.6</b> Summary of data exploration<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html"><i class="fa fa-check"></i><b>3</b> Review of probability, random variables, and random vectors<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#probability-basics"><i class="fa fa-check"></i><b>3.1</b> Probability Basics<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#random-variables"><i class="fa fa-check"></i><b>3.2</b> Random Variables<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#discrete-random-variables"><i class="fa fa-check"></i><b>3.2.1</b> Discrete random variables<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.2.2</b> Continuous random variables<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts-for-transformation-of-random-variables"><i class="fa fa-check"></i><b>3.2.3</b> Useful facts for transformation of random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3</b> Multivariate distributions<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#basic-properties"><i class="fa fa-check"></i><b>3.3.1</b> Basic properties<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#marginal-distributions"><i class="fa fa-check"></i><b>3.3.2</b> Marginal distributions<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#independence-of-random-variables"><i class="fa fa-check"></i><b>3.3.3</b> Independence of random variables<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#conditional-distributions"><i class="fa fa-check"></i><b>3.3.4</b> Conditional distributions<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#covariance"><i class="fa fa-check"></i><b>3.3.5</b> Covariance<span></span></a></li>
<li class="chapter" data-level="3.3.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>3.3.6</b> Useful facts for transformations of multiple random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#random-vectors"><i class="fa fa-check"></i><b>3.4</b> Random vectors<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#definition"><i class="fa fa-check"></i><b>3.4.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>3.4.2</b> Mean, variance, and covariance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>3.5</b> Properties of transformations of random vectors<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>3.6</b> Multivariate normal (Gaussian) distribution<span></span></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#definition-1"><i class="fa fa-check"></i><b>3.6.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts"><i class="fa fa-check"></i><b>3.6.2</b> Useful facts<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#example-1-1"><i class="fa fa-check"></i><b>3.7</b> Example 1<span></span></a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#bernoulli-distribution"><i class="fa fa-check"></i><b>3.7.1</b> Bernoulli distribution<span></span></a></li>
<li class="chapter" data-level="3.7.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#binomial-distribution"><i class="fa fa-check"></i><b>3.7.2</b> Binomial distribution<span></span></a></li>
<li class="chapter" data-level="3.7.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#poisson-distribution"><i class="fa fa-check"></i><b>3.7.3</b> Poisson Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#example-2-1"><i class="fa fa-check"></i><b>3.8</b> Example 2<span></span></a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-1"><i class="fa fa-check"></i><b>3.8.1</b> Problem 1<span></span></a></li>
<li class="chapter" data-level="3.8.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-2"><i class="fa fa-check"></i><b>3.8.2</b> Problem 2<span></span></a></li>
<li class="chapter" data-level="3.8.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-3"><i class="fa fa-check"></i><b>3.8.3</b> Problem 3<span></span></a></li>
<li class="chapter" data-level="3.8.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-4"><i class="fa fa-check"></i><b>3.8.4</b> Problem 4<span></span></a></li>
<li class="chapter" data-level="3.8.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-5"><i class="fa fa-check"></i><b>3.8.5</b> Problem 5<span></span></a></li>
<li class="chapter" data-level="3.8.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-6"><i class="fa fa-check"></i><b>3.8.6</b> Problem 6<span></span></a></li>
<li class="chapter" data-level="3.8.7" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-7"><i class="fa fa-check"></i><b>3.8.7</b> Problem 7<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html"><i class="fa fa-check"></i><b>4</b> Useful matrix facts<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#notation"><i class="fa fa-check"></i><b>4.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>4.2</b> Basic mathematical properties<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>4.2.1</b> Addition and subtraction<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>4.2.2</b> Scalar multiplication<span></span></a></li>
<li class="chapter" data-level="4.2.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>4.2.3</b> Matrix multiplication<span></span></a></li>
<li class="chapter" data-level="4.2.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>4.2.4</b> Associative property<span></span></a></li>
<li class="chapter" data-level="4.2.5" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>4.2.5</b> Distributive property<span></span></a></li>
<li class="chapter" data-level="4.2.6" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>4.2.6</b> No commutative property<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#transpose-and-related-properties"><i class="fa fa-check"></i><b>4.3</b> Transpose and related properties<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#definition-2"><i class="fa fa-check"></i><b>4.3.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#transpose-and-mathematical-operations"><i class="fa fa-check"></i><b>4.3.2</b> Transpose and mathematical operations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>4.4</b> Special matrices<span></span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>4.4.1</b> Square matrices<span></span></a></li>
<li class="chapter" data-level="4.4.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>4.4.2</b> Identity matrix<span></span></a></li>
<li class="chapter" data-level="4.4.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#symmetric"><i class="fa fa-check"></i><b>4.4.3</b> Symmetric<span></span></a></li>
<li class="chapter" data-level="4.4.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#idempotent"><i class="fa fa-check"></i><b>4.4.4</b> Idempotent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-inverse"><i class="fa fa-check"></i><b>4.5</b> Matrix inverse<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>4.6</b> Matrix derivatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html"><i class="fa fa-check"></i><b>5</b> Defining and fitting a linear model<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#background-and-terminology"><i class="fa fa-check"></i><b>5.1</b> Background and terminology<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#goals-of-regression"><i class="fa fa-check"></i><b>5.2</b> Goals of regression<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>5.3</b> Definition of a linear model<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#basic-construction-and-relationships"><i class="fa fa-check"></i><b>5.3.1</b> Basic construction and relationships<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#as-a-system-of-equations"><i class="fa fa-check"></i><b>5.3.2</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="5.3.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#using-matrix-notation"><i class="fa fa-check"></i><b>5.3.3</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#summarizing-the-components-of-a-linear-model"><i class="fa fa-check"></i><b>5.4</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#types-of-regression-models"><i class="fa fa-check"></i><b>5.5</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#standard-linear-model-assumptions-and-implications"><i class="fa fa-check"></i><b>5.6</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#mathematical-interpretation-of-coefficients"><i class="fa fa-check"></i><b>5.7</b> Mathematical interpretation of coefficients<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#coefficient-interpretation-in-simple-linear-regression"><i class="fa fa-check"></i><b>5.7.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#coefficient-interpretation-in-multiple-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Coefficient interpretation in multiple linear regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>6</b> Linear model estimation<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>6.1</b> A simple motivating example<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>6.2</b> Defining a linear model<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>6.2.1</b> Necessary components and notation<span></span></a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>6.2.2</b> Standard definition of linear model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>6.3</b> Estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>6.3.1</b> Fitted values, residuals, and RSS<span></span></a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>6.3.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>6.4</b> Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-coefficients"><i class="fa fa-check"></i><b>6.5</b> Estimation of the multiple linear regression coefficients)<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>6.5.1</b> Using matrix notation to represent a linear model<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>6.5.2</b> Residuals, fitted values, and RSS for multiple linear regression<span></span></a></li>
<li class="chapter" data-level="6.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#as-a-system-of-equations-1"><i class="fa fa-check"></i><b>6.5.3</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="6.5.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-1"><i class="fa fa-check"></i><b>6.5.4</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summarizing-the-components-of-a-linear-model-1"><i class="fa fa-check"></i><b>6.6</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#types-of-regression-models-1"><i class="fa fa-check"></i><b>6.7</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="6.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-linear-model-assumptions-and-implications-1"><i class="fa fa-check"></i><b>6.8</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="6.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#mathematical-interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>6.9</b> Mathematical interpretation of coefficients<span></span></a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#coefficient-interpretation-in-simple-linear-regression-1"><i class="fa fa-check"></i><b>6.9.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="6.9.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#coefficient-interpretation-in-multiple-linear-regression-1"><i class="fa fa-check"></i><b>6.9.2</b> Coefficient interpretation in multiple linear regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#exercises-1"><i class="fa fa-check"></i><b>6.10</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linear-model-inference.html"><a href="linear-model-inference.html"><i class="fa fa-check"></i><b>7</b> Linear model inference<span></span></a></li>
<li class="chapter" data-level="8" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html"><i class="fa fa-check"></i><b>8</b> Assumptions Stated and Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-concisely-stated"><i class="fa fa-check"></i><b>8.1</b> Standard assumptions concisely stated<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-prioritized"><i class="fa fa-check"></i><b>8.2</b> Standard assumptions prioritized<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Joshua French</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-model-estimation" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear model estimation<a href="linear-model-estimation.html#linear-model-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="a-simple-motivating-example" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> A simple motivating example<a href="linear-model-estimation.html#a-simple-motivating-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose you observe data related to the heights of 5 mothers and their
adult daughters. The observed heights (measured in inches) are provided
in Table <a href="linear-model-estimation.html#tab:mdheights">6.1</a>.</p>
<table>
<caption><span id="tab:mdheights">Table 6.1: </span>Heights of mothers and their adult daughters (in).</caption>
<thead>
<tr class="header">
<th align="right">observation</th>
<th align="right">mother’s height (in)</th>
<th align="right">daughter’s height (in)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">57.5</td>
<td align="right">61.5</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">60.5</td>
<td align="right">63.5</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">63.5</td>
<td align="right">63.5</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">66.5</td>
<td align="right">66.5</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">69.5</td>
<td align="right">66.5</td>
</tr>
</tbody>
</table>
<p>The 5 pairs of observed data are denoted
<span class="math display">\[(x_1, Y_1), (x_2, Y_2), \ldots, (x_5, Y_5),\]</span> with <span class="math inline">\((x_i, Y_i)\)</span>
denoting the data for observation <span class="math inline">\(i\)</span>. <span class="math inline">\(x_i\)</span> denotes the mother’s height
for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> denotes the daughter’s height for
observation <span class="math inline">\(i\)</span>. In this data set, e.g., <span class="math inline">\(x_3 = 63.5\)</span> and <span class="math inline">\(Y_5= 66.5\)</span>.</p>
<p>Figure <a href="linear-model-estimation.html#fig:mdheights-plot">6.1</a> displays a scatter plot of height data
provided in Table <a href="linear-model-estimation.html#tab:mdheights">6.1</a>. The relationship between the
points is approximately a straight line. Thus, we will model the typical
(mean) relationship between the height of mothers and their adult
daughters as a straight line.</p>
<div class="figure"><span style="display:block;" id="fig:mdheights-plot"></span>
<img src="Data-Analysis-with-Linear-Regression_files/figure-html/mdheights-plot-1.png" alt="A scatter plot displaying pairs of heights for a mother and her adult daughter." width="672" />
<p class="caption">
Figure 6.1: A scatter plot displaying pairs of heights for a mother and her adult daughter.
</p>
</div>
<p>The <span class="math inline">\(x_1,x_2,\ldots,x_5\)</span> are observed values of a random variable <span class="math inline">\(X\)</span>,
while <span class="math inline">\(Y_1, Y_2, \ldots, Y_5\)</span> are observed values of a random variable
<span class="math inline">\(Y\)</span>. Thus, <span class="math inline">\(X\)</span> denotes the height a mother and <span class="math inline">\(Y\)</span> denotes the height of
(one of) their adult daughter(s). We want to model variable <span class="math inline">\(Y\)</span> using
variable <span class="math inline">\(X\)</span>. The variable we are trying to model is known as the
<strong>response variable</strong>. The variables we use to model the response are
known as <strong>regressor variables</strong>. Response variables are also known as
<strong>outcome</strong>, <strong>output</strong>, or <strong>dependent</strong> variables. Regressor variables
are also known as <strong>explanatory</strong>, <strong>predictor</strong>, <strong>input</strong>,
<strong>dependent</strong>, or <strong>feature</strong> variables.</p>
<p>A regression model describes the typical relationship between the
response variable <span class="math inline">\(Y\)</span> as a function of the regressor variable <span class="math inline">\(X\)</span>. More
formally, the <strong>regression model</strong> for <span class="math inline">\(Y\)</span> as a function of <span class="math inline">\(X\)</span>, denoted
<span class="math inline">\(E(Y|X)\)</span> is the expected value of <span class="math inline">\(Y\)</span> conditional on the regressor <span class="math inline">\(X\)</span>.
The regression model specifically refers to the expected relationship
between the response and regressors.</p>
<p>A <strong>simple linear regression model</strong> assumes the regression model
between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is a straight line using the equation
<span class="math display">\[E(Y\mid X)=\beta_0 + \beta_1 X.\]</span> <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the
intercept and slope of our regression functions. In general, <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span> are known as <strong>regression coefficients</strong> and are model
parameters that we estimate from our data.</p>
<p>The estimated regression model is denoted by
<span class="math display">\[\hat{E}(Y|X)=\hat{\beta}_0 + \hat{\beta}_1 X,\]</span> where <span class="math inline">\(\hat{\beta}_0\)</span>
and <span class="math inline">\(\hat{\beta}_1\)</span> are values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that we
estimate from the data. A <span class="math inline">\(\hat{}\)</span> above a term indicates it is an
estimate. We will refer to <span class="math inline">\(\hat{E}(Y|X)\)</span> as the <strong>fitted model</strong> or
<strong>estimated regression model</strong>.</p>
<p>How do we determine the “best fitting” model? Consider Figure
<a href="linear-model-estimation.html#fig:three-fitted-lines">6.2</a>, in which 2 potential “best fitting”
models are drawn on the scatter plot of the height data. Which one is
best?</p>
<div class="figure"><span style="display:block;" id="fig:three-fitted-lines"></span>
<img src="Data-Analysis-with-Linear-Regression_files/figure-html/three-fitted-lines-1.png" alt="Comparison of three potential fitted models to some observed data. The fitted models are shown in grey." width="672" />
<p class="caption">
Figure 6.2: Comparison of three potential fitted models to some observed data. The fitted models are shown in grey.
</p>
</div>
<p>The rest of this chapter focuses on defining and estimating the
parameters of a <em>linear</em> regression model.</p>
</div>
<div id="defining-a-linear-model" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Defining a linear model<a href="linear-model-estimation.html#defining-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ss-necessary-components" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Necessary components and notation<a href="linear-model-estimation.html#ss-necessary-components" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin by defining notation for the components of a linear model and
some of their important properties. We repeat some of the previous
discussion for clarity.</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> denotes the response variable.</p>
<ul>
<li>The response variable is treated as a random variable.</li>
<li>We will observe realizations of this random variable for each
observation in our data set.</li>
</ul></li>
<li><p><span class="math inline">\(X\)</span> denotes a single regressor variable. <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, ,
<span class="math inline">\(X_{p-1}\)</span> denote distinct regressor variables if we are performing
multiple regression.</p>
<ul>
<li>The regressor variables are treated as non-random variables.</li>
<li>The observed values of the regressor variables are treated as
fixed, known values.</li>
</ul></li>
<li><p><span class="math inline">\(\mathbb{X}=\{X_1,\ldots,X_{p-1}\}\)</span> denotes the collection of all
regressors under consideration, though this notation is really only
needed in the context of multiple regression.</p></li>
<li><p><span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_{p-1}\)</span> denote <strong>regression
coefficients</strong>.</p>
<ul>
<li>Regression coefficients are statistical parameters that we will
estimate from our data.</li>
<li>The regression coefficients are treated as fixed (non-random)
but unknown values.</li>
<li>Regression coefficients are not observable.</li>
</ul></li>
<li><p><span class="math inline">\(\epsilon\)</span> denotes model <strong>error</strong>.</p>
<ul>
<li>The model error is more accurately described as random variation
of each observation from the regression model
<span class="math inline">\(E(\epsilon\mid\mathbb{X})\)</span>.</li>
<li>The error is treated as a random variable.</li>
<li>The error is assumed to have mean 0 for all values of the
regressors, i.e., <span class="math inline">\(E(\epsilon\mid\mathbb{X}) = 0\)</span>.</li>
<li>The variance of the errors is assumed to be a constant value for
all values of the regressors, i.e.,
<span class="math inline">\(\mathrm{var}(\epsilon\mid\mathbb{X})=\sigma^2\)</span>.</li>
<li>The error is never observable (except in the context of a
simulation study where the experimenter literally defines the
true model).</li>
</ul></li>
</ul>
</div>
<div id="standard-definition-of-linear-model" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Standard definition of linear model<a href="linear-model-estimation.html#standard-definition-of-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>linear model</strong> for <span class="math inline">\(Y\)</span> is defined by the equation <span class="math display" id="eq:lmdef">\[\begin{align}
Y &amp;= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1}
X_{p-1} + \epsilon \\
&amp;= E(Y \mid \mathbb{X}) + \epsilon \tag{5.1}
\end{align}\]</span></p>
<p>We write the model using the form in Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmdef">(5.1)</a> to
emphasize the fact <strong>the response value equals the expected response for
that combination of regressor values plus some error</strong>. It should be
clear from comparing Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmdef">(5.1)</a> with the previous line
that
<span class="math display">\[E(Y \mid \mathbb{X}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1},\]</span>
which we will prove later.</p>
<p>More generally, one can say that a regression model is linear if the
mean function can be written as a linear combination of the regression
coefficients and known values created from our regressor variables,
i.e.,
<span class="math display" id="eq:lmdef-cj">\[\begin{equation}
E(Y \mid X_1, X_2, \ldots, X_{p-1}) = \sum_{j=0}^{p-1} c_j \beta_j, \tag{6.1}
\end{equation}\]</span>
where <span class="math inline">\(c_0, c_1, \ldots, c_{p-1}\)</span> are known functions of
the regressor variables, e.g., <span class="math inline">\(c_1 = X_1 X_2 X_3\)</span>, <span class="math inline">\(c_3 = X_2^2\)</span>,
<span class="math inline">\(c_8 = \ln(X_1)/X_2^2\)</span>, etc. Thus, if <span class="math inline">\(g_0,\ldots,g_{p-1}\)</span> are functions
of <span class="math inline">\(\mathbb{X}\)</span>, then we can say that the regression model is linear if
it can be written as
<span class="math display">\[E(Y\mid \mathbb{X}) = \sum_{j=0}^{p-1} g_j(\mathbb{X})\beta_j.\]</span></p>
<p>Some examples of linear regression models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + +\beta_1 X + \beta_2 X^2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2^{-1}\)</span>.</li>
<li><span class="math inline">\(E(\ln(Y)|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y^{-1}|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
</ul>
<p>Some examples of non-linear regression models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0 + e^{\beta_1 X}\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + \beta_1 X/(\beta_2 + X)\)</span>.</li>
</ul>
<p>The latter regression models are non-linear models because there is no
way to express them using the expression in Equation <a href="linear-model-estimation.html#eq:lmdef-cj">(6.1)</a>.</p>
<p>There are many different methods of parameter estimation in statistics:
method-of-moments, maximum likelihood, Bayesian, etc. The most common
parameter estimation method for linear models is the <strong>least squares
method</strong>, which is commonly called <strong>Ordinary Least Squares (OLS)</strong>
estimation. OLS estimation estimates the regression coefficients with
the values that minimize the residual sum of squares (RSS), which we
will define shortly.</p>
</div>
</div>
<div id="estimation-of-the-simple-linear-regression-model" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Estimation of the simple linear regression model<a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ss:fv-resid-rss" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Fitted values, residuals, and RSS<a href="linear-model-estimation.html#ss:fv-resid-rss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that a simple linear regression model is defined by the equation
<span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon = E(Y|X) + \epsilon\]</span> where
<span class="math display">\[E(Y|X) = \beta_0 + \beta_1 X.\]</span> In a simple linear regression context,
we have <span class="math inline">\(n\)</span> observed responses <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> and <span class="math inline">\(n\)</span> regressor
values <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span>.</p>
<p>Let <span class="math inline">\(\hat{\beta}_j\)</span> denote the estimated value of <span class="math inline">\(\beta_j\)</span> and
<span class="math inline">\(\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X\)</span> denote the estimated
regression model.</p>
<p>The <span class="math inline">\(i\)</span>th fitted value is defined as
<span class="math display" id="eq:def-fitted-value-slr">\[\begin{equation}
\hat{Y}_i = \hat{E}(Y|X = x_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i. \tag{6.2}
\end{equation}\]</span>
Thus, the <span class="math inline">\(i\)</span>th fitted value is the estimated mean of <span class="math inline">\(Y\)</span> when the
regressor <span class="math inline">\(X=x_i\)</span>. More specifically, the <span class="math inline">\(i\)</span>th fitted value is the
estimated mean response for the combination of regressor values observed
for the <span class="math inline">\(i\)</span>th observation.</p>
<p>The <span class="math inline">\(i\)</span>th residual is defined as
<span class="math display" id="eq:def-residual-slr">\[\begin{equation}
\hat{\epsilon}_i = Y_i - \hat{Y}_i. \tag{6.3}
\end{equation}\]</span>
The <span class="math inline">\(i\)</span>th residual is the difference between the response and estimated
mean response of observation <span class="math inline">\(i\)</span>.</p>
<p><strong>The RSS of a regression model is the sum of its squared residuals</strong>.</p>
<p>The RSS for a simple linear regression model, as a function of the
estimated regression coefficients <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, is defined as
<span class="math display" id="eq:def-rss-slr">\[\begin{equation}
RSS(\hat{\beta}_0, \hat{\beta}_1) = \sum_{i=1}^n \hat{\epsilon}_i^2. \tag{6.4}
\end{equation}\]</span></p>
<p>Using several of objects defined above, there are many equivalent expressions for the RSS. Notably, Equation <a href="linear-model-estimation.html#eq:def-rss-slr">(6.4)</a> can be rewritten using Equations <a href="linear-model-estimation.html#eq:def-residual-slr">(6.3)</a> and <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(6.2)</a> as
<span class="math display">\[\begin{align*}
RSS(\hat{\beta}_0, \hat{\beta}_1) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 &amp; \\
&amp;= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
&amp;= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{align*}\]</span></p>
<p>The <strong>fitted model</strong> is the estimated model that minimizes the RSS, i.e., the fitted model (in the context of simple linear regression) is defined as
<span class="math display" id="eq:def-fitted-model-slr">\[\begin{equation}
\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X. \tag{6.5}
\end{equation}\]</span>
In a simple linear regression context, the fitted model is known as the
<strong>line of best fit</strong>.</p>
<p>In Figure <a href="linear-model-estimation.html#fig:rss-viz2">6.3</a>, we visualize the response values, fitted
values, residuals, and fitted model in a simple linear regression
context. Note that:</p>
<ul>
<li>The fitted model is shown as the dashed grey line and minimizes the RSS.</li>
<li>The fitted values, shown as blue x’s, are the values returned by evaluating the fitted
model at the observed regressor values.</li>
<li>The residuals, shown as solid orange lines, indicate the distance and direction between the observed responses and their corresponding fitted value. If the response is larger than the fitted value then the residual is positive, otherwise it is negative.</li>
<li>The RSS is the sum of the squared vertical distances between the
response and fitted values.</li>
</ul>
<div class="figure"><span style="display:block;" id="fig:rss-viz2"></span>
<img src="Data-Analysis-with-Linear-Regression_files/figure-html/rss-viz2-1.png" alt="Visualization of the response values, fitted values, residuals, and fitted model." width="672" />
<p class="caption">
Figure 6.3: Visualization of the response values, fitted values, residuals, and fitted model.
</p>
</div>
</div>
<div id="ols-estimators-of-the-simple-linear-regression-parameters" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> OLS estimators of the simple linear regression parameters<a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the RSS for a
simple linear regression model can be obtained analytically using basic
calculus under minimal assumptions. Specifically, the optimal analytical
solutions for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are valid as long as
the regressor values are not a constant value, i.e, <span class="math inline">\(x_i \neq x_j\)</span> for
at least some <span class="math inline">\(i,j\in \{1,2,\ldots,n\}\)</span>.</p>
<p>Define <span class="math inline">\(\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i\)</span> and
<span class="math inline">\(\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i\)</span>. The OLS estimators of the
simple linear regression coefficients are</p>
<p><span class="math display" id="eq:slr-beta1hat">\[\begin{align}
\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n x_i Y_i - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)\biggl(\sum_{i=1}^n Y_i\biggr)}{\sum_{i=1}^n x_i^2 - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)^2} \notag \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \notag \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})Y_i}{\sum_{i=1}^n (x_i - \bar{x})x_i} \tag{6.6}
\end{align}\]</span>
and
<span class="math display" id="eq:slr-beta0hat">\[\begin{equation}
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}. \tag{6.7}
\end{equation}\]</span></p>
<p>We emphasize once again that the OLS estimators of <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span> are the estimators that minimize the RSS.</p>
<p>In edition to the regression coefficients, the other parameter we
discussed (in Section <a href="linear-model-estimation.html#ss-necessary-components">6.2.1</a>) is the error
variance, <span class="math inline">\(\sigma^2\)</span>. The most common estimator of the error variance is
<span class="math display" id="eq:sigmasq-hat">\[\begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-p}, \tag{6.8}
\end{equation}\]</span>
where <span class="math inline">\(p\)</span> is the number of regression coefficients. In
general, <span class="math inline">\(n-p\)</span> is the degrees of freedom of the RSS. In a simple linear
regression context, the denominator of Equation <a href="linear-model-estimation.html#eq:sigmasq-hat">(6.8)</a> is <span class="math inline">\(n-2\)</span>.</p>
</div>
</div>
<div id="s:penguins-slr" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Penguins simple linear regression example<a href="linear-model-estimation.html#s:penguins-slr" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use the <code>penguins</code> data set in the <strong>palmerpenguins</strong> package
<span class="citation">(<a href="#ref-R-palmerpenguins" role="doc-biblioref">Horst, Hill, and Gorman 2020</a>)</span> to illustrate a very basic simple linear regression
analysis.</p>
<p>The <code>penguins</code> data set provides data related to various penguin species
measured in the Palmer Archipelago (Antarctica), originally provided by
<span class="citation"><a href="#ref-GormanEtAl2014" role="doc-biblioref">Gorman, Williams, and Fraser</a> (<a href="#ref-GormanEtAl2014" role="doc-biblioref">2014</a>)</span>. We start by loading the data into memory.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="linear-model-estimation.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span></code></pre></div>
<p>The data set includes 344 observations of
8 variables. The variables are:</p>
<ul>
<li><code>species</code>: a <code>factor</code> indicating the penguin species</li>
<li><code>island</code>: a <code>factor</code> indicating the island the penguin was observed</li>
<li><code>bill_length_mm</code>: a <code>numeric</code> variable indicating the bill length in
millimeters</li>
<li><code>bill_depth_mm</code>: a <code>numeric</code> variable indicating the bill depth in
millimeters</li>
<li><code>flipper_length_mm</code>: an <code>integer</code> variable indicating the flipper
length in millimeters</li>
<li><code>body_mass_g</code>: an <code>integer</code> variable indicating the body mass in
grams</li>
<li><code>sex</code>: a <code>factor</code> indicating the penguin sex (<code>female</code>, <code>male</code>)</li>
<li><code>year</code>: an integer denoting the study year the penguin was observed
(<code>2007</code>, <code>2008</code>, or <code>2009</code>)</li>
</ul>
<p>We begin by creating a scatter plot of <code>bill_length_mm</code> versus
<code>body_mass_g</code> (y-axis versus x-axis) in Figure <a href="linear-model-estimation.html#fig:penguin-plot-2">6.4</a>.
We see a clear positive association between body mass and bill length:
as the body mass increases, the bill length tends to increase. The
pattern is linear, i.e., roughly a straight line.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-model-estimation.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins,</span>
<span id="cb46-2"><a href="linear-model-estimation.html#cb46-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>,</span>
<span id="cb46-3"><a href="linear-model-estimation.html#cb46-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:penguin-plot-2"></span>
<img src="Data-Analysis-with-Linear-Regression_files/figure-html/penguin-plot-2-1.png" alt="A scatter plot of penguin bill length (mm) versus body mass (g)" width="672" />
<p class="caption">
Figure 6.4: A scatter plot of penguin bill length (mm) versus body mass (g)
</p>
</div>
<p>We first perform a single linear regression analysis manually using the
equations previously provided by regressing <code>bill_length_mm</code> on
<code>body_mass_g</code>.</p>
<p>Using the <code>summary</code> function on the <code>penguins</code> data frame, we see that
both <code>bill_length_mm</code> and <code>body_mass_g</code> have <code>NA</code> values.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="linear-model-estimation.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(penguins)</span>
<span id="cb47-2"><a href="linear-model-estimation.html#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       species          island    bill_length_mm  bill_depth_mm  </span></span>
<span id="cb47-3"><a href="linear-model-estimation.html#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  </span></span>
<span id="cb47-4"><a href="linear-model-estimation.html#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  </span></span>
<span id="cb47-5"><a href="linear-model-estimation.html#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  </span></span>
<span id="cb47-6"><a href="linear-model-estimation.html#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                                  Mean   :43.92   Mean   :17.15  </span></span>
<span id="cb47-7"><a href="linear-model-estimation.html#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                                  3rd Qu.:48.50   3rd Qu.:18.70  </span></span>
<span id="cb47-8"><a href="linear-model-estimation.html#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                                  Max.   :59.60   Max.   :21.50  </span></span>
<span id="cb47-9"><a href="linear-model-estimation.html#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                                  NA&#39;s   :2       NA&#39;s   :2      </span></span>
<span id="cb47-10"><a href="linear-model-estimation.html#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  flipper_length_mm  body_mass_g       sex           year     </span></span>
<span id="cb47-11"><a href="linear-model-estimation.html#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  </span></span>
<span id="cb47-12"><a href="linear-model-estimation.html#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  </span></span>
<span id="cb47-13"><a href="linear-model-estimation.html#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  </span></span>
<span id="cb47-14"><a href="linear-model-estimation.html#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Mean   :200.9     Mean   :4202                Mean   :2008  </span></span>
<span id="cb47-15"><a href="linear-model-estimation.html#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  </span></span>
<span id="cb47-16"><a href="linear-model-estimation.html#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  Max.   :231.0     Max.   :6300                Max.   :2009  </span></span>
<span id="cb47-17"><a href="linear-model-estimation.html#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  NA&#39;s   :2         NA&#39;s   :2</span></span></code></pre></div>
<p>We want to remove the rows of <code>penguins</code> where either <code>body_mass_g</code> or
<code>bill_length_mm</code> have <code>NA</code> values. We do that below using the <code>na.omit</code>
function (selecting only the relevant variables) and assign the cleaned
object the name <code>penguins_clean</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="linear-model-estimation.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># remove rows of penguins where bill_length_mm or body_mass_g have NA values </span></span>
<span id="cb48-2"><a href="linear-model-estimation.html#cb48-2" aria-hidden="true" tabindex="-1"></a>penguins_clean <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(penguins[,<span class="fu">c</span>(<span class="st">&quot;bill_length_mm&quot;</span>, <span class="st">&quot;body_mass_g&quot;</span>)])</span></code></pre></div>
<p>We extract the <code>bill_length_mm</code> variable from the <code>penguins</code> data frame
and assign it the name <code>y</code> since it will be the response variable. We
extract the <code>body_mass_g</code> variable from the <code>penguins</code> data frame and
assign it the name <code>y</code> since it will be the predictor variable. We also
determine the number of observations and assign that value the name <code>n</code>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="linear-model-estimation.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract response and predictor from penguins_clean</span></span>
<span id="cb49-2"><a href="linear-model-estimation.html#cb49-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>bill_length_mm</span>
<span id="cb49-3"><a href="linear-model-estimation.html#cb49-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> penguins_clean<span class="sc">$</span>body_mass_g</span>
<span id="cb49-4"><a href="linear-model-estimation.html#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># determine number of observations</span></span>
<span id="cb49-5"><a href="linear-model-estimation.html#cb49-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span></code></pre></div>
<p>We now compute <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>. Note that placing
<code>()</code> around the assignment operations will both perform the assign and
print the results.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="linear-model-estimation.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compute OLS estimates of beta1 and beta0</span></span>
<span id="cb50-2"><a href="linear-model-estimation.html#cb50-2" aria-hidden="true" tabindex="-1"></a>(b1 <span class="ot">&lt;-</span> (<span class="fu">sum</span>(x <span class="sc">*</span> y) <span class="sc">-</span> <span class="fu">sum</span>(x) <span class="sc">*</span> <span class="fu">sum</span>(y) <span class="sc">/</span> n)<span class="sc">/</span>(<span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>) <span class="sc">-</span> <span class="fu">sum</span>(x)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n))</span>
<span id="cb50-3"><a href="linear-model-estimation.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.004051417</span></span>
<span id="cb50-4"><a href="linear-model-estimation.html#cb50-4" aria-hidden="true" tabindex="-1"></a>(b0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> b1 <span class="sc">*</span> <span class="fu">mean</span>(x))        </span>
<span id="cb50-5"><a href="linear-model-estimation.html#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 26.89887</span></span></code></pre></div>
<p>The estimated value of <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(\hat{\beta}_0=26.90\)</span> and the
estimated value of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat{\beta}_1=0.004\)</span>. The basic
mathematical interpretation of our results is that:</p>
<ul>
<li>(<span class="math inline">\(\hat{\beta}_1\)</span>): If a penguin has a body mass 1 gram larger than
another penguin, we expect the larger penguins bill length to be
0.004 millimeters longer.</li>
<li>(<span class="math inline">\(\hat{\beta}_0\)</span>):A penguin with a body mass of 0 grams is expected
to have a bill length of 26.9 millimeters.</li>
</ul>
<p>The latter interpretation is clearly non-sensical and is caused by the
fact that we are extrapolating far outside the observed body mass
values. The relationship between body mass and bill length is different
for penguin chicks versus adults.</p>
<p>We can use the <code>abline</code> function to overlay the fitted model on the
observed data. Note that in simple linear regression, <span class="math inline">\(\hat{\beta}_1\)</span>
corresponds to the slope of the fitted line and <span class="math inline">\(\hat{\beta}_0\)</span> will be
the intercept.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="linear-model-estimation.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bill_length_mm <span class="sc">~</span> body_mass_g, <span class="at">data =</span> penguins,</span>
<span id="cb51-2"><a href="linear-model-estimation.html#cb51-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;bill length (mm)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;body mass (g)&quot;</span>,</span>
<span id="cb51-3"><a href="linear-model-estimation.html#cb51-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Penguin size measurements&quot;</span>)</span>
<span id="cb51-4"><a href="linear-model-estimation.html#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="co"># a is the intercept and b is the slope</span></span>
<span id="cb51-5"><a href="linear-model-estimation.html#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> b0, <span class="at">b =</span> b1)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p>The fit of the model to our observed data seems reasonable.</p>
<p>We can also compute the residuals,
<span class="math inline">\(\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n\)</span>, the fitted values
<span class="math inline">\(\hat{y}_1,\ldots,\hat{y}_n\)</span>, and the associated RSS,
<span class="math inline">\(RSS=\sum_{i=1}^n \hat{\epsilon}_i^2\)</span>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="linear-model-estimation.html#cb52-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x <span class="co"># compute fitted values</span></span>
<span id="cb52-2"><a href="linear-model-estimation.html#cb52-2" aria-hidden="true" tabindex="-1"></a>ehat <span class="ot">&lt;-</span> y <span class="sc">-</span> yhat <span class="co"># compute residuals</span></span>
<span id="cb52-3"><a href="linear-model-estimation.html#cb52-3" aria-hidden="true" tabindex="-1"></a>(rss <span class="ot">&lt;-</span> <span class="fu">sum</span>(ehat<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># sum of the squared residuals</span></span>
<span id="cb52-4"><a href="linear-model-estimation.html#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 6564.494</span></span>
<span id="cb52-5"><a href="linear-model-estimation.html#cb52-5" aria-hidden="true" tabindex="-1"></a>(sigmasqhat <span class="ot">&lt;-</span> rss<span class="sc">/</span>(n<span class="dv">-2</span>)) <span class="co"># estimated error variance</span></span>
<span id="cb52-6"><a href="linear-model-estimation.html#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 19.30734</span></span></code></pre></div>
</div>
<div id="estimation-of-the-multiple-linear-regression-coefficients" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Estimation of the multiple linear regression coefficients)<a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now consider the context where we want to estimate the parameters of
a linear model with 1 or more regressors, i.e., when
<span class="math display">\[Y=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1} + \epsilon.\]</span></p>
<p>The multiple linear regression model relating the responses, the regressors, and the errors for all <span class="math inline">\(n\)</span> observations is defined by the system of equations
<span class="math display" id="eq:lmSystem">\[\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
\tag{5.2}
\end{equation}\]</span></p>
<div id="using-matrix-notation-to-represent-a-linear-model" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Using matrix notation to represent a linear model<a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To simplify estimation of the regression coefficients in a linear
regression model, we must use matrix notation to describe the system of
equation defining our linear model.</p>
<p>We define the following notation:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]\)</span> denotes the column vector
containing the <span class="math inline">\(n\)</span> responses.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> denotes the matrix containing a column of 1s and the
observed regressor values, specifically,
<span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p-1} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p-1}
\end{bmatrix}.\]</span></li>
<li><span class="math inline">\(\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]\)</span>
denotes the column vector containing the <span class="math inline">\(p\)</span> regression
coefficients.</li>
<li><span class="math inline">\(\boldsymbol{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]\)</span>
denotes the column vector contained the <span class="math inline">\(n\)</span> errors.</li>
</ul>
<p>Then the system of equations defining the linear model in
<a href="defining-and-fitting-a-linear-model.html#eq:lmSystem">(5.2)</a> can be written as
<span class="math display">\[\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}.\]</span>
Thus, a linear model can be represented as a system of linear equations
using matrices. A model that cannot be represented as a system of linear
equations using matrices is not a linear model.</p>
</div>
<div id="ss:fv-resid-rss-mlr" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Residuals, fitted values, and RSS for multiple linear regression<a href="linear-model-estimation.html#ss:fv-resid-rss-mlr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We proceed with discussion of residuals, fitted values, and RSS for the multiple linear regression context using matrix notation.</p>
<p>The vector of estimated values for the coefficients contained in <span class="math inline">\(\boldsymbol{\beta}\)</span> is denoted by
<span class="math display" id="eq:def-beta-matrix">\[\begin{equation}
\hat{\boldsymbol{\beta}}=[\hat{\beta}_0,\hat{\beta}_1,\ldots,\hat{\beta}_{p-1}]. \tag{6.9}
\end{equation}\]</span></p>
<p>The vector of regressor values for the <span class="math inline">\(i\)</span>th observation is denoted by
<span class="math display" id="eq:def-ith-regressor-matrix">\[\begin{equation}
\mathbf{x}_i=[1,x_{i,1},\ldots,x_{i,p-1}], \tag{6.10}
\end{equation}\]</span>
where the 1 is needed to account for the intercept in our model.</p>
<p>Extending the original definition of a fitted value in Equation <a href="linear-model-estimation.html#eq:def-fitted-value-slr">(6.2)</a>, the <span class="math inline">\(i\)</span>th fitted value in the context of multiple linear regression is defined as
<span class="math display" id="eq:def-fitted-value-matrix">\[\begin{align}
\hat{Y}_i &amp;= \hat{E}(Y|\mathbb{X} = \mathbf{x}_i) \notag \\
&amp;= \hat{\beta}_0 + \hat{\beta}_1 x_{i,1} + \cdots + \hat{\beta}_{p-1} x_{i,p-1} \notag \\
&amp;= \mathbf{x}_i^T\hat{\boldsymbol{\beta}}. \tag{6.11}
\end{align}\]</span></p>
<p>The (column) vector of fitted values is defined as
<span class="math display" id="eq:def-fitted-values-matrix">\[\begin{equation}
\hat{\mathbf{y}} = [\hat{Y}_1,\ldots,\hat{Y}_n]. \tag{6.12}
\end{equation}\]</span></p>
<p>Extending the original definition of a residual in Equation <a href="linear-model-estimation.html#eq:def-residual-slr">(6.3)</a>,
the <span class="math inline">\(i\)</span>th residual in the context of multiple linear regression can be written as
<span class="math display" id="eq:lmdef" id="eq:sigmasq-hat" id="eq:slr-beta1hat" id="eq:slr-beta0hat" id="eq:slr-beta1hat" id="eq:betahat">\[\begin{align}
\hat{\epsilon}_i = Y_i - \hat{Y}_i=Y_i-\mathbf{x}_i^T\hat{\boldsymbol{\beta}},
\end{equation}
using Equation \@ref(eq:def-fitted-value-matrix).

The RSS for a simple linear regression model, as a function of the
estimated regression coefficients, is \begin{align*}
RSS(\hat{\beta}_0, \hat{\beta}_1) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
 &amp;= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{align*}

### OLS estimator of the linear model parameters

The OLS estimator of the regression coefficient vector,
$\boldsymbol{\beta}$, is

```{=tex}
\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}, \tag{6.13}
\end{equation}
```
## Penguins multiple linear regression example {#s:penguins-mlr}


The data set includes 344 observations of
8 variables. The variables are:

-   `species`: a `factor` indicating the penguin species
-   `island`: a `factor` indicating the island the penguin was observed
-   `bill_length_mm`: a `numeric` variable indicating the bill length in
    millimeters
-   `bill_depth_mm`: a `numeric` variable indicating the bill depth in
    millimeters
-   `flipper_length_mm`: an `integer` variable indicating the flipper
    length in millimeters
-   `body_mass_g`: an `integer` variable indicating the body mass in
    grams
-   `sex`: a `factor` indicating the penguin sex (`female`, `male`)
-   `year`: an integer denoting the study year the penguin was observed
    (`2007`, `2008`, or `2009`)
    

```r
mlmod &lt;- lm(bill_length_mm ~ body_mass_g + flipper_length_mm, data = penguins)
summary(mlmod)
#&gt; 
#&gt; Call:
#&gt; lm(formula = bill_length_mm ~ body_mass_g + flipper_length_mm, 
#&gt;     data = penguins)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -8.8064 -2.5898 -0.7053  1.9911 18.8288 
#&gt; 
#&gt; Coefficients:
#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)       -3.4366939  4.5805532  -0.750    0.454    
#&gt; body_mass_g        0.0006622  0.0005672   1.168    0.244    
#&gt; flipper_length_mm  0.2218655  0.0323484   6.859 3.31e-11 ***
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 4.124 on 339 degrees of freedom
#&gt;   (2 observations deleted due to missingness)
#&gt; Multiple R-squared:  0.4329, Adjusted R-squared:  0.4295 
#&gt; F-statistic: 129.4 on 2 and 339 DF,  p-value: &lt; 2.2e-16
```

## Categorical predictors

## Penguins multiple linear regression example with categorical predictor {#s:penguins-mlr}


```r
library(ggplot2)
ggplot(data = penguins) + geom_point(aes(x = body_mass_g, y = bill_length_mm, col = species))
#&gt; Warning: Removed 2 rows containing missing values (geom_point).
```

&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-55-1.png&quot; width=&quot;672&quot; /&gt;



```r
lmodb &lt;- lm(bill_length_mm ~ body_mass_g + species + body_mass_g:species, data = penguins)
summary(lmodb)
#&gt; 
#&gt; Call:
#&gt; lm(formula = bill_length_mm ~ body_mass_g + species + body_mass_g:species, 
#&gt;     data = penguins)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -6.4208 -1.6461  0.0919  1.4718  9.3138 
#&gt; 
#&gt; Coefficients:
#&gt;                                Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)                  26.9941391  1.5926015  16.950  &lt; 2e-16 ***
#&gt; body_mass_g                   0.0031879  0.0004271   7.464 7.27e-13 ***
#&gt; speciesChinstrap              5.1800537  3.2746719   1.582    0.115    
#&gt; speciesGentoo                -0.2545907  2.7138655  -0.094    0.925    
#&gt; body_mass_g:speciesChinstrap  0.0012748  0.0008740   1.459    0.146    
#&gt; body_mass_g:speciesGentoo     0.0009030  0.0006066   1.489    0.138    
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 2.399 on 336 degrees of freedom
#&gt;   (2 observations deleted due to missingness)
#&gt; Multiple R-squared:  0.8098, Adjusted R-squared:  0.807 
#&gt; F-statistic: 286.1 on 5 and 336 DF,  p-value: &lt; 2.2e-16
```


```r
lmodc &lt;- lm(bill_length_mm ~ body_mass_g*species, data = penguins)
coefficients(lmodb)
#&gt;                  (Intercept)                  body_mass_g 
#&gt;                26.9941391367                 0.0031878758 
#&gt;             speciesChinstrap                speciesGentoo 
#&gt;                 5.1800537287                -0.2545906615 
#&gt; body_mass_g:speciesChinstrap    body_mass_g:speciesGentoo 
#&gt;                 0.0012748183                 0.0009029956
coefficients(lmodc)
#&gt;                  (Intercept)                  body_mass_g 
#&gt;                26.9941391367                 0.0031878758 
#&gt;             speciesChinstrap                speciesGentoo 
#&gt;                 5.1800537287                -0.2545906615 
#&gt; body_mass_g:speciesChinstrap    body_mass_g:speciesGentoo 
#&gt;                 0.0012748183                 0.0009029956
```

## Evaluating model fit

## Summary of notation

## Summary of functions used in this chapter

## Summarizing the components of a linear model

We have already introduced a lot of objects. To aid in making sense of
their notation, their purpose in the model, whether they can be
observed, and whether they are modeled as a random variable (vector) or
fixed, non-random values, we summarize things below.

We&#39;ve already talked about observing the response variable and the
predictor variables. So these objects are observable. However, we have
no way to measure the regression coefficients or the error. These are
not observable.

On the other hand, we treat the response variable as a random variable.
Perhaps surprisingly, we treated the predictor variables as a fixed,
non-random variables. The regression coefficients are treated as fixed,
non-random but unknown values. This is standard for parameters in a
statistical model. The errors are also treated as random variables. In
fact, since both the predictor variables and the regression coefficients
are non-random, the only way for the response to be a random variable
based on Equation \@ref(eq:lmSystem) is for the errors to be random.

We summarize this information in the table below for the objects
previously discussed using the various notations introduced.

+-----------------+-----------------+-----------------+-----------------+
| Notation        | Description     | Observable      | Random          |
+=================+=================+=================+=================+
| $Y$             | response        | Yes             | Yes             |
|                 | variable        |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $Y_i$           | response value  | Yes             | Yes             |
|                 | for the $i$th   |                 |                 |
|                 | observation     |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $\mathbf{y}$    | the $n\times 1$ | Yes             | Yes             |
|                 | column vector   |                 |                 |
|                 | of response     |                 |                 |
|                 | values          |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $X$             | regressor       | Yes             | No              |
|                 | variable        |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $X_j$           | the $j$th       | Yes             | No              |
|                 | regressor       |                 |                 |
|                 | variable        |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $x_{i,j}$       | the value of    | Yes             | No              |
|                 | the $j$th       |                 |                 |
|                 | regressor       |                 |                 |
|                 | variable for    |                 |                 |
|                 | the $i$th       |                 |                 |
|                 | observation     |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $\mathbf{X}$    | the $n\times p$ | Yes             | No              |
|                 | matrix of       |                 |                 |
|                 | regressor       |                 |                 |
|                 | values          |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $\beta_j$       | the regression  | No              | No              |
|                 | coefficient     |                 |                 |
|                 | associated with |                 |                 |
|                 | the $j$th       |                 |                 |
|                 | regressor       |                 |                 |
|                 | variable        |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $\boldsymbol{\beta}$| the $p\times 1$ | No              | No          |
|                 | column vector   |                 |                 |
|                 | of regression   |                 |                 |
|                 | coefficients    |                 |                 |
+-----------------+-----------------+-----------------+-----------------+
| $\epsilon$      | the error       | No              | Yes             |
+-----------------+-----------------+-----------------+-----------------+
| $\epsilon_i$    | the error       | No              | Yes             |
|                 | associated with |                 |                 |
|                 | observation $i$ |                 |                 |
+-----------------+-----------------+-----------------+-----------------+

## Going Deeper

### Manually estimating the simple linear regression coefficients

In this section we will manually perform the Penguins simple linear regression
analysis provided in Section \@ref{s:penguins-slr}.

Recall that the `penguins` data frame in the **palmerpenguins* package contained
the variables:

*   `bill_length_mm`: a `numeric` variable indicating the bill length in
    millimeters
*   `body_mass_g`: an `integer` variable indicating the body mass in
    grams

We will use formulas for $\hat{\beta}_0$ and $\hat{\beta}_1$ in 
Equations \@ref\tag{6.6}
and \@ref\tag{6.7} to manually compute the estimated regression 
coefficients for the simple linear regression model when 
regressing `bill_length_mm` on `body_mass_g`.

We first load the `penguins` data frame. We then select the `bill_length_mm` and `body_mass_g` variables of the data frame and use the pipe operator to pass the simplified data frame to the `summary` function. 



```r
data(penguins, package = &quot;palmerpenguins&quot;) # load data
penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)] |&gt; summary() # simplify and summarize
#&gt;  bill_length_mm   body_mass_g  
#&gt;  Min.   :32.10   Min.   :2700  
#&gt;  1st Qu.:39.23   1st Qu.:3550  
#&gt;  Median :44.45   Median :4050  
#&gt;  Mean   :43.92   Mean   :4202  
#&gt;  3rd Qu.:48.50   3rd Qu.:4750  
#&gt;  Max.   :59.60   Max.   :6300  
#&gt;  NA&#39;s   :2       NA&#39;s   :2
```

Both `bill_length_mm` and `body_mass_g` have `NA` values that will poison our calculations if we naively use those variables in our calculations, so we must remove them prior to calculation.

The `na.omit` function attempts to handle missing values in R objects. The `penguins` data frame has class `data.frame`. For a `data.frame`, `na.omit` will remove any rows that have `NA` values.  Note that if you are only concerned with the `NA` values for certain variables of a data frame then you should apply the `na.omit` to the data frame containing only those variables, otherwise `na.omit` may remove rows unnecessarily because a different variable has an `NA` value.

Compare the dimensions of the results when we apply `na.omit` to `penguins` versus
`penguins` with only `bill_length_mm` and `body_mass_g`.


```r
penguins |&gt; na.omit() |&gt; dim() # dimensions penguins after filtering rows with NA
#&gt; [1] 333   8
penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)] |&gt; na.omit() |&gt; dim()
#&gt; [1] 342   2
```

The data frame obtained after applying `na.omit` to the `penguins`
data frame has only 333 rows, while the data frame obtained by applying `na.omit`
to only the `bill_length_mm` and `body_mass_g` columns of `penguins` has 342 rows. This
is because some of the other variables in `penguins` have `NA` values in rows that
the `bill_length_mm` and `body_mass_g` columns do not, so the `na.omit` function must remove
additional rows.

Continuing our analysis, we create a new data frame, `penguins_clean`, that is 
obtained by selecting the `bill_length_mm` and `body_mass_g` variables of `penguins` and
then using the `na.omit` function to retain only the rows without `NA` values.


```r
# remove rows of penguins where bill_length_mm or body_mass_g have NA values 
penguins_clean &lt;- na.omit(penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)])
```

We extract the `bill_length_mm` variable from the `penguins_clean` data frame
and assign it the name `y` since it will be the response variable. We
extract the `body_mass_g` variable from the `penguins` data frame and
assign it the name `x` since it will be the regressor variable. We also
determine the number of observations and assign that value the name `n`.


```r
# extract response and regressor from penguins_clean
y &lt;- penguins_clean$bill_length_mm
x &lt;- penguins_clean$body_mass_g
# determine number of observations
n &lt;- length(y)
```

We now compute $\hat{\beta}_1$ and $\hat{\beta}_0$ using the formulas in Equations \@ref\tag{6.6} and \@ref\tag{6.7}, respectively. Note that placing
`()` around the assignment operations will both perform the assignment and
print the results.


```r
# compute OLS estimates of beta1 and beta0
(b1 &lt;- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n))
#&gt; [1] 0.004051417
(b0 &lt;- mean(y) - b1 * mean(x))        
#&gt; [1] 26.89887
```

The estimated value of $\beta_0$ is $\hat{\beta}_0=$0 and the
estimated value of $\beta_1$ is $\hat{\beta}_1=0.004$. The basic
mathematical interpretation of our results is that:

-   $\hat{\beta}_1$: If a penguin has a body mass 1 gram larger than
    another penguin, we expect the larger penguins bill length to be
    0.004 millimeters longer.
-   $\hat{\beta}_0$:A penguin with a body mass of 0 grams is expected
    to have a bill length of 26.9 millimeters.

The latter interpretation is clearly non-sensical and is caused by the
fact that we are extrapolating far outside the observed body mass
values. The relationship between body mass and bill length is different
for penguin chicks versus adults.

We can use the `abline` function to overlay the fitted model on the
observed data. Note that in simple linear regression, $\hat{\beta}_1$
corresponds to the slope of the fitted line and $\hat{\beta}_0$ will be
the intercept.


```r
plot(bill_length_mm ~ body_mass_g, data = penguins,
     ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;,
     main = &quot;Penguin size measurements&quot;)
# a is the intercept and b is the slope
abline(a = b0, b = b1)
```

&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-63-1.png&quot; width=&quot;672&quot; /&gt;

The fit of the model to our observed data seems reasonable.

We can also compute the residuals,
$\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n$, the fitted values
$\hat{y}_1,\ldots,\hat{y}_n$, and the associated RSS,
$RSS=\sum_{i=1}^n \hat{\epsilon}_i^2$.


```r
yhat &lt;- b0 + b1 * x # compute fitted values
ehat &lt;- y - yhat # compute residuals
(rss &lt;- sum(ehat^2)) # sum of the squared residuals
#&gt; [1] 6564.494
(sigmasqhat &lt;- rss/(n-2)) # estimated error variance
#&gt; [1] 19.30734
```

### Manually estimating the multiple linear regression coefficients

### Parameter estimation and matrix decompositions

### Updating a model

### More discussion of `formula` for model-building

The models fit by, e.g., the lm and glm functions are specified in a
compact symbolic form. The \~ operator is basic in the formation of such
models. An expression of the form y \~ model is interpreted as a
specification that the response y is modelled by a linear predictor
specified symbolically by model. Such a model consists of a series of
terms separated by + operators. The terms themselves consist of variable
and factor names separated by : operators. Such a term is interpreted as
the interaction of all the variables and factors appearing in the term.

In addition to + and :, a number of other operators are useful in model
formulae. The \* operator denotes factor crossing: a*b interpreted as
a+b+a:b. The \^ operator indicates crossing to the specified degree. For
example (a+b+c)\^2 is identical to (a+b+c)*(a+b+c) which in turn expands
to a formula containing the main effects for a, b and c together with
their second-order interactions. The %in% operator indicates that the
terms on its left are nested within those on the right. For example a +
b %in% a expands to the formula a + a:b. The - operator removes the
specified terms, so that (a+b+c)\^2 - a:b is identical to a + b + c +
b:c + a:c. It can also used to remove the intercept term: when fitting a
linear model y \~ x - 1 specifies a line through the origin. A model
with no intercept can be also specified as y \~ x + 0 or y \~ 0 + x.

While formulae usually involve just variable and factor names, they can
also involve arithmetic expressions. The formula log(y) \~ a + log(x) is
quite legal. When such arithmetic expressions involve operators which
are also used symbolically in model formulae, there can be confusion
between arithmetic and symbolic operator use.

To avoid this confusion, the function I() can be used to bracket those
portions of a model formula where the operators are used in their
arithmetic sense. For example, in the formula y \~ a + I(b+c), the term
b+c is to be interpreted as the sum of b and c.

Variable names can be quoted by backticks `like this` in formulae,
although there is no guarantee that all code using formulae will accept
such non-syntactic names.

Most model-fitting functions accept formulae with right-hand-side
including the function offset to indicate terms with a fixed coefficient
of one. Some functions accept other &#39;specials&#39; such as strata or cluster
(see the specials argument of terms.formula).

There are two special interpretations of . in a formula. The usual one
is in the context of a data argument of model fitting functions and
means &#39;all columns not otherwise in the formula&#39;: see terms.formula. In
the context of update.formula, only, it means &#39;what was previously in
this part of the formula&#39;.

When formula is called on a fitted model object, either a specific
method is used (such as that for class &quot;nls&quot;) or the default method. The
default first looks for a &quot;formula&quot; component of the object (and
evaluates it), then a &quot;terms&quot; component, then a formula parameter of the
call (and evaluates its value) and finally a &quot;formula&quot; attribute.

There is a formula method for data frames. When there&#39;s &quot;terms&quot;
attribute with a formula, e.g., for a model.frame(), that formula is
returned. If you&#39;d like the previous (R &lt;= 3.5.x) behavior, use the
auxiliary DF2formula() which does not consider a &quot;terms&quot; attribute.
Otherwise, if there is only one column this forms the RHS with an empty
LHS. For more columns, the first column is the LHS of the formula and
the remaining columns separated by + form the RHS.


&lt;!--chapter:end:05-linear-model-estimation.Rmd--&gt;

---
title: &quot;Joshua French&quot;
date: &quot;2022-06-10&quot;
output:
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

# Parameter estimation for linear models

In this chapter we focus on estimating the parameters of a linear
regression model. We also discuss important properties of the parameter
estimators.

To make the discussion easier to follow, we start by describing the
*simple* linear regression model, which is a linear model with only a
single predictor.

&lt;!-- Fitting a regression model is the same thing as estimating the parameters of a simple linear regression model.  --&gt;

There are many different methods of parameter estimation in statistics:
method-of-moments, maximum likelihood, Bayesian, etc. The most common
parameter estimation method for linear models is **least squares
method**, which is perhaps comonly called **Ordinary Least Squares
(OLS)** estimation. OLS estimation estimates the regression coefficients
with the values that minimize the residuals sum of squares (RSS), which
we will define shortly.

## OLS estimation of the simple linear regression model

In a simple linear regression context, we have $n$ observed responses
$Y_1,Y_2,\ldots,Y_n$ and $n$ predictor values $x_1,x_2,\ldots,x_n$.

Recall that for a simple linear regression model
$$Y = \beta_0 + \beta_1 X + \epsilon = E(Y|X) + \epsilon$$ with
$$E(Y|X) = \beta_0 + \beta_1 X.$$ We need to define some new notation
and objects to define the RSS.

Let $\hat{\beta}_j$ denote the estimated value of $\beta_j$ and the
estimated mean response as a function of the predictor $X$ is
$$\hat{E}(Y|X) = \hat{\beta}_0 + \hat{\beta}_1 X.$$

The $i$th fitted value is defined as
$$\hat{Y}_i = \hat{E}(Y|X = x_i) = \hat{\beta}_0 + \hat{\beta}_1 x_i.$$
Thus, the $i$th fitted value is the estimated mean of $Y$ when the
predictor $X=x_i$. More specifically, the $i$th fitted value is the
estimated mean response of the $i$th observation.

The $i$th residual is defined as $$\hat{\epsilon}_i = Y_i - \hat{Y}_i.$$
The $i$th residual is the difference between the response and estimated
mean response of observation $i$.

**The RSS of a regression model is the sum of its squared residuals**.

The RSS for a simple linear regression model, as a function of the
estimated regression coefficients, is \begin{align*}
RSS(\hat{\beta}_0, \hat{\beta}_1) &amp;= \sum_{i=1}^n \hat{\epsilon}_i^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 \\
&amp;= \sum_{i=1}^n (Y_i - \hat{E}(Y|X=x_i))^2 \\
 &amp;= \sum_{i=1}^n (Y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2.
\end{align*}

The **fitted model** is the estimated model that minimizes the RSS. In a
simple linear regression context, the fitted model is known as the
**line of best fit**.

In Figure \@ref(fig:rss-viz), we attempt to visualize the response
values, fitted values, residuals, and line of best fit in a simple
linear regression context. Notice that:

-   The fitted values are the value returned by the line of best fit
    when it is evaluated at the observed predictor values.
    Alternatively, the fitted value for each observation is the y-value
    obtained when intersecting the line of best fit with a vertical line
    drawn from each observed predictor value.
-   The residual is the vertical distance between each response value
    and the fitted value.
-   The RSS seeks to minimize the sum of the squared vertical distances
    between the response and fitted values.

&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/rss-viz-1.png&quot; alt=&quot;Visualization of the response values, fitted values, residuals, and line of best fit.&quot; width=&quot;672&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:rss-viz)Visualization of the response values, fitted values, residuals, and line of best fit.&lt;/p&gt;
&lt;/div&gt;

### Visualizing the RSS as a function of the estimated coefficients

As we have attempted to emphasize through its notation,
$RSS(\hat{\beta}_0, \hat{\beta}_1)$ is a function of $\hat{\beta}_0$ and
$\hat{\beta}_1$. OLS estimation for the simple linear regression model
seeks to find the values of the estimated coefficients that minimize the
$RSS(\hat{\beta}_0, \hat{\beta}_1)$. In the example below, we visualize
this three-dimensional surface to see how difficult it would be to
optimize the RSS computationally .

Consider the Pearson and Lee&#39;s height data (`PearsonLee` in the
**HistData** package) previously discussed. For that data set, we tried
to model the child&#39;s height (`child`) based on the height of the child&#39;s
parents (`parent`). Thus, our response variable is `child` and our
predictor variable is `parent`. We seek to estimate the regression
equation
$$E(\mathtt{child} \mid \mathtt{parent}) = \beta_0 + \beta_1 \mathtt{parent}$$
with the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the
associated RSS.

We first load the height data, extract the response and predictor and
assign them the names `y` and `x`.


```r
# load height data
data(PearsonLee, package = &quot;HistData&quot;)
# extract response and predictor variables from data set
y &lt;- PearsonLee$child
x &lt;- PearsonLee$parent
```

We now create a function that computes the RSS as a function of
$\hat{\beta}_0$ and $\hat{\beta}_1$ (called `b0` and `b1`, respectively
in the code below). The function takes the vector `b = c(b0, b1)`,
extracts `b0` and `b1` from this vector, computes the fitted values
(`yhat`) for the provided `b0` and `b1`, computes the corresponding
residuals (`ehat`), and the returns the sum of the squared residuals,
i.e., the RSS.


```r
# function to compute the RSS
# b = c(b0, b1)
compute_rss &lt;- function(b) {
  b0 = b[1] # extract b0 from b
  b1 = b[2] # extract b1 from b
  yhat &lt;- b0 + b1 * x # compute fitted values
  ehat &lt;- y - yhat # compute residuals
  return(sum(ehat^2)) # return RSS
}
```

Next, we specify sequences of `b0` and `b1` values to consider for
optimizing the RSS. We create a matrix, `rss_mat` to store the computed
RSS for each combination of `b0` and `b1`. We then use a double `for`
loop to evaluate the RSS for each combination of `b0` and `b1` in our
sequences.


```r
# sequences of candidate b0 and b1 values
b0_seq &lt;- seq(41.06, 41.08, len = 101)
b1_seq &lt;- seq(0.383, 0.385, len = 101)
# matrix to store rss values
rss_mat &lt;- matrix(nrow = length(b0_seq), ncol = length(b1_seq))
# use double loop to compute RSS for all combinations of b0_seq and b1_seq
# seq_along(b0_seq) returns the vector 1:length(b0_seq), but is safer 
for (i in seq_along(b0_seq)) {
  for (j in seq_along(b1_seq)) {
    rss_mat[i, j] &lt;- compute_rss(c(b0_seq[i], b1_seq[j]))
  }
}
```

We draw a contour plot of the RSS surface using the `contour` function.


```r
# draw a contour plot of the RSS surface
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = &quot;b0&quot;, ylab = &quot;b1&quot;)
title(&quot;RSS surface of Pearson and Lee height data&quot;)
```

&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-68-1.png&quot; width=&quot;672&quot; /&gt;

A contour plot uses contour lines to describe the height of the $z$
dimension of a 3-dimensional $(x, y, z)$ surface. Each line/contour
indicates the height of the surface along that line. Note that in the
graphic above, the contours are basically straight lines. There&#39;s no
easily identifiable combinations of `b0` and `b1` the produce the
minimum RSS.

We can approximate the optimal values of `b0` and `b1` that minimize the
RSS through the `optim` function. The optim function takes two main
arguments:

-   `par`: a vector of starting values for the optimization algorithm.
    In our case, this will be the starting values for `b0` and `b1`.
-   `fn`: a function of `par` to minimize.

The `optim` function will return a list with several pieces of
information (see `?stats::optim`) for details. We want the `par`
component of the returned list, which is the `par` vector that
(approximately) minimizes `fn`. We then use the `points` function to
plot the &quot;optimal&quot; values of `b0` and `b1` that minimize the RSS.


```r
# use the optim function to find the values of b0 and b1 that minimize the RSS
# par is the vector of initial values
# fn is the function to minimize
# $par extracts the values found by optim to minimize fn
optimal_b &lt;- optim(par = c(41, 0.4), fn = compute_rss)$par
# print the optimal values of b
optimal_b
#&gt; [1] 41.0655877  0.3842737
# plot optimal value as an X on the contour plot
contour(x = b0_seq, y = b1_seq, z = rss_mat, xlab = &quot;b0&quot;, ylab = &quot;b1&quot;)
title(&quot;RSS surface of Pearson and Lee height data&quot;)
points(x = optimal_b[1], y = optimal_b[2], pch = 4)
```

&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-69-1.png&quot; width=&quot;672&quot; /&gt;

What is our takeaway from this example? It&#39;s probably not ideal to
numerically search for the values of $\hat{\beta}_0$ and $\hat{\beta}_1$
that minimize $RSS(\hat{\beta}_0$, $\hat{\beta}_1)$. Instead, we should
seek an exact solution using mathematics.

### OLS estimators of the simple linear regression parameters

Define $\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i$ and
$\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$.

The OLS estimators of the regression coefficients for a simple linear
regression coefficients are

\begin{align*}
\hat{\beta}_1 &amp;= \frac{\sum_{i=1}^n x_i Y_i - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)\biggl(\sum_{i=1}^n Y_i\biggr)}{\sum_{i=1}^n x_i^2 - \frac{1}{n} \biggl(\sum_{i=1}^n x_i\biggr)^2} \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
&amp;= \frac{\sum_{i=1}^n (x_i - \bar{x})Y_i}{\sum_{i=1}^n (x_i - \bar{x})x_i}
\end{align*} and \begin{equation}
\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{x}.
\end{equation}

Thought it&#39;s already been said, we state once again that the OLS
estimators of $\beta_0$ and $\beta_1$ shown above are the estimators
that minimize the RSS.

The other parameter we&#39;ve discussed is the error variance, $\sigma^2$.
The most common estimator of the error variance is \begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-p}, \tag{6.8}
\end{equation} where $p$ is the number of regression coefficients. In
general, $n-p$ is the degrees of freedom of the RSS. In a simple linear
regression context, the denominator of \@ref(eq:sigmasq-hat) is $n-2$.

## Penguins simple linear regression example

We will use the `penguins` data set in the **palmerpenguins** package
[@R-palmerpenguins] to illustrate a very basic simple linear regression
analysis.

The `penguins` data set provides data related to various penguin species
measured in the Palmer Archipelago (Antarctica), originally provided by
@GormanEtAl2014. We start by loading the data into memory.


```r
data(penguins, package = &quot;palmerpenguins&quot;)
```

The data set includes 344 observations of
8 variables. The variables are:

-   `species`: a `factor` indicating the penguin species
-   `island`: a `factor` indicating the island the penguin was observed
-   `bill_length_mm`: a `numeric` variable indicating the bill length in
    millimeters
-   `bill_depth_mm`: a `numeric` variable indicating the bill depth in
    millimeters
-   `flipper_length_mm`: an `integer` variable indicating the flipper
    length in millimeters
-   `body_mass_g`: an `integer` variable indicating the body mass in
    grams
-   `sex`: a `factor` indicating the penguin sex (`female`, `male`)
-   `year`: an integer denoting the study year the penguin was observed
    (`2007`, `2008`, or `2009`)

We begin by creating a scatter plot of `bill_length_mm` versus
`body_mass_g` (y-axis versus x-axis) in Figure \@ref(fig:penguin-plot).
We see a clear positive association between body mass and bill length:
as the body mass increases, the bill length tends to increase. The
pattern is linear, i.e., roughly a straight line.


```r
plot(bill_length_mm ~ body_mass_g, data = penguins,
     ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;,
     main = &quot;Penguin size measurements&quot;)
```

&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/penguin-plot-1.png&quot; alt=&quot;A scatter plot of penguin bill length (mm) versus body mass (g)&quot; width=&quot;672&quot; /&gt;
&lt;p class=&quot;caption&quot;&gt;(\#fig:penguin-plot)A scatter plot of penguin bill length (mm) versus body mass (g)&lt;/p&gt;
&lt;/div&gt;

We first perform a single linear regression analysis manually using the
equations previously provided by regressing `bill_length_mm` on
`body_mass_g`.

Using the `summary` function on the `penguins` data frame, we see that
both `bill_length_mm` and `body_mass_g` have `NA` values.


```r
summary(penguins)
#&gt;       species          island    bill_length_mm  bill_depth_mm  
#&gt;  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
#&gt;  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
#&gt;  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
#&gt;                                  Mean   :43.92   Mean   :17.15  
#&gt;                                  3rd Qu.:48.50   3rd Qu.:18.70  
#&gt;                                  Max.   :59.60   Max.   :21.50  
#&gt;                                  NA&#39;s   :2       NA&#39;s   :2      
#&gt;  flipper_length_mm  body_mass_g       sex           year     
#&gt;  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
#&gt;  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
#&gt;  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  
#&gt;  Mean   :200.9     Mean   :4202                Mean   :2008  
#&gt;  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
#&gt;  Max.   :231.0     Max.   :6300                Max.   :2009  
#&gt;  NA&#39;s   :2         NA&#39;s   :2
```

We want to remove the rows of `penguins` where either `body_mass_g` or
`bill_length_mm` have `NA` values. We do that below using the `na.omit`
function (selecting only the relevant variables) and assign the cleaned
object the name `penguins_clean`.


```r
# remove rows of penguins where bill_length_mm or body_mass_g have NA values 
penguins_clean &lt;- na.omit(penguins[,c(&quot;bill_length_mm&quot;, &quot;body_mass_g&quot;)])
```

We extract the `bill_length_mm` variable from the `penguins` data frame
and assign it the name `y` since it will be the response variable. We
extract the `body_mass_g` variable from the `penguins` data frame and
assign it the name `y` since it will be the predictor variable. We also
determine the number of observations and assign that value the name `n`.


```r
# extract response and predictor from penguins_clean
y &lt;- penguins_clean$bill_length_mm
x &lt;- penguins_clean$body_mass_g
# determine number of observations
n &lt;- length(y)
```

We now compute $\hat{\beta}_1$ and $\hat{\beta}_0$. Note that placing
`()` around the assignment operations will both perform the assign and
print the results.


```r
# compute OLS estimates of beta1 and beta0
(b1 &lt;- (sum(x * y) - sum(x) * sum(y) / n)/(sum(x^2) - sum(x)^2/n))
#&gt; [1] 0.004051417
(b0 &lt;- mean(y) - b1 * mean(x))        
#&gt; [1] 26.89887
```

The estimated value of $\beta_0$ is $\hat{\beta}_0=26.90$ and the
estimated value of $\beta_1$ is $\hat{\beta}_1=0.004$. The basic
mathematical interpretation of our results is that:

-   ($\hat{\beta}_1$): If a penguin has a body mass 1 gram larger than
    another penguin, we expect the larger penguins bill length to be
    0.004 millimeters longer.
-   ($\hat{\beta}_0$):A penguin with a body mass of 0 grams is expected
    to have a bill length of 26.9 millimeters.

The latter interpretation is clearly non-sensical and is caused by the
fact that we are extrapolating far outside the observed body mass
values. The relationship between body mass and bill length is different
for penguin chicks versus adults.

We can use the `abline` function to overlay the fitted model on the
observed data. Note that in simple linear regression, $\hat{\beta}_1$
corresponds to the slope of the fitted line and $\hat{\beta}_0$ will be
the intercept.


```r
plot(bill_length_mm ~ body_mass_g, data = penguins,
     ylab = &quot;bill length (mm)&quot;, xlab = &quot;body mass (g)&quot;,
     main = &quot;Penguin size measurements&quot;)
# a is the intercept and b is the slope
abline(a = b0, b = b1)
```

&lt;img src=&quot;Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-75-1.png&quot; width=&quot;672&quot; /&gt;

The fit of the model to our observed data seems reasonable.

We can also compute the residuals,
$\hat{\epsilon}_1,\ldots,\hat{\epsilon}_n$, the fitted values
$\hat{y}_1,\ldots,\hat{y}_n$, and the associated RSS,
$RSS=\sum_{i=1}^n \hat{\epsilon}_i^2$.


```r
yhat &lt;- b0 + b1 * x # compute fitted values
ehat &lt;- y - yhat # compute residuals
(rss &lt;- sum(ehat^2)) # sum of the squared residuals
#&gt; [1] 6564.494
(sigmasqhat &lt;- rss/(n-2)) # estimated error variance
#&gt; [1] 19.30734
```

## Fitting a linear model using R

We now describe how to use R to fit a linear model to data.

The `lm` function uses OLS to fit a linear model to data. The function
has two major arguments:

-   `data`: the data frame in which the model variables are stored. This
    can be omitted if the variables are already stored in memory.

-   `formula`: a @wilkinsonrogers1973 style formula describing the
    linear regression model. Assuming the `y` is the response, `x`,
    `x1`, `x2`, `x3` are available numeric predictors:

    -   `y ~ x` describes a simple linear regression model based on
        $E(Y|X)=\beta_0+\beta_1 X$.
    -   `y ~ x1 + x2` describes a multiple linear regression model based
        on $E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2$.
    -   `y ~ x1 + x2 + x1:x2` and `y ~ x1 * x2` describe a multiple
        linear regression model based on
        $E(Y|X_1, X_2)=\beta_0+\beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2$.
    -   `y ~ -1 + x1 + x2` describes a multiple linear regression model
        without an intercept, in this case,
        $E(Y|X_1, X_2)=\beta_1 X_1 + \beta_2 X_2$.
    -   `y ~ x + I(x^2)` describe a multiple linear regression model
        based on $E(Y|X)=\beta_0+\beta_1 X + \beta_2 X^2$.

We fit a linear model regressing `body_mass_g` on `bill_length_mm` using
the `penguins` data frame and store it in the object `lmod`. `lmod` is
an object of class `lm`.


```r
lmod &lt;- lm(bill_length_mm ~ body_mass_g, data = penguins) # fit model
class(lmod) # class of lmod
#&gt; [1] &quot;lm&quot;
```

There are a number of methods (generic function that do something
specific when applied to a certain type of object). Commonly used ones
include:

-   `residuals`: extracts $\hat{\boldsymbol{\epsilon}}$ from an `lm`
    object.

-   `fitted`: extracts $\hat{\mathbf{y}}$ from an `lm` object.

-   `coef` or `coefficients`: extracts $\hat{\boldsymbol{\beta}}$ from
    an `lm` object.

-   `deviance`: extracts the RSS from an `lm` object.

-   `sigma`: extracts $\hat{\sigma}$ from an `lm` object.

-   `df.residual`: extracts $n-p$, the degrees of freedom for the RSS,
    from an `lm` object.

-   `summary`: provides:

    -   A 5-number summary of the $\hat{\boldsymbol{\epsilon}}$
    -   A table that lists the predictors, the `Estimate` of the
        associated coefficients, the **estimated** standard error of the
        estimates (`Std.Error`), the computed test statistic associated
        with testing $H_0: \beta_j = 0$ versus $H_a: \beta_j \neq 0$ for
        $j=0,1,\ldots,p-1$ (`t value`), and the associated p-value of
        each test `Pr(&gt;|t|)`.

We now use some of the methods to extract important characteristics of
our fitted model. We then check whether the values obtained from these
methods match our manual calculations.


```r
(coeffs2 &lt;- coefficients(lmod)) # extract, assign, and print coefficients
#&gt;  (Intercept)  body_mass_g 
#&gt; 26.898872424  0.004051417
ehat2 &lt;- residuals(lmod) # extract and assign residuals
yhat2 &lt;- fitted(lmod) # extract and assign fitted values
rss2 &lt;- deviance(lmod) # extract and assign rss
sigmasqhat2 &lt;- rss2/df.residual(lmod) # estimated error variance
# compare to manually computed values
all.equal(c(b0, b1), coeffs2, check.attributes = FALSE)
#&gt; [1] TRUE
all.equal(ehat, ehat2, check.attributes = FALSE)
#&gt; [1] TRUE
all.equal(rss, rss2)
#&gt; [1] TRUE
all.equal(sigmasqhat, sigmasqhat2)
#&gt; [1] TRUE
# methods(class=&quot;lm&quot;)
```

### Derivation of OLS simple linear regression estimators

Use calculus to derive the OLS estimator of the regression coefficients.
Take the partial derivatives of $RSS(\hat{\beta}_0, \hat{\beta}_1)$ with
respect to $\hat{\beta}_0$ and $\hat{\beta}_1$, set the derivatives
equal to zero, and solve for $\hat{\beta}_0$ and $\hat{\beta}_1$ to find
the critical points of $RSS(\hat{\beta}_0, \hat{\beta}_1)$. Technically,
you must show that the Hessian matrix of
$RSS(\hat{\beta}_0, \hat{\beta}_1)$ is positive definite to verify that
our solution minimizes the RSS, but we won&#39;t do that here.

$$\\[4in]$$

&lt;!--chapter:end:06-fitting-a-linear-model.Rmd--&gt;

---
title: &quot;Joshua French&quot;
date: &quot;2022-06-10&quot;
output:
  html_document:
    df_print: paged
---

# Interpreting a fitted linear model

## Orthogonality

Let $$\mathbf{x}_j=(x_{1,j},\ldots,x_{n,j})^T$$ denote the $n\times 1$ column vector of observed values for regressor $X_j$. Regressors, $\mathbf{x}_j$ and $\mathbf{x}_k$ are **orthogonal** if $\mathbf{x}_j^T \mathbf{x}_k=0$. 

Let $\boldsymbol{1}_{n\times1}$ denote an $n\times 1$ column vector of 1s. The definition of orthogonal vectors above implies that $\mathbf{x}_j$ is orthogonal to $\boldsymbol{1}_{n\times1}$ if $$
\mathbf{x}_j^T \boldsymbol{1}_{n\times1} = \sum_{i=1}^n x_{i,j} = 0,$$
i.e., if the values in $\mathbf{x}_j$ sum to zero.

Let $\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{i,j}$ denote the sample mean of $\mathbf{x}_j$ and $\bar{\mathbf{x}}_j = \bar{x}_j \boldsymbol{1}_{n\times 1}$ denote the column vector that repeats $\bar{x}_j$ $n$ times. 

**Centering** $\mathbf{x}_j$ involves subtracting the sample mean of $\mathbf{x}_j$ from $\mathbf{x}_j$, i.e., $\mathbf{x}_j - \bar{\mathbf{x}}_j$. 

Regressors $\mathbf{x}_j$ and $\mathbf{x}_k$ are **uncorrelated** if they are orthogonal after being centered, i.e., if
$$(\mathbf{x}_j - \bar{\mathbf{x}}_j)^T (\mathbf{x}_k - \bar{\mathbf{x}}_k).$$
Note that the sample covariance between vectors $\mathbf{x}_j$ and $\mathbf{x}_k$ is 
\begin{align*}
\widehat{\mathrm{cov}}(\mathbf{x}_j, \mathbf{x}_k) &amp;= \frac{1}{n}\sum_{i=1}^n (x_i,j - \bar{x}_j)(x_i,k - \bar{x}_k) \\
 &amp;= \frac{1}{n-1}(\mathbf{x}_j - \bar{\mathbf{x}}_j)^T (\mathbf{x}_k - \bar{\mathbf{x}}_k).
\end{align*}
Thus, two centered regressors are orthogonal if their covariance is zero.

It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response.  Specifically:

**If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors.**

Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s.

We consider a simple example to demonstrate how orthogonality of regressors impacts the estimated regression coefficients.



In the code below, we define vectors `y`, `x1`, and `x2`. 

```r
y &lt;- c(1, 4, 6, 8, 9)        # create an arbitrary response vector
x1 &lt;- c(7, 5, 5, 7, 7)       # create regressor 1
x2 &lt;- c(-1, 2, -3, 1, 5/7)  # create regressor 2 to be orthogonal to x1
```

Regressors `x1` and `x2` are orthogonal since their crossproduct $\mathbf{x}_1^T \mathbf{x}_2$ (in R, `crossprod(x1, x2)`) equals zero. 

```r
crossprod(x1, x2) # crossproduct is zero so x1 and x2 are orthogonal
#&gt;      [,1]
#&gt; [1,]    0
```
In the code below, we regress `y` on `x1` without an intercept (`lmod1`). The estimated coefficient for `x1` is $\hat{\beta}_1=0.893$. Next, we then regress `y` on `x1` and `x2` without an intercept (`lmod2`). The estimated coefficients for `x1` and `x2` are $\hat{\beta}_1=0.893$ and $\hat{\beta}_2=0.221$, respectively. Because `x1` and `x2` are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for `x1` stays the same in both models.

```r
lmod1 &lt;- lm(y ~ x1 - 1)
coef(lmod1)
#&gt;       x1 
#&gt; 0.893401
lmod2 &lt;- lm(y ~ x1 + x2 - 1)
coef(lmod2)
#&gt;        x1        x2 
#&gt; 0.8934010 0.2210526
```

The previous models (`lmod1` and `lmod2`) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our $\mathbf{X}$ matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s.

However, neither `x1` nor `x2` is orthogonal with the column of ones. We define the vector `ones` below, which is a column of 1s, and compute the crossproduct between `ones` and the two regressors. Since the crossproducts are not zero, `x1` and `x2` are not orthogonal to the column of ones.

```r
ones &lt;- rep(1, 5)
crossprod(ones, x1) # not zero
#&gt;      [,1]
#&gt; [1,]   31
crossprod(ones, x2) # not zero
#&gt;            [,1]
#&gt; [1,] -0.2857143
```

If we add the column of ones to `lmod2` (i.e., if we include the intercept in the model), then the coefficients for both `x1` and `x2` change because these regressors are not orthogonal to the column of 1s, as verified by the R output below. Comparing `lmod2` and `lmod3`, $\hat{\beta}_1$ changes from $0.893$ to $0.397$ and $\hat{\beta}_2$ changes from $0.221$ to $0.279$.


```r
lmod3 &lt;- lm(y ~ x1 + x2)
coef(lmod3)
#&gt; (Intercept)          x1          x2 
#&gt;   3.1547101   0.3969746   0.2791657
```
For orthogonality of our regressors to be most impactful, the model&#39;s regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesn&#39;t impact the estimated coefficients of the other regressors. In the code below, we define centered regressors `x3` and `x4` to be uncorrelated, i.e., `x3` and `x4` have sample mean zero and are orthogonal to each other.


```r
x3 &lt;-  c(0, -1, 1, 0, 0) # sample mean is zero
x4 &lt;- c(0, 0, 0, 1, -1)  # sample mean is zero
cov(x3, x4)              # 0, so x3 and x4 are uncorrelated
#&gt; [1] 0
```

If we fit a linear regression model with any combination of `ones`, `x3`, or `x4` as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below.


```r
coef(lm(y ~ 1))           # only column of 1s
coef(lm(y ~ x3 - 1))      # only x3
coef(lm(y ~ x4 - 1))      # only x4
coef(lm(y ~ x3))          # 1s and x3
coef(lm(y ~ x4))          # 1s and x4
coef(lm(y ~ x3 + x4 - 1)) # x3 and x4
coef(lm(y ~ x3 + x4))     # 1s, x3, and x4
```
We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesn&#39;t impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were $\hat{\beta}_{int}=5.6$, $\hat{\beta}_{3}=1.0$, $\hat{\beta}_{4}=-0.5$ when the relevant regressor was included in the model.

The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the $\mathbf{X}$ matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, `x1`, `x2`, `x3`, and `x4`.


```r
crossprod(model.matrix(~ x1 + x2 + x3 + x4))
#&gt;             (Intercept)  x1         x2 x3        x4
#&gt; (Intercept)   5.0000000  31 -0.2857143  0 0.0000000
#&gt; x1           31.0000000 197  0.0000000  0 0.0000000
#&gt; x2           -0.2857143   0 15.5102041 -5 0.2857143
#&gt; x3            0.0000000   0 -5.0000000  2 0.0000000
#&gt; x4            0.0000000   0  0.2857143  0 2.0000000
```
Consider the sequence of models below.

```r
coef(lm(y ~ 1))
#&gt; (Intercept) 
#&gt;         5.6
```
The model with only an intercept has an estimated coefficient of $\hat{\beta}_{int}=5.6$. If we add the `x1` to the model with an intercept, then both coefficients change because they are not orthogonal to each other.

```r
lmod4 &lt;- lm(y ~ x1) # model with 1s and x1
coef(lmod4)
#&gt; (Intercept)          x1 
#&gt;         2.5         0.5
```
If we add `x2` to `lmod4`, we might think that only $\hat{\beta}_{int}$ will change because `x1` and `x2` are orthogonal to each other. However, because `x2` is not orthogonal to all of the other regressors in the model (`x1` and the column of 1s), both $\hat{\beta}_{int}$ and $\hat{\beta}_1$ will change. The easiest way to realize this is to look at `lmod2` above with only `x1` and `x2`. When we add the column of 1s to `lmod2`, both $\hat{\beta}_1$ and $\hat{\beta}_2$ will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term.


```r
coef(lm(y ~ x1 + x2))
#&gt; (Intercept)          x1          x2 
#&gt;   3.1547101   0.3969746   0.2791657
```

However, note that `x3` is orthogonal to the column of 1s and `x1`. Thus, if we add `x3` to `lmod4`, which includes both a column of 1s and `x1`, `x3` will not change the estimated coefficients for the intercept or `x1`.


```r
coef(lm(y ~ x1 + x3))
#&gt; (Intercept)          x1          x3 
#&gt;         2.5         0.5         1.0
```

Additionally, since `x4` is orthogonal to the column of 1s, `x1`, and `x3`, adding `x4` to the previous model will not change the estimated coefficients for any of the other variables already in the model.


```r
coef(lm(y ~ x1 + x3 + x4))
#&gt; (Intercept)          x1          x3          x4 
#&gt;         2.5         0.5         1.0        -0.5
```

Lastly, if we can partition our $\mathbf{X}$ matrix usch that $\mathbf{X}^T \mathbf{X}$ is a block diagonal matrix, then the none of the blocks of variables will affect the estimated coefficients of the other variables.

Define a new regressor `x5` below. `x5` is orthogonal to the column of 1s and `x1`, but not `x4`. 


```r
x5 &lt;- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4
crossprod(cbind(1, x1, x4, x5))
#&gt;        x1 x4 x5
#&gt;     5  31  0  0
#&gt; x1 31 197  0  0
#&gt; x4  0   0  2 -1
#&gt; x5  0   0 -1  2
```

This means that if we fit the model with only the column of 1s and the intercept, the model only with `x4` and `x5`, and then fit the model with the column of 1s, `x1`, `x4`, and `x5`, then the coefficients $\hat{\beta}_{int}$ and $\hat{\beta}_{1}$ are not impacted when `x4` and `x5` are added to the model. Similarly, $\hat{\beta}_{4}$ and $\hat{\beta}_{5}$ are not impacted when the column of 1s and `x1` are added to the model with `x4` and `x5`. See the output below.


```r
lm(y ~ x1)           # model with 1s and x1
#&gt; 
#&gt; Call:
#&gt; lm(formula = y ~ x1)
#&gt; 
#&gt; Coefficients:
#&gt; (Intercept)           x1  
#&gt;         2.5          0.5
lm(y ~ x4 + x5 - 1)  # model with x4 and x5 only
#&gt; 
#&gt; Call:
#&gt; lm(formula = y ~ x4 + x5 - 1)
#&gt; 
#&gt; Coefficients:
#&gt; x4  x5  
#&gt; -3  -5
lm(y ~ x1 + x4 + x5) # model with 1s, x1, x4, x5
#&gt; 
#&gt; Call:
#&gt; lm(formula = y ~ x1 + x4 + x5)
#&gt; 
#&gt; Coefficients:
#&gt; (Intercept)           x1           x4           x5  
#&gt;         2.5          0.5         -3.0         -5.0
```




&lt;!--chapter:end:07-interpreting-a-fitted-linear-model.Rmd--&gt;

---
title: &quot;Joshua French&quot;
date: &quot;2022-06-10&quot;
output:
  html_document:
    df_print: paged
---

# Categorical predictors

Categorical  predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the categorical variables. In what follows, we consider several common linear regression models that involve a categorical variable. To simplify our discussion, we only consider the setting where there is a single categorical variable to add to our model. Similarly, we only consider the setting where there is a single numeric variable.

We begin by defining some notation.

Let $X$ denote a numeric regressor, with $x_i$ denoting the value of $X$ for observation $i$.

Let $F$ denote a categorical variable with levels $L_1, L_2, \ldots, L_K$. The $F$ stands for &quot;factor&quot;, while the $L$ stands for &quot;level&quot;. The notation $f_i$ denotes the value of $F$ for observation $i$. 

## Indicator/dummy variables

We may recall that if $\mathbf{X}$ denotes our matrix of regressors and $\mathbf{y}$ our vector of responses, then (assuming the columns of $\mathbf{X}$ are linearly independent) the OLS solution for $\boldsymbol{\beta}$ is $$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.$$ In order to compute the estimated coefficients, both $\mathbf{X}$ and $\mathbf{y}$ must contain numeric values. How can we use a categorical predictor in our regression model when the levels are not numeric values? In order to use a categorical predictor in a regression model, we must transform it into a set of one or more **indicator** or **dummy variables**, which we explain in more detail below.

An **indicator function** is a function that takes the value 1 of a certain property is true and 0 otherwise. An indicator variable is the variable that results from applying an indicator function to each observation in a data set. Many notations exist for indicator functions. We will adopt the notation 

\begin{equation*}
I_S(x) = \begin{cases}
1 &amp; \textrm{if}\;x \in S\\
0 &amp; \textrm{if}\;x \notin S
\end{cases},
\end{equation*}

which is shorthand for a function that returns 1 if $x$ is in the set $S$ and 0 otherwise. 

We let $D_j$ denote the indicator (dummy) variable for factor level $L_j$ of $F$. The value of $D_j$ for observation $i$ is denoted $d_{i,j}$, with $$d_{i,j} = I_{\{L_j\}}(f_i),$$ i.e., $d_{i,j}$ is 1 if observation $i$ has factor level $L_j$ and 0 otherwise.

## Common of linear models with categorical predictors

It is common to use notation like $E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 +\beta_2 X_2$ when discussing linear regression models. That notation is generally simple and convenient, but can be unclear. Asking a researcher what the estimate of $\beta_2$ is in a model is ambiguous because it will depend on the order the researcher added the variables to the model. To more closely connect each coefficient with the regressor to which it is related, we will use the notation $\beta_X$ to denote the coefficient for regressor $X$ and $\beta_{D_j}$ to denote the coefficient for regressor $D_j$. Similarly, $\beta_{int}$ denotes the intercept included in our model.

### One-way ANOVA

#### Definition

A one-way analysis of variance (ANOVA) assumes a constant mean for each level of a categorical variable. A general one-way ANOVA relating a response variable $Y$ to a factor variable $F$ with $K$ levels may be formulated as $$E(Y|F) = \beta_{int} + \beta_{D_2} D_2 + \ldots + \beta_{D_K} D_K.$$ Alternatively, in terms of the individual responses, we may formulate the one-way ANOVA  model as 
$$Y_i = \beta_{int} + \beta_{D_2} d_{i,2} + \cdots + \beta_{D_K} d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$ 

This may bring up some questions that need answering.

*Why does the one-way ANOVA model only contains dummy variables for the last $K-1$ levels of $F$?* This is not a mistake. If we included dummy variables for all levels of $F$, then the matrix of regressors would have linearly dependent columns because the sum of the dummy variables for all $K$ levels would equal the column of 1s for the intercept.

*Why do we omit the dummy variable for the first level of $F$?* This is simply convention. We could omit the dummy variable for any single level of $F$. However, it is conventional to designate one level the reference level and to omit that variable. As we will see when discussing interpretation of the coefficient, the reference level becomes the level of $F$ that all other levels are compared to.

*Could we omit the intercept instead of one of the dummy variables?* Yes, you could. There is no mathematical or philosophical issues with this. However, this can create problems when you construct models including both categorical and numeric regressors. The standard approach is recommended because it typically makes our model easier to interpret and extend.

#### Interpretation

We interpret the coefficients of our one-way ANOVA with respect to the change in the mean response.

Suppose an observation of level $L_1$. We can determine that the mean response is 

\begin{align*}
E(Y|F=L_1) &amp;= \beta_{int} + \beta_{D_2} 0 + \cdots + \beta_{D_K} 0 \\
&amp;= \beta_{int}.
\end{align*}

Similarly, when an observation has level $L_2$, then 
\begin{align*}
E(Y|F=L_2) &amp;= \beta_{int} + \beta_{D_2} 1 + \beta_{D_3} 0 + \cdots + \beta_{D_K} 0 \\
&amp;= \beta_{int} + \beta_{D_2}.
\end{align*}
This helps us to see the general relationship that
$$E(Y|F=L_j) = \beta_{int} + \beta_{D_j},\quad j=2,\ldots,K.$$
 In the context of a one-way ANOVA:

* $\beta_{int}$ represents the expected response for observations having the reference level.
* $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having the reference level and level $L_j$. 
  * You can verify this by computing $E(Y|F=L_j) - E(Y|F=L_1)$ (for $j = 2, \ldots, K$).

### Main effects models

A main effects model is also called a parallel lines model since the regression equations for each factor level produce lines parallel to one another.

A parallel lines model is formulated as
$$ Y_i = \beta_{int} + \beta_{X} x_i + \beta_{L_2} d_{i,2} + \cdots + \beta_{L_K} d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$

When an observation has level $L_1$, then the expected response is $\beta_{int} + \beta_1 X$. More specifically, $$E(Y|X = x, F=L_1) = \beta_{int} + \beta_X x + \beta_{L_2} 0 + \cdots + \beta_{L_K} 0 = \beta_{int} + \beta_X x.$$ Thus, $E(Y|X = 0, F=L_1) = \beta_{int}.$

When an observation has level $L_2$, the expected response is $\beta_{int} + \beta_{X} X + \beta_{L_2}$. More formally, 
$$E(Y|X = x, F=L_2) = \beta_{int} + \beta_X x + \beta_{L_2} 1 + \beta_{L_3} 0 + \cdots + \beta_{L_K} 0 = \beta_{int} + \beta_X x + \beta_{L_2}.$$ Thus, the mean response for observations having level $L_2$ is $\beta_{int} + \beta_{L_2} + \beta_{X} x$. In general, 
$$E(Y|X = x, F=L_j) = \beta_{int} + \beta_X x + \beta_{L_j},\quad j = 2,\ldots,K.$$ Thus,

$$E(Y|X=x, F=L_j) - E(Y|X=x, F=L_1) = (\beta_{int} + \beta_X x + \beta_{L_j}) - (\beta_{int} + \beta_X x) = \beta_{L_j}.$$

In the context of parallel lines models:

* $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
* $\beta_X$ is the expected change in the response when $X$ increases by 1 unit for a fixed level of $F$.
* $\beta_{L_j}$, for $j=2,\ldots,K$ represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X$ fixed at the same value. 

### Interaction models 

An interaction model is also called a separate lines model since the regression equations for each factor level produce lines that are distinct and separate.

A separate lines model is formulated as
$$ Y_i = \beta_{int} + \beta_{X} x_i + \beta_{L_2} d_{i,2} + \cdots + \beta_{L_K} d_{i,K} + + \beta_{X L_2} x_i d_{i,2} + \cdots +  \beta_{X L_j} x_i d_{i,K} + \epsilon_i,\quad i=1,2,\ldots n.$$
When an observation has level $L_1$, then the expected response is $\beta_{int} + \beta_1 X$. More specifically, $$E(Y|X = x, F=L_1) = \beta_{int} + \beta_X x + \beta_{L_2} 0 + \cdots + \beta_{L_K} 0  + \beta_{X L_2} x_i 0 + \cdots +  \beta_{X L_K} x_i 0 = \beta_{int} + \beta_X x.$$ Thus, $E(Y|X = 0, F=L_1) = \beta_{int}.$

When an observation has level $L_j$, for $j=2,\ldots,K$, the expected response is $\beta_{int} + \beta_{X} X + \beta_{L_j} + \beta_{X L_J} X.$ More formally, 
$$E(Y|X = x, F=L_j) = \beta_{int} + \beta_X x + \beta_{L_j} + \beta_{X L_j} x.$$


Note that $$E(Y|X=0, F=L_1) = \beta_{int}.$$ 
Additionally, we note that $$E(Y|X=0, F=L_j) - E(Y|X=0, F=L_1) = (\beta_{int} + \beta_X 0 + \beta_{L_j} + \beta_{X L_J} 0) - (\beta_{int} + \beta_X 0) = \beta_{L_j}.$$

In the context of separate lines models:

* $\beta_{int}$ represents the expected response for observations having the reference level when the numeric regressor $X = 0$.
* $\beta_{L_j}$, for $j=2,\ldots,K$, represents the expected change in the response when comparing observations having level $L_1$ and $L_j$ with $X=0$.
* $\beta_X$ represents the expected change in the response when $X$ increases by 1 unit for observations having the reference level.
* $\beta_X L_j$, for $j=2,\ldots,K$, represents the difference in the expected rate of change when $X$ increases by 1 unit for observations have the baseline level in comparison to level $L_j$.

### Extensions

In the models above, we have only discussed possibilities with a single numeric variable and a single factor variable. Naturally, one can consider models with multiple factor variables, multiple numeric variables, interactions between factor variables, interactions between numeric variables, etc. The models become more complicated, but the ideas are similar. One simply has to keep track of what role each coefficient plays in the model.


&lt;!-- We could fit other ANCOVA models, such as the common intercept model, but the parallel lines and separate lines models are the most common. --&gt;

&lt;!-- You can add additional interactions, squared and cubic explanatory variables to the model, etc. --&gt;

&lt;!--chapter:end:08-categorical-predictors.Rmd--&gt;

---
title: &quot;Joshua French&quot;
date: &quot;2022-06-10&quot;
output:
  html_document:
    df_print: paged
---

# Assessing and addressing collinearity


&lt;!--chapter:end:09-assessing-and-addressing-collinearity.Rmd--&gt;


# References {-}


&lt;!--chapter:end:10-references.Rmd--&gt;

---
title: &quot;Joshua French&quot;
date: &quot;2022-06-10&quot;
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
---

# Defining and fitting a linear model

In this chapter, we define a linear model and discuss the basic estimation of its parameters. We leave discussion of more theoretical aspects of the model to subsequent chapters.

## Background and terminology

Regression models are used to model the relationship between:

* one or more **response** variables and
* one or more **predictor** variables.

The distinction between these two types variables is their purpose in the model.

* Predictor variables are used to predict the value of the response variable.

Response variables are also known as **outcome**, **output**, or **dependent** variables.

Predictor variables are also known as **explanatory**, **regressor**, **input**, **dependent**, or **feature** variables.

Note:  Because the variables in our model are often interrelated, describing these variables as independent or dependent variables is vague and is best avoided.

A distinction is sometimes made between **regression models** and **classification models**. In that case:

* Regression models attempt to predict a numerical response.
* Classification models attempt to predict the category level a response will have.

## Goals of regression

The basic goals of a regression model are to:

1. *Predict* future or unknown response values based on specified values of the predictors.
    * What will the selling price of a home be?
2. *Describe* relationships (associations) between predictor variables and the response.
    * What is the general relationship between the selling price of a home and the number of bedrooms the home has?

With our regression model, we also hope to be able to:

1. *Generalize* our results from the sample to the a larger population of interest.
    * E.g., we want to extend our results from a small set of college students to all college students.
2. *Infer causality* between our predictors and the response.
    * E.g., if we give a person a vaccine, then this causes the person&#39;s risk of catching the disease to decrease.

**A &quot;true model&quot; doesn&#39;t exist for real data**. The data-generating process is far more complex than the models we can realistically fit to the data. Thus, finding the true model should not be the goal of a regression analysis. A regression analysis should attempt to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.)

## Definition of a linear model

A **linear model** is a regression model in which the regression coefficients (to be discussed later) enter the model linearly.

* A linear model is just a specific type of regression model.

### Basic construction and relationships

We begin by defining notation for the objects we will need and clarifying some of their important properties.

* $Y$ denotes the response variable.
  * The response variable is treated as a random variable.
  * We will observe realizations of this random variable for each observation in our data set.
* $X$ denotes a single predictor variable. $X_1$, $X_2$, \ldots, $X_{p-1}$ denote the predictor variables $1,2,\ldots,p$.
  * The predictor variables are treated as non-random variables.
  * We will observe values of the predictors variables for each observation in our data set.
* $\beta_0$, $\beta_1$, \ldots, $\beta_{p-1}$ denote **regression coefficients**.
  * Regression coefficients are statistical parameters that we will estimate from our data.
  * Like all statistical parameters, regression coefficients are treated as fixed (non-random) but unknown values.
  * Regression coefficients are not observable.
* $\epsilon$ denotes **error**.
  * The error is not observable.
  * The error is treated as a random variable.
  * The error is assumed to have mean 0, i.e., $E(\epsilon) = 0$.
  * Since $E(\epsilon) = 0$ and $X$ is non-random, the expectation of $\epsilon$ conditional on $X$ is also 0, i.e., $E(\epsilon | X) = 0$.
  * In this context, error doesn&#39;t mean &quot;mistake&quot; or &quot;malfunction&quot;. $\epsilon$ is simply the deviation of the response from its mean.

A **linear model** for $Y$ is defined by the equation
\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_{p-1} X_{p-1} + \epsilon.
\tag{5.1}
\end{equation}

We now emphasize the relationship between the response, the mean response, and the error. The mean of the response variable will depend on the values of the predictor variables. Thus, we can only discuss the expectation of the response variable conditional on the values of the predictor variables. This is denoted as $E(Y | X_1, \ldots, X_{p-1})$.

For simplicity, assume our linear model only has a single predictor (this is an example of simple linear regression). Based on what we&#39;ve presented, we have that

\begin{align}
E(Y|X) &amp;= E(\beta_0 + \beta_1  X + \epsilon | X) \\
 &amp;= E(\beta_0 | X) + E(\beta_1 X | X) + E(\epsilon | X) \\
 &amp;= \beta_0 + \beta_1 X + 0\\
 &amp;= \beta_0 + \beta_1 X.
\end{align}\]</span></p>
<p>The second line follows from the fact that the expectation of a sum of random variables is the sum of the expectation of the random variables. The third line follows from the fact that the expected value of a constant (non-random) value is the constant (the regression coefficients and <span class="math inline">\(X\)</span> are non-random) and by our assumption that the errors have mean 0 (unconditionally or conditionally on the predictor variable.)</p>
<p>Thus, we see that we see that for a simple linear regression model
<span class="math display">\[ Y = E(Y|X) + \epsilon.\]</span>
For a model with multiple predictors, this extends to
<span class="math display">\[Y = E(Y|X_1, X_2, \ldots, X_{p-1}) + \epsilon.\]</span>
Thus, our response may be written as the sum of the mean response conditional on the predictors, <span class="math inline">\(E(Y|X_1, X_2, \ldots, X_{p-1})\)</span>, and the error. This is why previously we discussed the fact that the error is simply the deviation of the response from its mean.</p>
<p>Alternatively, one can say that a regression model is linear if the mean function can be written as a linear combination of the regression coefficients and known values, i.e.,
<span class="math display">\[E(Y|X_1, X_2, \ldots, X_{p-1}) = \sum_{j=0}^{p-1} c_j \beta_j,\]</span>
where <span class="math inline">\(c_0, c_1, \ldots, c_{p-1}\)</span> are known values. In fact, the <span class="math inline">\(c_i, i = 1,2,\ldots,n\)</span> can be any function of <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>! e.g., <span class="math inline">\(c_1 = X_1 * X_2 * X_3\)</span>, <span class="math inline">\(c_3 = X_2^2\)</span>, <span class="math inline">\(c_8 = ln(X_1)/X_2^2\)</span>.</p>
<p>Some examples of linear models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0 + +\beta_1 X + \beta_2 X^2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 * X_2\)</span>.</li>
<li><span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2^{-1}\)</span>.</li>
<li><span class="math inline">\(E(\ln(Y)|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
<li><span class="math inline">\(E(Y^{-1}|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2\)</span>.</li>
</ul>
<p>Some examples of non-linear models:</p>
<ul>
<li><span class="math inline">\(E(Y|X) = \beta_0 + e^{\beta_1 X}\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = \beta_0 + \beta_1 X/(\beta_2 + X)\)</span>.</li>
</ul>
<!-- The model in Equation \@ref(eq:lmdef) is a **statistical model** because there is uncertainty in the response.  -->
</div>
<div id="as-a-system-of-equations-1" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> As a system of equations<a href="linear-model-estimation.html#as-a-system-of-equations-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A linear regression analysis will model the data using a linear model. Suppose we have sampled <span class="math inline">\(n\)</span> observations from a population. We now introduce some additional notation:</p>
<ul>
<li><span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span> denote the response values for the <span class="math inline">\(n\)</span> observations.</li>
<li><span class="math inline">\(x_{i,j}\)</span> denotes the observed value of predictor <span class="math inline">\(j\)</span> for observation <span class="math inline">\(i\)</span>.
<ul>
<li>We use lowercase <span class="math inline">\(x\)</span> to indicate that this is the observed value of the predictor.</li>
</ul></li>
<li><span class="math inline">\(\epsilon_1, \epsilon_2, \ldots, \epsilon_n\)</span> denote the errors for the <span class="math inline">\(n\)</span> observations.</li>
</ul>
<p>The linear model relating the responses, the predictors, and the errors is defined by the system of equations
<span class="math display" id="eq:lmSystem">\[\begin{equation}
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \cdots + \beta_{p-1} x_{i,p-1} + \epsilon_i,\quad i=1,2,\ldots,n.
\tag{5.2}
\end{equation}\]</span></p>
<p>Based on our previous work, we can also write Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmSystem">(5.2)</a> as
<span class="math display">\[\begin{equation}
Y_i = E(Y_i | X_1 = x_{i,1}, \ldots, X_{p-1} = x_{i,p-1}) + \epsilon_i,\quad i=1,2,\ldots,n.
\end{equation}\]</span></p>
</div>
<div id="using-matrix-notation-1" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Using matrix notation<a href="linear-model-estimation.html#using-matrix-notation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices. We define the following notation:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} = [Y_1, Y_2, \ldots, Y_n]^T\)</span> denotes the column vector containing the <span class="math inline">\(n\)</span> responses.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> denotes the matrix containing a column of 1s and the observed predictor values, specifically, <span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,p-1} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,p-1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,p-1}
\end{bmatrix}.\]</span></li>
<li><span class="math inline">\(\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots, \beta_{p-1}]^T\)</span> denotes the column vector containing the <span class="math inline">\(p\)</span> regression coefficients.</li>
<li><span class="math inline">\(\boldsymbol{\epsilon} = [\epsilon_1, \epsilon_2, \ldots, \epsilon_n]^T\)</span> denotes the column vector contained the <span class="math inline">\(n\)</span> errors.
Then the system of equations defining the linear model in <a href="defining-and-fitting-a-linear-model.html#eq:lmSystem">(5.2)</a> can be written as
<span class="math display">\[\mathbf{y} = \mathbf{X}\mathbf{\beta} + \boldsymbol{\epsilon}.\]</span>
Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model.</li>
</ul>
</div>
</div>
<div id="summarizing-the-components-of-a-linear-model-1" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Summarizing the components of a linear model<a href="linear-model-estimation.html#summarizing-the-components-of-a-linear-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have already introduced a lot of objects. To aid in making sense of their notation, their purpose in the model, whether they can be observed, and whether they are modeled as a random variable (vector) or fixed, non-random values, we summarize things below.</p>
<p>We’ve already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable.</p>
<p>On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmSystem">(5.2)</a> is for the errors to be random.</p>
<p>We summarize this information in the table below for the objects previously discussed using the various notations introduced.</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Notation</th>
<th>Description</th>
<th>Observable</th>
<th>Random</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td>response variable</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y_i\)</span></td>
<td>response value for the <span class="math inline">\(i\)</span>th observation</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{y}\)</span></td>
<td>the <span class="math inline">\(n\times 1\)</span> column vector of response values</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X\)</span></td>
<td>predictor variable</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X_j\)</span></td>
<td>the <span class="math inline">\(j\)</span>th predictor variable</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_{i,j}\)</span></td>
<td>the value of the <span class="math inline">\(j\)</span>th predictor variable for the <span class="math inline">\(i\)</span>th observation</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{X}\)</span></td>
<td>the <span class="math inline">\(n\times p\)</span> matrix of predictor values</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\beta_j\)</span></td>
<td>the regression coefficient associated with the <span class="math inline">\(j\)</span>th predictor variable</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\boldsymbol{\beta}\)</span></td>
<td>the <span class="math inline">\(p\times 1\)</span> column vector of regression coefficients</td>
<td>No</td>
<td>No</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\epsilon\)</span></td>
<td>the error</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\epsilon_i\)</span></td>
<td>the error associated with observation <span class="math inline">\(i\)</span></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\boldsymbol{\epsilon}\)</span></td>
<td>the <span class="math inline">\(n\times 1\)</span> column vector of errors</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<div id="types-of-regression-models-1" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Types of regression models<a href="linear-model-estimation.html#types-of-regression-models-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The are many “named” types of regression models. You may hear or see people use these terms when describing their model. Here is a brief overview of some common regression models.</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Defining characteristics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple</td>
<td>an intercept term and one predictor variable</td>
</tr>
<tr class="even">
<td>Multiple</td>
<td>more than one predictor variable</td>
</tr>
<tr class="odd">
<td>Multivariate</td>
<td>more than one response variable</td>
</tr>
<tr class="even">
<td>Linear</td>
<td>the regression coefficients enter the model linearly</td>
</tr>
<tr class="odd">
<td>Analysis of variance (ANOVA)</td>
<td>predictors are all categorical</td>
</tr>
<tr class="even">
<td>Analysis of covariance (ANCOVA)</td>
<td>at least one quantitative predictor and at least one categorical predictor</td>
</tr>
<tr class="odd">
<td>Generalized linear model (GLM)</td>
<td>a type of “generalized” regression model when the responses do not come from a normal distribution.</td>
</tr>
</tbody>
</table>
</div>
<div id="standard-linear-model-assumptions-and-implications-1" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> Standard linear model assumptions and implications<a href="linear-model-estimation.html#standard-linear-model-assumptions-and-implications-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The formulation of a linear model typically makes additional assumptions beyond the
ones previously mentioned, specifically, about the errors, <span class="math inline">\(\epsilon_1,\epsilon_2,\ldots,\epsilon_n\)</span>.</p>
<p>We have already mentioned that fact that we are assuming <span class="math inline">\(E(\epsilon_i)=0\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>.</p>
<p>We also typically assume that the errors have constant variances, i.e., <span class="math display">\[var(\epsilon_i) = \sigma^2, \quad i=1,2,\ldots,n,\]</span>
and that the errors are uncorrelated, i.e., <span class="math display">\[cov(\epsilon_i, \epsilon_j) = 0, \quad i,j=1,2,\ldots,n,\quad i\neq j.\]</span></p>
<p>Additionally, we assume that the errors are identically distributed. Formally, that may be written as
<span class="math display" id="eq:errordist">\[\begin{equation}
\epsilon_i \sim F, i=1,2,\ldots,n,
\tag{5.3}
\end{equation}\]</span>
where <span class="math inline">\(F\)</span> is some arbitrary distribution. The <span class="math inline">\(\sim\)</span> means “distributed as.” In other words, Equation <a href="defining-and-fitting-a-linear-model.html#eq:errordist">(5.3)</a> means, “<span class="math inline">\(\epsilon_i\)</span> is distributed as <span class="math inline">\(F\)</span> for <span class="math inline">\(i\)</span> equal to <span class="math inline">\(1,2,\ldots,n\)</span>.” However, it is more common to assume the errors have a normal (Gaussian) distribution. Two uncorrelated normal random variables are also independent (this is true for normal random variables, but is not generally true for other distributions). Thus, we may concisely state the typical error assumptions as
<span class="math display">\[\epsilon_1,\epsilon_2,\ldots,\epsilon_n \stackrel{i.i.d.}{\sim} N(0, \sigma^2),\]</span>
which combines the following assumptions:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(\epsilon_i)=0\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>.</li>
<li><span class="math inline">\(var(\epsilon_i)=\sigma^2\)</span> for <span class="math inline">\(i=1,2,\ldots,n\)</span>.</li>
<li><span class="math inline">\(cov(\epsilon_i,\epsilon_j)=0\)</span> for <span class="math inline">\(i\neq j\)</span> with <span class="math inline">\(i,j=1,2,\ldots,n\)</span>.</li>
<li><span class="math inline">\(\epsilon_i\)</span> has a normal distribution for <span class="math inline">\(i=1,2,\ldots,n\)</span>.</li>
</ol>
<p>Using the notation previously developed, the assumptions above may be stated in vector notation as
<span class="math display" id="eq:error-assumptions">\[\begin{equation}
\boldsymbol{\epsilon} \sim N(\mathbf{0}_{n\times 1}, \sigma^2 I_n), \tag{5.4}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{0}_{n\times 1}\)</span> is the <span class="math inline">\(n \times 1\)</span> vector of zeros and <span class="math inline">\(I_n\)</span> is the <span class="math inline">\(n\times n\)</span> identity matrix.</p>
<p>Writing the error assumptions in the form of Equation <a href="defining-and-fitting-a-linear-model.html#eq:error-assumptions">(5.4)</a> allows us to see other important properties of a linear model. Specifically, if we make the assumptions in Equation <a href="defining-and-fitting-a-linear-model.html#eq:error-assumptions">(5.4)</a>, then</p>
<p><span class="math display" id="eq:dist-properties-y">\[\begin{equation}
\mathbf{y} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma^2 I_n).
\tag{5.5}
\end{equation}\]</span></p>
<p>How do we know that Equation <a href="defining-and-fitting-a-linear-model.html#eq:dist-properties-y">(5.5)</a> is true? Show that:</p>
<ul>
<li><span class="math inline">\(E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}\)</span> and</li>
<li><span class="math inline">\(\mathrm{var}(\mathbf{y}) = \sigma^2 I_n\)</span>.</li>
<li>Note <span class="math inline">\(\mathbf{y}\)</span> is a linear function of the multivariate normal vector <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, so <span class="math inline">\(\mathbf{y}\)</span> must also have a multivariate normal distribution.</li>
</ul>
</div>
<div id="mathematical-interpretation-of-coefficients-1" class="section level2 hasAnchor" number="6.9">
<h2><span class="header-section-number">6.9</span> Mathematical interpretation of coefficients<a href="linear-model-estimation.html#mathematical-interpretation-of-coefficients-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The regression coefficients have simple mathematical interpretations in basic settings.</p>
<div id="coefficient-interpretation-in-simple-linear-regression-1" class="section level3 hasAnchor" number="6.9.1">
<h3><span class="header-section-number">6.9.1</span> Coefficient interpretation in simple linear regression<a href="linear-model-estimation.html#coefficient-interpretation-in-simple-linear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a simple linear regression model, so that <span class="math inline">\(E(Y|X)=\beta_0 + \beta_1 X.\)</span> The interpretations of the coefficients are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when the predictor is 0, i.e., <span class="math inline">\(\beta_0=E(Y|X=0)\)</span>.</li>
<li><span class="math inline">\(\beta_1\)</span> is the expected change in the response when the predictor increases 1 unit, i.e., <span class="math inline">\(\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)\)</span>.</li>
</ul>
</div>
<div id="coefficient-interpretation-in-multiple-linear-regression-1" class="section level3 hasAnchor" number="6.9.2">
<h3><span class="header-section-number">6.9.2</span> Coefficient interpretation in multiple linear regression<a href="linear-model-estimation.html#coefficient-interpretation-in-multiple-linear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we have a multiple linear regression model, so that <span class="math inline">\(E(Y|X_1,\ldots,X_{p-1})=\beta_0 + \beta_1 X_1 + \cdots + \beta_{p-1} X_{p-1}.\)</span> Let <span class="math inline">\(\mathbb{X} = \{X_1,\ldots,X_{p-1}\}\)</span> be the set of predictors and <span class="math inline">\(\mathbb{X}_{-j} = \mathbb{X}\setminus\{X_j\}\)</span>, i.e., the set of predictors without <span class="math inline">\(X_j\)</span>.</p>
<p>The interpretations of the coefficients are:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the expected response when all predictors are 0, i.e., <span class="math inline">\(\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)\)</span>.</li>
<li><span class="math inline">\(\beta_j\)</span> is the expected change in the response when predictor <span class="math inline">\(j\)</span> increases 1 unit and the other predictors stay the same, i.e., <span class="math inline">\(\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})\)</span> where <span class="math inline">\(\mathbf{x}^*\in \mathbb{R}^{p-2}\)</span> is a fixed vector of length <span class="math inline">\(p-2\)</span> (the number of predictors excluding <span class="math inline">\(X_j\)</span>).</li>
</ul>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="6.10">
<h2><span class="header-section-number">6.10</span> Exercises<a href="linear-model-estimation.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>If given a set of data with several variables, how would you decide what the response variable and the predictor variables would be?</li>
<li>Which objects in the linear model formula in Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmdef">(5.1)</a> are considered random? Which are considered fixed?</li>
<li>Which objects in the linear model formula in Equation <a href="defining-and-fitting-a-linear-model.html#eq:lmdef">(5.1)</a> are observable? Which are not observable?</li>
<li>What are the typical goals of a regression analysis?</li>
<li>List the typical assumptions made for the errors in a linear model.</li>
<li>Without using a formula, what is the basic difference between a linear model and a non-linear model?</li>
<li>Assuming that <span class="math inline">\(\boldsymbol{\epsilon} ~ N(\mathbf{0}_{n\times 1}, \sigma^2 I_n)\)</span> and <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, show that:
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(E(\mathbf{y}) = \mathbf{X}\boldsymbol{\beta}\)</span>.</li>
<li><span class="math inline">\(\mathrm{var}(\mathbf{y}) = \sigma^2 I_n\)</span>.</li>
</ol></li>
<li>In the context of simple linear regression under the standard assumptions, show that:
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta_0=E(Y|X=0)\)</span>.</li>
<li><span class="math inline">\(\beta_1=E(Y|X=x_0+1)-E(Y|X=x_0)\)</span>.</li>
</ol></li>
<li>In the context of multiple linear regression under the standard assumptions, show that:
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\beta_0=E(Y|X_1=0,\ldots,X_{p-1}=0)\)</span>.</li>
<li>For <span class="math inline">\(j=1,2,\ldots,p-1\)</span>, <span class="math inline">\(\beta_j=E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0}+1)-E(Y|\mathbb{X}_{-j} = \mathbf{x}^*, X_{j+1} = x_{0})\)</span> where <span class="math inline">\(\mathbf{x}^*\)</span> is a fixed vector of the appropriate size.</li>
</ol></li>
</ol>
<!-- ## Regression for Pearson's height data -->
<!-- @wachsmuth_et_al2003 compiled child and parent height data from English familes tabulated by @pearson1897 and @pearson_lee1903. The data are available in the `PearsonLee` data set in the **HistData** package [@R-HistData]. The `PearsonLee` data frame includes the variables: -->
<!-- * `child`: child height (inches). -->
<!-- * `parent`: parent height (inches). -->
<!-- * `gp`: a factor with levels `fd` (father/daughter), `fs` (father/son), `md` (mother/daughter), `ms` (mother/son) indicating the parent/child relationship. -->
<!-- * `par`: a factor with levels `Father`, `Mother` indicating the parent measured. -->
<!-- * `chl`: a factor with levels `Daughter`, `Son` indicating the child's relationship to the parent. -->
<!-- It is natural to wonder whether the height of a parent could explain the height of their child. We can consider a regression analysis that regresses child's height (the response variable) on parent's height (the predictor variable). The additional variables `gp`, `par`, and `chl` could also be used as predictor variables in our analysis. We perform an informal (linear) regression analysis visually  using **ggplot2** [@R-ggplot2].  -->
<!-- Consider a plot of child's height versus parent's height. -->
<!-- ```{r} -->
<!-- data(PearsonLee, package = "HistData") # load data -->
<!-- library(ggplot2) # load ggplot2 package -->
<!-- # create ggplot object for repeated use -->
<!-- # we'll be using common aesthetics across multiple geometries -->
<!-- # so we put them in the ggplot function -->
<!-- # also improve the x, y labels -->
<!-- ggheight <- ggplot(data = PearsonLee,  -->
<!--                     mapping = aes(x = parent, y = child)) +  -->
<!--              xlab("parent height (in)") + ylab("child height (in)") -->
<!-- ggheight + geom_point() # scatter plot of child vs parent height -->
<!-- ``` -->
<!-- We see a positive linear association between parent height and child height: as the height of the parent increases, the height of the child also tends to increase. -->
<!-- A simple linear regression model describes the relationship between a response and a predictor variable using the "best fitting" straight line (we'll formalize what best means later). We add the estimated simple linear regression model to our previous plot below using the `geom_smooth` function. The line fits reasonably well. -->
<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add estimated line -->
<!-- ``` -->
<!-- We may also wonder whether the type of parent (father/mother) or child (daughter/son) affects the relationship. We facet our scatter plots based on the `par` and `chl` variables below. While the overall patterns are similar, we notice that Father heights tend to be larger than Mother heights and Son heights tend to be larger than Daughter heights.  -->
<!-- ```{r} -->
<!-- ggheight + geom_point() + -->
<!--   facet_grid(par ~ chl) # facet the data by parent/child type -->
<!-- ``` -->
<!-- Having seen the previous graphic, we may wonder whether we can better model the relationship between parent and child height by accounting for which parent and child were measured. An interaction model assumes that the intercept and slope of each combination of parent/child is the different. We fit and plot an interaction model below. -->
<!-- ```{r} -->
<!-- ggheight + geom_point() + facet_grid(par ~ chl) +   -->
<!--        geom_smooth(method = lm, formula = y ~ x, se = FALSE) # add interaction  model to data -->
<!-- ``` -->
<!-- Other questions we could explore are whether the slopes across the different parent/child combinations are the same, whether the variability of the data is constant as parent height changes, predicting heights outside the range of the observed data, the precision of our estimated model, etc. -->
<!-- Regression analysis will generally be much more complex that was is presented above, but this example hopefully gives you an idea of the kinds of questions regression analysis can help you answer.  -->
<!-- ## Scatter plots and linear regression -->
<p><!-- Scatter plots are a convenient way to study the potential relationship between a single response and a single predictor variable. --></p>
<p><!-- ### Height inheritability --></p>
<p><!-- Karl Pearson (1857-1936) organized the collection of $n=1375$ heights of mothers in the United Kingdom under the age of 65 and one of their adult daughters over the age of 18.  These data are available in the `Heights` data set in the **alr4** package. We are interested in the inheritance from the mother to the daughter, so the mother's height (`mheight`) is used as the predictor variable and the daughter's height (`dheight`) is used as the response variable. --></p>
<p><!-- Questions of interest: --></p>
<p><!-- * Do taller mothers tend to have taller daughters -->
<!-- * Do shorter mothers tend to have shorter daughters? --></p>
<p><!-- ```{r} -->
<!-- data(Heights, package = "alr4") -->
<!-- str(Heights) -->
<!-- plot(dheight ~ mheight, data = Heights, -->
<!--      xlab = "mother's height (in)", -->
<!--      ylab = "daughter's height (in)", -->
<!--      xlim = c(55, 75), ylim = c(55, 75)) -->
<!-- ``` -->
<!-- There seems to be a clear trend between mother's heights and daughter's heights. The taller the mother, the taller the daughter *tends* to be. --></p>
<p><!-- ### Predicting snowfall --></p>
<p><!-- The `ftcollinssnow` data set in the **alr4** package measures late (September 1st until December 31st) and early (January 1st to June 30th) season snowfall for Fort Collins, CO between Late 1900 and Early 1993. --></p>
<p><!-- Question of interest: Can late season snowfall predict snowfall in the early part of the next year? --></p>
<p><!-- ```{r} -->
<!-- data("ftcollinssnow", package = "alr4") # load data -->
<!-- str(ftcollinssnow) # examine structure -->
<!-- plot(Late ~ Early, data = ftcollinssnow) # plot data -->
<!-- # add "line of best fit" -->
<!-- abline(lm(Late ~ Early, data = ftcollinssnow), lty = 2) -->
<!-- # sample mean line -->
<!-- abline(mean(ftcollinssnow$Late), 0) -->
<!-- ``` --></p>
<p><!-- A plot of the snowfall data for the two time periods suggest that this relationship is weak or they may be uncorrelated. -->
<!-- * The dashed line indicates the "linear of best fit" , while the solid line indicates the average of the Late snowfall. --></p>
<p><!-- ### Turkey growth -->
<!-- Pens of turkeys were fed the same diet, except that each pen was supplemented with a `Dose` of amino acid methionine as a percentage of the total diet of the birds. The amino acid methionine was provided using three different `Source`s (one standard and two experimental). The `Weight` gain (g) of the turkeys was measured. These data are available in the `turkey` data in the **alr4** package. --></p>
<p><!-- Questions of interest: --></p>
<p><!-- * Is there a relationship between weight gain of the turkeys and the dose amount? If so, is the relationship linear? -->
<!-- * Does the source of the methionine impact the weight gain of the turkeys? --></p>
<p><!-- Consider a plot of the average `Weight` gain (g) of the turkeys as a function of the `Dose` amount (% of diet), separating the groups by the `Source` of the methionine. --></p>
<p><!-- ```{r} -->
<!-- data(turkey, package = "alr4") -->
<!-- str(turkey) -->
<!-- summary(turkey) # the source factor (S) is not a factor -->
<!-- turkey$S = factor(turkey$S) -->
<!-- levels(turkey$S) <- c("control", "new source a", "new source b") -->
<!-- names(turkey) <- c("Dose", "Gain", "Source", "Replications", "SD") # rename variables -->
<!-- # create turkey data ggplot -->
<!-- library(ggplot2) # load ggplot2 package -->
<!-- gg_turkey <- ggplot(turkey, -->
<!--                     mapping = aes(x = Dose, y = Gain, -->
<!--                                   color = Source, shape = Source)) -->
<!-- gg_turkey + geom_point() + geom_line() -->
<!-- ``` --></p>
<p><!-- Weight gain increases with dose amount, but doesn't appear to be linear. --></p>
<!-- The amino acid source may slightly affect the growth trajectory of the turkeys. -->
<!-- An alternative version of the previous plot using the **lattice** package -->
<!-- ```{r} -->
<!-- library(lattice) # load lattice package -->
<!-- xyplot(Gain ~ Dose, data = turkey, groups = Source, -->
<!--        auto.key = TRUE, type = "b") -->
<!-- ``` -->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-GormanEtAl2014" class="csl-entry">
Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. <span>“Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).”</span> <em>PLOS ONE</em> 9 (3): 1–14. <a href="https://doi.org/10.1371/journal.pone.0090081">https://doi.org/10.1371/journal.pone.0090081</a>.
</div>
<div id="ref-R-palmerpenguins" class="csl-entry">
Horst, Allison, Alison Hill, and Kristen Gorman. 2020. <em>Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data</em>. <a href="https://CRAN.R-project.org/package=palmerpenguins">https://CRAN.R-project.org/package=palmerpenguins</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="defining-and-fitting-a-linear-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-model-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data-Analysis-with-Linear-Regression.pdf", "Data-Analysis-with-Linear-Regression.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
