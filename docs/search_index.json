[["interpreting-a-fitted-linear-model.html", "Chapter 7 Interpreting a fitted linear model 7.1 Orthogonality", " Chapter 7 Interpreting a fitted linear model 7.1 Orthogonality Let \\[\\mathbf{x}_j=(x_{1,j},\\ldots,x_{n,j})^T\\] denote the \\(n\\times 1\\) column vector of observed values for regressor \\(X_j\\). Regressors, \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) are orthogonal if \\(\\mathbf{x}_j^T \\mathbf{x}_k=0\\). Let \\(\\boldsymbol{1}_{n\\times1}\\) denote an \\(n\\times 1\\) column vector of 1s. The definition of orthogonal vectors above implies that \\(\\mathbf{x}_j\\) is orthogonal to \\(\\boldsymbol{1}_{n\\times1}\\) if \\[ \\mathbf{x}_j^T \\boldsymbol{1}_{n\\times1} = \\sum_{i=1}^n x_{i,j} = 0,\\] i.e., if the values in \\(\\mathbf{x}_j\\) sum to zero. Let \\(\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{i,j}\\) denote the sample mean of \\(\\mathbf{x}_j\\) and \\(\\bar{\\mathbf{x}}_j = \\bar{x}_j \\boldsymbol{1}_{n\\times 1}\\) denote the column vector that repeats \\(\\bar{x}_j\\) \\(n\\) times. Centering \\(\\mathbf{x}_j\\) involves subtracting the sample mean of \\(\\mathbf{x}_j\\) from \\(\\mathbf{x}_j\\), i.e., \\(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j\\). Regressors \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) are uncorrelated if they are orthogonal after being centered, i.e., if \\[(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j)^T (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k).\\] Note that the sample covariance between vectors \\(\\mathbf{x}_j\\) and \\(\\mathbf{x}_k\\) is \\[\\begin{align*} \\widehat{\\mathrm{cov}}(\\mathbf{x}_j, \\mathbf{x}_k) &amp;= \\frac{1}{n}\\sum_{i=1}^n (x_i,j - \\bar{x}_j)(x_i,k - \\bar{x}_k) \\\\ &amp;= \\frac{1}{n-1}(\\mathbf{x}_j - \\bar{\\mathbf{x}}_j)^T (\\mathbf{x}_k - \\bar{\\mathbf{x}}_k). \\end{align*}\\] Thus, two centered regressors are orthogonal if their covariance is zero. It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response. Specifically: If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors. Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s. We consider a simple example to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below, we define vectors y, x1, and x2. y &lt;- c(1, 4, 6, 8, 9) # create an arbitrary response vector x1 &lt;- c(7, 5, 5, 7, 7) # create regressor 1 x2 &lt;- c(-1, 2, -3, 1, 5/7) # create regressor 2 to be orthogonal to x1 Regressors x1 and x2 are orthogonal since their crossproduct \\(\\mathbf{x}_1^T \\mathbf{x}_2\\) (in R, crossprod(x1, x2)) equals zero. crossprod(x1, x2) # crossproduct is zero so x1 and x2 are orthogonal ## [,1] ## [1,] 0 In the code below, we regress y on x1 without an intercept (lmod1). The estimated coefficient for x1 is \\(\\hat{\\beta}_1=0.893\\). Next, we then regress y on x1 and x2 without an intercept (lmod2). The estimated coefficients for x1 and x2 are \\(\\hat{\\beta}_1=0.893\\) and \\(\\hat{\\beta}_2=0.221\\), respectively. Because x1 and x2 are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for x1 stays the same in both models. lmod1 &lt;- lm(y ~ x1 - 1) coef(lmod1) ## x1 ## 0.893401 lmod2 &lt;- lm(y ~ x1 + x2 - 1) coef(lmod2) ## x1 x2 ## 0.8934010 0.2210526 The previous models (lmod1 and lmod2) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our \\(\\mathbf{X}\\) matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s. However, neither x1 nor x2 is orthogonal with the column of ones. We define the vector ones below, which is a column of 1s, and compute the crossproduct between ones and the two regressors. Since the crossproducts are not zero, x1 and x2 are not orthogonal to the column of ones. ones &lt;- rep(1, 5) crossprod(ones, x1) # not zero ## [,1] ## [1,] 31 crossprod(ones, x2) # not zero ## [,1] ## [1,] -0.2857143 If we add the column of ones to lmod2 (i.e., if we include the intercept in the model), then the coefficients for both x1 and x2 change because these regressors are not orthogonal to the column of 1s, as verified by the R output below. Comparing lmod2 and lmod3, \\(\\hat{\\beta}_1\\) changes from \\(0.893\\) to \\(0.397\\) and \\(\\hat{\\beta}_2\\) changes from \\(0.221\\) to \\(0.279\\). lmod3 &lt;- lm(y ~ x1 + x2) coef(lmod3) ## (Intercept) x1 x2 ## 3.1547101 0.3969746 0.2791657 For orthogonality of our regressors to be most impactful, the models regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesnt impact the estimated coefficients of the other regressors. In the code below, we define centered regressors x3 and x4 to be uncorrelated, i.e., x3 and x4 have sample mean zero and are orthogonal to each other. x3 &lt;- c(0, -1, 1, 0, 0) # sample mean is zero x4 &lt;- c(0, 0, 0, 1, -1) # sample mean is zero cov(x3, x4) # 0, so x3 and x4 are uncorrelated ## [1] 0 If we fit a linear regression model with any combination of ones, x3, or x4 as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below. coef(lm(y ~ 1)) # only column of 1s coef(lm(y ~ x3 - 1)) # only x3 coef(lm(y ~ x4 - 1)) # only x4 coef(lm(y ~ x3)) # 1s and x3 coef(lm(y ~ x4)) # 1s and x4 coef(lm(y ~ x3 + x4 - 1)) # x3 and x4 coef(lm(y ~ x3 + x4)) # 1s, x3, and x4 We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesnt impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were \\(\\hat{\\beta}_{int}=5.6\\), \\(\\hat{\\beta}_{3}=1.0\\), \\(\\hat{\\beta}_{4}=-0.5\\) when the relevant regressor was included in the model. The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the \\(\\mathbf{X}\\) matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, x1, x2, x3, and x4. crossprod(model.matrix(~ x1 + x2 + x3 + x4)) ## (Intercept) x1 x2 x3 x4 ## (Intercept) 5.0000000 31 -0.2857143 0 0.0000000 ## x1 31.0000000 197 0.0000000 0 0.0000000 ## x2 -0.2857143 0 15.5102041 -5 0.2857143 ## x3 0.0000000 0 -5.0000000 2 0.0000000 ## x4 0.0000000 0 0.2857143 0 2.0000000 Consider the sequence of models below. coef(lm(y ~ 1)) ## (Intercept) ## 5.6 The model with only an intercept has an estimated coefficient of \\(\\hat{\\beta}_{int}=5.6\\). If we add the x1 to the model with an intercept, then both coefficients change because they are not orthogonal to each other. lmod4 &lt;- lm(y ~ x1) # model with 1s and x1 coef(lmod4) ## (Intercept) x1 ## 2.5 0.5 If we add x2 to lmod4, we might think that only \\(\\hat{\\beta}_{int}\\) will change because x1 and x2 are orthogonal to each other. However, because x2 is not orthogonal to all of the other regressors in the model (x1 and the column of 1s), both \\(\\hat{\\beta}_{int}\\) and \\(\\hat{\\beta}_1\\) will change. The easiest way to realize this is to look at lmod2 above with only x1 and x2. When we add the column of 1s to lmod2, both \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term. coef(lm(y ~ x1 + x2)) ## (Intercept) x1 x2 ## 3.1547101 0.3969746 0.2791657 However, note that x3 is orthogonal to the column of 1s and x1. Thus, if we add x3 to lmod4, which includes both a column of 1s and x1, x3 will not change the estimated coefficients for the intercept or x1. coef(lm(y ~ x1 + x3)) ## (Intercept) x1 x3 ## 2.5 0.5 1.0 Additionally, since x4 is orthogonal to the column of 1s, x1, and x3, adding x4 to the previous model will not change the estimated coefficients for any of the other variables already in the model. coef(lm(y ~ x1 + x3 + x4)) ## (Intercept) x1 x3 x4 ## 2.5 0.5 1.0 -0.5 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
