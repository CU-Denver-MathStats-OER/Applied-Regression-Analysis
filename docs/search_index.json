[["index.html", "Joshua French Preliminaries", " Joshua French Joshua French 2021-08-31 Preliminaries I recommend you execute the following commands install packages we may use in this course. # packages related to books books = c(&quot;faraway&quot;, &quot;alr4&quot;, &quot;car&quot;, &quot;rms&quot;) install.packages(books) # packages related to tidy/tidying data tidy = c(&quot;broom&quot;, &quot;tidyr&quot;, &quot;dplyr&quot;) install.packages(tidy) # packages related to plotting moreplots = c(&quot;ggplot2&quot;, &quot;ggthemes&quot;, &quot;lattice&quot;, &quot;HH&quot;) install.packages(moreplots) # packages related to model diagnostics diag = c(&quot;leaps&quot;, &quot;lmtest&quot;, &quot;gvlma&quot;, &quot;caret&quot;) install.packages(diag) # packages related to workflow workflow = c(&quot;remotes&quot;) install.packages(workflow) Lastly, we need to install the perturb package, which is currently not available through the install.packages function. To install this from the package developers GitHub repository, we run the command below in the Console. remotes::install_github(repo = &quot;JohnHendrickx/Perturb&quot;) Acknowledgments The bookdown package (Xie 2021) was used to generate this book. References "],["r-foundations.html", "Chapter 1 R Foundations 1.1 What is R? 1.2 Where to get R (and R Studio Desktop) 1.3 R Studio Layout 1.4 Running code, scripts, and comments 1.5 Packages 1.6 Getting help 1.7 Data types and structures 1.8 Assignment 1.9 Vectors 1.10 Helpful functions 1.11 Data Frames 1.12 Logical statements 1.13 Subsetting with logical statements 1.14 Ecosystem debate", " Chapter 1 R Foundations Meaningful data analysis requires the use of computer software. R statistical software is one of the most popular tools for data analysis both in academia and the broader workforce. In what follows, I will attempt to lay a foundation of basic knowledge and skills with R that you will need for data analysis. I make no attempt to be exhaustive, and many other important aspects of using R (like plotting) will be discussed later, as needed. 1.1 What is R? R is programming language and environment designed for statistical computing. It was introduced by Robert Gentleman and Robert Ihaka in 1993. It is modeled after the S programming language. R is free, open source, and runs on Windows, Macs, Linux, and other types of computers. R is an interactive programming language You type and execute a command in the Console for immediate feedback in contrast to a compiled programming language, which compiles a program that is then executed. R is highly extendable. Many user-created packages are available to extend the functionality beyond what is installed by default. Users can write their own functions and easily add software libraries to R. 1.2 Where to get R (and R Studio Desktop) R may be downloaded from the R Projects website. This link should bring you to the relevant page for downloading the software. R Studio Desktop is a free front end for R provided by R Studio. R Studio Desktop makes doing data analysis with R much easier by adding an Integrated Development Environment (IDE) and providing many other features. Currently, you may download R Studio at this link. You may need to navigate the R Studio website directly if this link no longer functions. Install R and R Studio Desktop before continuing. Then open R Studio Desktop as you continue to learn about R. 1.3 R Studio Layout R Studio Desktop has four panes: Console: the pane where the code is executed. Source: the pane where you prepare commands to be executed. Environment/History: the pane where you can see all the objects in your workspace, your command history, and other things. The Files/Plot/Packages/Help: the pane where you navigate between directories, where plots can be viewed, where you can see the packages available to be loaded, and where you can get help. To see all R Studio panes, press the keys Shift + Ctrl + Alt + 0 1.4 Running code, scripts, and comments Code is executed in R by typing it in the Console and hitting enter. Instead of typing all of your code in the Console and hitting enter, its better to write your code in a Script and execute the code separately. A new script can be obtained by executing File -&gt; New File -&gt; R Script or pressing Ctrl + Shift + n (on a PC) or Cmd + Shift + n on a Mac. There are various ways to run code from a Script file. The most common ones are: Highlight the code you want to run and hit the Run button at the top of the Script pane. Highlight the code you want to run and press Ctrl + Enter on your keyboard. If you dont highlight anything, by default, R Studio runs the command the cursor currently lies on. To save a script, click File -&gt; Save or press Ctrl + s (on a PC) or Cmd + s (on a Mac). A comment is a set of text ignored by R when submitted to the Console. A comment is indicated by the # symbol. Nothing to the right of the # is executed in the Console. To comment (or uncomment) multiple lines in R, highlight the code you want to comment and press Ctrl + Shift + c on a PC or Cmd + Shift + c on a Mac. 1.4.1 Example Perform the following tasks: Type 1+1 in the Console and hit enter. Open a new Script in R Studio. mean(1:3) in your Script file. Type # mean(1:3) in your Script file. Run the commands from the Script using an approach mentioned above. 1.5 Packages Packages are collections of functions, data, and other objects that extend the functionality installed by default in R. R packages can be installed using the install.packages function and loaded using the library function. 1.5.1 Example Practice installing and loading a package by doing the following: Install the set of faraway package by executing the command install.packages(\"faraway\"). Load the faraway package by executing the command library(faraway). 1.6 Getting help There are a number of helps to get help in R. If you know the command for which you want help, then exectue ?command in the Console. * e.g., ?lm * This also may work with data sets, package names, object classes, etc. If you want to search the documentation for a certain topic, then execute ??topic in the Console. * If you need help deciphering an error, identifying packages to perform a certain analysis, how to do something better, then a web search is likely to help. 1.6.1 Example Do the following: Execute ?lm in the Console to get help on the lm function, which is one of the main functions used for fitting linear models. Execute ??logarithms in the Console to search the R documentation for information about logarithms. Do a web search for something along the lines of How do I change the size of the axis labels in an R plot? 1.7 Data types and structures 1.7.1 Basic data types R has 6 basic (atomic) vector types: character - collections of characters. E.g., \"a\", \"hello world!\" double - decimal numbers. e.g., 1.2, 1.0 integer - whole numbers. In R, you must add L to the end of a number to specify it as an integer. E.g., 1L is an integer but 1 is a double. logical - boolean values, TRUE and FALSE complex - complex numbers. E.g., 1+3i raw - a type to hold raw bytes. The typeof function returns the R internal type or storage mode of any object. Consider the following commands and output: # determine basic data type typeof(1) #&gt; [1] &quot;double&quot; typeof(1L) #&gt; [1] &quot;integer&quot; typeof(&quot;hello world!&quot;) #&gt; [1] &quot;character&quot; 1.7.2 Other important object types There are other important types of objects in R that are not basic. We will discuss a few. The R Project manual provides additional information about available types. 1.7.2.1 Numeric An object is numeric if it is of type integer or double. In that case, its mode is said to be numeric. The is.numeric function tests whether an object can be interpreted as numbers. We can use it to determine whether an object is numeric. Some examples: # is the object numeric? is.numeric(&quot;hello world!&quot;) #&gt; [1] FALSE is.numeric(1) #&gt; [1] TRUE is.numeric(1L) #&gt; [1] TRUE 1.7.2.2 NULL NULL is a special object to indicate an object is absent. An object having a length of zero is not the same thing as an object being absent. 1.7.2.3 NA A missing value occurs when the value of something isnt known. R uses the special object NA to represent missing value. If you have a missing value, you should represent that value as NA. Note: \"NA\" is not the same thing as NA. 1.7.2.4 Functions A function is an object the performs a certain action or set of actions based on objects it receives from its arguments. 1.7.3 Data structures R operates on data structures. A data structure is simply some sort of container that holds certain kinds of information R has 5 basic data structures: vector matrix array data frame list Vectors, matrices, and arrays are homogeneous objects that can only store a single data type at a time. Data frames and lists can store multiple data types. Vectors and lists are considered one-dimensional objects. A list is technically a vector. Vectors of a single type are atomic vectors. (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#List-objects) Matrices and data frames are considered two-dimensional objects. Arrays can be n-dimensional objects. This is summarized in the table below, which is based on a table in the first edition of Hadley Wickhams Advanced R. dimensionality homogeneous heterogeneous 1d vector list 2d matrix data frame nd array 1.8 Assignment To store a data structure in the computers memory we must assign it a name. Data structures can be stored using the assignment operator &lt;- or =. Some comments: In general, both &lt;- and = can be used for assignment. Pressing the Alt and - keys simultaneously on a PC or Linux machine (Option and - on a Mac) will insert &lt;- into the R console and script files. If you are creating an R Markdown file, then this command will only insert &lt;- if you are in an R computing environment. &lt;- and = are NOT synonyms, but can be used identically most of the time. Its safest to use &lt;- for assignment. Once an object has been assigned a name, it can be printed by executing the name of the object or using the print function. 1.8.1 Example In the following code, we compute the mean of a vector and print the result. # compute the mean of 1, 2, ..., 10 and assign the name m m &lt;- mean(1:10) m # print m #&gt; [1] 5.5 print(m) # print m a different way #&gt; [1] 5.5 1.9 Vectors A vector is a single-dimensional set of data of the same type. 1.9.1 Creation The most basic way to create a vector is the c function. The c function combines values into a vector or list. e.g., the following commands create vectors of type numeric, character, and logical, respectively. c(1, 2, 5.3, 6, -2, 4) c(\"one\", \"two\", \"three\") c(TRUE, TRUE, FALSE, TRUE) 1.9.2 Creating patterned vectors R provides a number of functions for creating vectors following certain consistent patterns. The seq (sequence) function is used to create an equidistant series of numeric values. Some examples: seq(1, 10): A sequence of numbers from 1 to 10 in increments of 1. 1:10: A sequence of numbers from 1 to 10 in increments of 1. seq(1, 20, by = 2): A sequence of numbers from 1 to 20 in increments of 2. seq(10, 20, len = 100): A sequence of numbers from 10 to 20 of length 100. The rep (replicate) function can be used to create a vector by replicating values. Some examples: rep(1:3, times = 3): Repeat the sequence 1, 2, 3 three times in a row. rep(c(\"trt1\", \"trt2\", \"trt3\"), times = 1:3): Repeat trt1 once, trt2 twice, and trt3 three times. rep(1:3, each = 3): Repeat each element of the sequence 1, 2, 3 three times. 1.9.3 Example Execute the following commands in the Console to see what you get. # vector creation c(1, 2, 5.3, 6, -2, 4) c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;) c(TRUE, TRUE, FALSE, TRUE) # sequences of values seq(1, 10) 1:10 seq(1, 20, by = 2) seq(10, 20, len = 100) # replicated values rep(1:3, times = 3) rep(c(&quot;trt1&quot;, &quot;trt2&quot;, &quot;trt3&quot;), times = 1:3) rep(1:3, each = 3) Vectors can be combined into a new object using the c function. 1.9.4 Example Execute the following commands in the Console v1 &lt;- 1:5 # create a vector v1 # print the vector #&gt; [1] 1 2 3 4 5 print(v1) #&gt; [1] 1 2 3 4 5 v2 &lt;- c(1, 10, 11) # create a new vector new &lt;- c(v1, v2) # combine and assign the combined vectors new # print the combined vector #&gt; [1] 1 2 3 4 5 1 10 11 1.9.5 Categorical vectors Categorical data should be stored as a factor in R. The factor function takes values that can be coerced to a character and converts them to an object of class factor. Some examples: # create some factor variables f1 &lt;- factor(rep(1:6, times = 3)) f1 #&gt; [1] 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 #&gt; Levels: 1 2 3 4 5 6 f2 &lt;- factor(c(&quot;a&quot;, 7, &quot;blue&quot;, &quot;blue&quot;, FALSE)) f2 #&gt; [1] a 7 blue blue FALSE #&gt; Levels: 7 a blue FALSE 1.9.6 Example Create a vector named grp that has two levels: a and b, where the first 7 values are a and the second 4 values are b. 1.9.7 Extracting parts of a vector Subsets of the elements of a vector can be extracted by appending an index vector in square brackets [] to the name of the vector . Lets create the numeric vector 2, 4, 6, 8, 10, 12, 14, 16. # define a sequence 2, 4, ..., 16 a &lt;- seq(2, 16, by = 2) a #&gt; [1] 2 4 6 8 10 12 14 16 Lets access the 2nd, 4th, and 6th elements of a. # extract subset of vector a[c(2, 4, 6)] #&gt; [1] 4 8 12 Lets access all elements in a EXCEPT the 2nd, 4th, and 6th using the minus (-) sign in front of the index vector. # extract subset of vector using minus a[-c(2, 4, 6)] #&gt; [1] 2 6 10 14 16 Lets access all elements in a except elements 3 through 6. a[-(3:6)] #&gt; [1] 2 4 14 16 1.10 Helpful functions 1.10.1 General functions Some general functions commonly used to describe data objects: length(x): length of x sum(x): sum elements in x mean(x): sample mean of elements in x var(x): sample variance of elements in x sd(x): sample standard deviation of elements in x range(x): range (minimum and maximum) of elements in x log(x): (natural) logarithm of elements in x summary(x): a summary of x. Output changes depending on the class of x. str(x): provides information about the structure of x. Usually, the class of the object and some information about its size. 1.10.2 Example Run the following commands in the Console: # common functions x &lt;- rexp(100) # sample 100 iid values from an Exponential(1) distribution length(x) # length of x sum(x) # sum of x mean(x) # sample mean of x var(x) # sample variance of x sd(x) # sample standard deviation of x range(x) # range of x log(x) # logarithm of x summary(x) # summary of x str(x) # structure of x 1.10.3 Functions related to statistical distributions Suppose that a random variable \\(X\\) has the dist distribution: p[dist](q, ...): returns the cdf of \\(X\\) evaluated at q, i.e., \\(p=P(X\\leq q)\\). q[dist](p, ...): returns the inverse cdf (or quantile function) of \\(X\\) evaluated at \\(p\\), i.e., \\(q = \\inf\\{x: P(X\\leq x) \\geq p\\}\\). d[dist](x, ...): returns the mass or density of \\(X\\) evaluated at \\(x\\) (depending on whether its discrete or continuous). r[dist](n, ...): returns an i.i.d. random sample of size n having the same distribution as \\(X\\). The ... indicates that additional arguments describing the parameters of the distribution may be required. Execute ?Distributions to get information about the distributions available in R by default. 1.10.4 Example Execute the following commands in R to see the output. What is each command doing? # statistical calculations pnorm(1.96, mean = 0, sd = 1) qunif(0.6, min = 0, max = 1) dbinom(2, size = 20, prob = .2) dexp(1, rate = 2) rchisq(100, df = 5) pnorm(1.96, mean = 0, sd = 1) returns the probability that a standard normal random variable is less than or equal to 1.96, i.e., \\(P(X \\leq 1.96)\\). qunif(0.6, min = 0, max = 1) returns the value \\(x\\) such that \\(P(X\\leq x) = 0.6\\) for a uniform random variable on the interval \\([0, 1]\\). dbinom(2, size = 20, prob = .2) returns the probability that \\(P(X=2)\\) for \\(X~\\textrm{Binom}(n=20,\\pi=0.2)\\). dexp(1, rate = 2) evaluates the density of an exponential random variable with mean = 1/2 at \\(x=1\\). rchisq(100, df = 5) returns a sample of 100 observations from a chi-squared random variable with 5 degrees of freedom. 1.11 Data Frames Data frames are two-dimensional data objects. Each column of a data frame is a vector (or variable) of possibly different data types. This is a fundamental data structure used by most of Rs modeling software. In general, I recommend tidy data, which means that each variable forms a column of the data frame, and each observation forms a row. 1.11.1 Creation Data frames are created by passing vectors into the data.frame function. The names of the columns in the data frame are the names of the vectors you give the data.frame function. Consider the following simple example. # create basic data frame d &lt;- c(1, 2, 3, 4) e &lt;- c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;, NA) f &lt;- c(TRUE, TRUE, TRUE, FALSE) df &lt;- data.frame(d,e,f) df #&gt; d e f #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE The columns of a data frame can be renamed using the names function on the data frame. # name columns of data frame names(df) &lt;- c(&quot;ID&quot;, &quot;Color&quot;, &quot;Passed&quot;) df #&gt; ID Color Passed #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE The columns of a data frame can be named when you are first creating the data frame by using name = for each vector of data. # create data frame with better column names df2 &lt;- data.frame(ID = d, Color = e, Passed = f) df2 #&gt; ID Color Passed #&gt; 1 1 red TRUE #&gt; 2 2 white TRUE #&gt; 3 3 blue TRUE #&gt; 4 4 &lt;NA&gt; FALSE 1.11.2 Extracting parts of a data frame The column vectors of a data frame may be extracted using $ and specifying the name of the desired vector. df$Color would access the Color column of data frame df. Part of a data frame can also be extracted by thinking of at as a general matrix and specifying the desired rows or columns in square brackets after the object name. For example, if we had a data frame named df: df[1,] would access the first row of df. df[1:2,] would access the first two rows of df. df[,2] would access the second column of df. df[1:2, 2:3] would access the information in rows 1 and 2 of columns 2 and 3 of df. If you need to select multiple columns of a data frame by name, you can pass a character vector with column names in the column position of []. df[, c(\"Color\", \"Passed\")] would extract the Color and Passed columns of df. 1.11.3 Example Execute the following commands in the Console: # Extract parts of a data frame df3 &lt;- data.frame(numbers = 1:5, characters = letters[1:5], logicals = c(TRUE, TRUE, FALSE, TRUE, FALSE)) df3 # print df df3$logicals # access the logicals vector of df3 df3[1, ] # access the first column of df3 df3[, 3] # access the third column of df3 df3[, 2:3] # access the column 2 and 3 of df3 df3[, c(&quot;numbers&quot;, &quot;logicals&quot;)] # access the numbers and logical columns of df3 Students often can work more conveniently with vectors, so it is sometimes desirable to access a part of a data frame and assign it a new name for later use. For example, to access the ID column of df2 and assign it the name newID, we could execute newID &lt;- df2$ID. 1.11.4 Importing Data The read.table function imports data from file into R as a data frame. Usage: read.table(file, header = TRUE, sep = \",\") file is the file path and name of the file you want to import into R. If you dont know the file path, set file = file.choose() will bring up a dialog box asking you to locate the file you want to import. header specifies whether the data file has a header (variable labels for each column of data in the first row of the data file). If you dont specify this option in R or use header = FALSE, then R will assume the file doesnt have any headings. header = TRUE tells R to read in the data as a data frame with column names taken from the first row of the data file. sep specifies the delimiter separating elements in the file. If each column of data in the file is separated by a space, then use sep = \" \" If each column of data in the file is separated by a comma, then use sep = \",\" If each column of data in the file is separated by a tab, then use sep = \"\\t\". Here is an example reading a csv (comma separated file) with a header: # import data as data frame dtf &lt;- read.table(file = &quot;https://raw.githubusercontent.com/jfrench/DataWrangleViz/master/data/covid_dec4.csv&quot;, header = TRUE, sep = &quot;,&quot;) str(dtf) #&gt; &#39;data.frame&#39;: 50 obs. of 7 variables: #&gt; $ state_name: chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... #&gt; $ state_abb : chr &quot;AL&quot; &quot;AK&quot; &quot;AZ&quot; &quot;AR&quot; ... #&gt; $ deaths : int 3831 142 6885 2586 19582 2724 5146 782 19236 9725 ... #&gt; $ population: num 387000 96500 498000 238000 2815000 ... #&gt; $ income : int 25734 35455 29348 25359 31086 35053 37299 32928 27107 28838 ... #&gt; $ hs : num 82.1 91 85.6 82.9 80.7 89.7 88.6 87.7 85.5 84.3 ... #&gt; $ bs : num 21.9 27.9 25.9 19.5 30.1 36.4 35.5 27.8 25.8 27.3 ... Note that the read_table function in the readr package and the fread function in the data.table package are perhaps better ways of reading in tabular data and use similar syntax. 1.12 Logical statements 1.12.1 Basic comparisons Sometimes we need to know if the elements of an object satisfy certain conditions. This can be determined using the logical operators &lt;, &lt;=, &gt;, &gt;=, ==, !=. == means equal to. != means NOT equal to. 1.12.2 Example Execute the following commands in R and see what you get. What is each statement performing? # logical statements # a &lt;- seq(2, 16, by = 2) # creating the vector a a a &gt; 10 a &lt;= 4 a == 10 a != 10 1.12.3 And and Or statements More complicated logical statements can be made using &amp; and |. &amp; means and Only TRUE &amp; TRUE returns TRUE. Otherwise the &amp; operator returns FALSE. | means or Only a single value in an | statement needs to be true for TRUE to be returned. Note that: TRUE &amp; TRUE returns TRUE FALSE &amp; TRUE returns FALSE FALSE &amp; FALSE returns FALSE TRUE | TRUE returns TRUE FALSE | TRUE returns TRUE FALSE | FALSE returns FALSE # relationship between logicals &amp; (and), | (or) TRUE &amp; TRUE #&gt; [1] TRUE FALSE &amp; TRUE #&gt; [1] FALSE FALSE &amp; FALSE #&gt; [1] FALSE TRUE | TRUE #&gt; [1] TRUE FALSE | TRUE #&gt; [1] TRUE FALSE | FALSE #&gt; [1] FALSE 1.12.4 Example Execute the following commands in R and see what you get. # complex logical statements (a &gt; 6) &amp; (a &lt;= 10) (a &lt;= 4) | (a &gt;= 12) 1.13 Subsetting with logical statements Logical statements can be used to return parts of an object satisfying the appropriate criteria. Specifically, we pass logical statements within the square brackets used to access part of a data structure. 1.13.1 Example Execute the following code: # accessing parts of a vector using logicals a a &lt; 6 a[a &lt; 6] a == 10 a[a == 10] (a &lt; 6) | ( a == 10) a[(a &lt; 6) | ( a == 10)] # accessing parts of a data frame # create a logical vector based on whether # a state_abb in dtf is &quot;CA&quot; or &quot;CO&quot; ca_or_co &lt;- is.element(dtf$state_abb, c(&quot;CA&quot;, &quot;CO&quot;)) ca_or_co # access the CA and CA rows of dtf dtf[ca_or_co,] 1.14 Ecosystem debate We will typically work with base R, which are commands and functions R offers by default. The tidyverse (https://www.tidyverse.org) is a collection of R packages that provides a unified framework for data manipulation and visualization. Since this course focuses more on modeling than data manipulation, I will typically focus on approaches in base R. I will use functions from the tidyverse when it greatly simplifies analysis, data manipulation, or visualization. "],["data-exploration.html", "Chapter 2 Data exploration 2.1 Data analysis process 2.2 Data exploration 2.3 Kidney Example 2.4 Visualizing data with base graphics 2.5 Visualizing data with ggplot2 2.6 Summary of data exploration", " Chapter 2 Data exploration Based on Chapter 1 of LMWR2, Chapter 1 of ALR4 2.1 Data analysis process Define a statistical question of interest. Collect relevant data. Analyze the data. Interpret your analysis. Make a decision. The formulation of a problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill - Albert Einstein 2.1.1 Problem Formulation Understand the physical background. Statisticians often work in collaboration with others and need to understand something about the subject area. Understand the objective. What are your goals? Make sure you know what the client wants. Put the problem into statistical terms. This is often the most challenging step and where irreparable errors are sometimes made. That a statistical method can read in and process the data is not enough. The results of an inept analysis may be meaningless. 2.1.2 Data collection Data collection: How the data were collected has a crucial impact on what conclusions can be made. Are the data observational or experimental? Are the data a sample of convenience or were they obtained via a designed sample survey? Is there nonresponse bias? The data you do not see may be just as important as the data you do see. Are there missing values? This is a common problem that is troublesome and time consuming to handle. How are the data coded? How are the qualitative variables represented? What are the units of measurement? Beware of data entry errors and other corruption of the data. Perform some data sanity checks. 2.2 Data exploration An initial exploration of the data should be performed prior to any formal analysis or modeling. Initial data analysis should consist of numerical summaries and appropriate plots. 2.2.1 Numerical summaries of data Statistics can be used to numerically summarize aspects of the data: mean standard deviation (SD) maximum and minimum correlation other measures, as appropriate 2.2.2 Visual summaries of data Plots can provide a useful visual summary of the data. For one numerical variable: boxplots, histograms, density plots, etc. For one categorial variable: bar charts. For two numerical variables: scatter plots. For one numerical and one categorical variables: parallel boxplots or density plots that distinguish between category level. For two categorical variables: panels of bar charts. For three or more variables: one or two variable plots with distinguishing colors or line types, interactive and dynamic graphics. Good graphics are essential in data analysis. They help us to understand our data structure so that we can avoid mistakes. They help us decide on a model. They help communicate the results of our analysis. Graphics can be more convincing than text at times. 2.2.3 What to look for When summarizing the data, look for: outliers data-entry errors skewness unusual distributions patterns or structure 2.3 Kidney Example The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The following variables were recorded: pregnant - number of times pregnant glucose - plasma glucose concentration at 2 hours in an oral glucose tolerance test diastolic - diastolic blood pressure (mm Hg) triceps - triceps skin fold thickness (mm) insulin - 2-hour serum insulin (mu U/ml) bmi - body mass index (weight in kg/(height in m2)) diabetes - diabetes pedigree function age - age (years) test - test whether the patient showed signs of diabetes (coded zero if negative, one if positive). The data may be obtained from the UCI Repository of machine learning databases at https://archive.ics.uci.edu/ml. Lets load and examine the structure of the data data(pima, package = &quot;faraway&quot;) str(pima) # structure #&gt; &#39;data.frame&#39;: 768 obs. of 9 variables: #&gt; $ pregnant : int 6 1 8 1 0 5 3 10 2 8 ... #&gt; $ glucose : int 148 85 183 89 137 116 78 115 197 125 ... #&gt; $ diastolic: int 72 66 64 66 40 74 50 0 70 96 ... #&gt; $ triceps : int 35 29 0 23 35 0 32 0 45 0 ... #&gt; $ insulin : int 0 0 0 94 168 0 88 0 543 0 ... #&gt; $ bmi : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... #&gt; $ diabetes : num 0.627 0.351 0.672 0.167 2.288 ... #&gt; $ age : int 50 31 32 21 33 30 26 29 53 54 ... #&gt; $ test : int 1 0 1 0 1 0 1 0 1 1 ... head(pima) # first six rows #&gt; pregnant glucose diastolic triceps insulin bmi diabetes age test #&gt; 1 6 148 72 35 0 33.6 0.627 50 1 #&gt; 2 1 85 66 29 0 26.6 0.351 31 0 #&gt; 3 8 183 64 0 0 23.3 0.672 32 1 #&gt; 4 1 89 66 23 94 28.1 0.167 21 0 #&gt; 5 0 137 40 35 168 43.1 2.288 33 1 #&gt; 6 5 116 74 0 0 25.6 0.201 30 0 tail(pima) # last six rows #&gt; pregnant glucose diastolic triceps insulin bmi diabetes age test #&gt; 763 9 89 62 0 0 22.5 0.142 33 0 #&gt; 764 10 101 76 48 180 32.9 0.171 63 0 #&gt; 765 2 122 70 27 0 36.8 0.340 27 0 #&gt; 766 5 121 72 23 112 26.2 0.245 30 0 #&gt; 767 1 126 60 0 0 30.1 0.349 47 1 #&gt; 768 1 93 70 31 0 30.4 0.315 23 0 2.3.1 Numerically summarizing the data The summary command is a useful way to numerically summarize a data frame. The summary function will compute the minimum, 0.25 quantile, mean, median, 0.75 quantile, and maximum of a numeric variable. The summary function will count the number of values having each level for a factor variable. Lets summarize the pima data frame. summary(pima) #&gt; pregnant glucose diastolic triceps #&gt; Min. : 0.000 Min. : 0.0 Min. : 0.00 Min. : 0.00 #&gt; 1st Qu.: 1.000 1st Qu.: 99.0 1st Qu.: 62.00 1st Qu.: 0.00 #&gt; Median : 3.000 Median :117.0 Median : 72.00 Median :23.00 #&gt; Mean : 3.845 Mean :120.9 Mean : 69.11 Mean :20.54 #&gt; 3rd Qu.: 6.000 3rd Qu.:140.2 3rd Qu.: 80.00 3rd Qu.:32.00 #&gt; Max. :17.000 Max. :199.0 Max. :122.00 Max. :99.00 #&gt; insulin bmi diabetes age #&gt; Min. : 0.0 Min. : 0.00 Min. :0.0780 Min. :21.00 #&gt; 1st Qu.: 0.0 1st Qu.:27.30 1st Qu.:0.2437 1st Qu.:24.00 #&gt; Median : 30.5 Median :32.00 Median :0.3725 Median :29.00 #&gt; Mean : 79.8 Mean :31.99 Mean :0.4719 Mean :33.24 #&gt; 3rd Qu.:127.2 3rd Qu.:36.60 3rd Qu.:0.6262 3rd Qu.:41.00 #&gt; Max. :846.0 Max. :67.10 Max. :2.4200 Max. :81.00 #&gt; test #&gt; Min. :0.000 #&gt; 1st Qu.:0.000 #&gt; Median :0.000 #&gt; Mean :0.349 #&gt; 3rd Qu.:1.000 #&gt; Max. :1.000 2.3.2 Cleaning the data Cleaning data involves finding and correcting data quality issues. The pima data set has some odd characterics: The minimum diastolic blood pressure is zero. Thats generally an indication of a health problem. The test variable appears to be numeric but should be a factor (categorical) variable. Many other variables have unusual zeros. Look for anything unusual or unexpected, perhaps indicating a data-entry error. Lets look at the first 40 sorted diastolic values. sort(pima$diastolic)[1:40] #&gt; [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [26] 0 0 0 0 0 0 0 0 0 0 24 30 30 38 40 The first 35 values of diastolic are zero. Thats a problem. It seems that 0 was used in place of a missing value. This is very bad since 0 is a real number and this problem may be overlooked, which can lead to faulty analysis! This is why we must check our data carefully for things that dont make sense. The value for missing data in R is NA. Several variables share this problem. Lets set the 0s that should be missing values to NA. pima$diastolic[pima$diastolic == 0] &lt;- NA pima$glucose[pima$glucose == 0] &lt;- NA pima$triceps[pima$triceps == 0] &lt;- NA pima$insulin[pima$insulin == 0] &lt;- NA pima$bmi[pima$bmi == 0] &lt;- NA The test variable is a categorical variable, not numerical. R thinks the test variable is numeric. In R, a categorical variable is a factor. We need to convert the test variable to a factor. Lets convert test to a factor. pima$test &lt;- factor(pima$test) summary(pima$test) #&gt; 0 1 #&gt; 500 268 500 of the cases were negative and 268 were positive. We can provide more descriptive labels using the levels function. We change the 0 and 1 levels to negative and positive to make the data more descriptive. A summary of the updates test variable shows why this is useful. levels(pima$test) &lt;- c(&quot;negative&quot;, &quot;positive&quot;) summary(pima$test) #&gt; negative positive #&gt; 500 268 2.4 Visualizing data with base graphics 2.4.1 Histograms The hist function can be used create a histogram of a numerical vector. The labels of the plot can be customized using the xlab and ylab arguments. The main title of the plot can be customized using the main argument. Here is a slightly customized histogram of diastolic blood pressure. hist(pima$diastolic, xlab = &quot;diastolic blood pressure&quot;, main = &quot;&quot;) The histogram is approximately bell-shaped and centered around 70. We can change the number of breaks in the histogram by specifying the breaks argument of the hist function. Consider how the plot changes below. hist(pima$diastolic, xlab = &quot;diastolic blood pressure&quot;, main = &quot;&quot;, breaks = 20) 2.4.2 Density plots Many people prefer the density plot over the histogram because the histogram is more sensitive to its options. A density plot is essentially a smoothed version of a histogram. It isnt as blocky. It sometimes has weird things happen at the boundaries. The plot and density function can be combined to construct a density plot. plot(density(pima$diastolic, na.rm = TRUE), main = &quot;&quot;) In the example above, we have to specify na.rm = TRUE so that the density is only computed using the non-missing values. 2.4.3 Index plots An index plot is a scatter plot of a numeric variable versus the index of each value (i.e., the position of the value in the vector). This is most useful for sorted vectors. A scatter plot of sorted numeric values versus their index can be used to identify outliers and see whether the data has many repeated values. plot(sort(pima$diastolic), ylab = &quot;sorted diastolic bp&quot;) The flat spots in the plot above show that the diastolic variable has many repeated values. 2.4.4 Bivariate scatter plots Bivariate scatter plots can be used to identify the relationship between two numeric variables. A scatter plot of diabetes vs diastolic blood pressure is shown below. plot(diabetes ~ diastolic, data = pima) There is no clear pattern in the points, so its difficult to claim a relationship between the two variables. 2.4.5 Bivariate boxplots A parallel boxplot of diabetes score versus test result is shown below. plot(diabetes ~ test, data = pima) The median diabetes score seems to be a bit higher for positive tests in comparison to the negative tests. 2.4.6 Multiple plots in one figure The par function can be used to construct multiple plots in one figure. The mfrow argument can be used to specify the number of rows and columns of plots you need. A 1 by 2 set of plots is shown below. par(mfrow = c(1, 2)) plot(diabetes ~ diastolic, data = pima) plot(diabetes ~ test, data = pima) par(mfrow = c(1, 1)) # reset to a single plot 2.5 Visualizing data with ggplot2 The previous plots were created using Rs base graphics system. base graphics are fast and simple to produce while looking professional. A fancier alternative is to construct plots using the ggplot2 package. In its simplest form, to construct a (useful) plot in ggplot2, you need to provide: A ggplot object. This is usually the object that holds your data frame. The data frame is passed to ggplot via the data argument. A geometry object Roughly speaking, this is the kind of plot you want. e.g., geom_hist for a histogram, geom_point for a scatter plot, geom_density for a density plot. An aesthetic mapping Aesthetic mappings describe how variables in the data are mapped to visual properties of a geometry. This is where you specify which variable with be the x variable, the y variable, which variable will control color in the plots, etc. 2.5.1 A ggplot2 histogram library(ggplot2) ggpima &lt;- ggplot(pima) ggpima + geom_histogram(aes(x=diastolic)) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 35 rows containing non-finite values (stat_bin). 2.5.2 A ggplot2 density plot ggpima + geom_density(aes(x = diastolic)) #&gt; Warning: Removed 35 rows containing non-finite values (stat_density). 2.5.3 A ggplot2 scatter plot ggpima + geom_point(aes(x = diastolic, y = diabetes)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.4 Scaling ggplot2 plots In general, scaling is the process by which ggplot2 maps variables to unique values. When this is done for discrete variables, ggplot2 will often scale the variable to distinct colors, symbols, or sizes, depending on the aesthetic mapped. In the example below, we map the test variable to the shape aesthetic, which is then scaled to different shapes for the different test levels. ggpima + geom_point(aes(x = diastolic, y = diabetes, shape = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). Alternatively, we can map the test variable to the color aesthetic, which creates a plot with different colors for each observation based on the test level. ggpima + geom_point(aes(x = diastolic, y = diabetes, color = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). We can even combine these two aesthetic mappings in a single plot to get different colors and symbols for each level of test. ggpima + geom_point(aes(x = diastolic, y = diabetes, shape = test, color = test)) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.5 Facetting in ggplot2 Facetting creates separate panels (facets) of a data frame based on one or more facetting variables. Below, we facet the data by the test result. ggpima + geom_point(aes(x = diastolic, y = diabetes)) + facet_grid(~ test) #&gt; Warning: Removed 35 rows containing missing values (geom_point). 2.5.6 Summary of ggplot2 To create a ggplot2 plot: Create aggplot object using the ggplot function. Specify the data frame the data is contained in (e.g., the data frame is pima). Specify the geometry for the plot (the kind of plot you want to produce) Specify the aesthetics using aes. The aesthetic specifies what you see, such as position in the \\(x\\) or \\(y\\) direction or aspects such as shape or color. The aesthetic can be specified in the geometry, or if you have consistent aesthetics across multiple geometries, in the ggplot statement. The advantage of ggplot2 is more apparent in producing complex plots involving more than two variables. ggplot2 makes it easy to plot the data for each group with different colors, symbols, line types, etc. ggplot2 will automatically provide a legend mapping the attributes to the different groups. ggplot2 makes it easy to create separate panels with plots for the observations having a certain characteristic. 2.6 Summary of data exploration You should use both numerical and graphical summaries of data prior to modeling data. Data exploration helps us to: Gain understanding about our data Identify problems or unusual features of our data Identify patterns in our data Decide on a modeling approach for the data etc. "],["review-of-probability-random-variables-and-random-vectors.html", "Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics 3.2 Random Variables 3.3 Multivariate distributions 3.4 Random vectors 3.5 Properties of transformations of random vectors 3.6 Multivariate normal (Gaussian) distribution 3.7 Example 1 3.8 Example 2", " Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. A set is a (possibly empty) collection of elements. Sets are denoted as a set of elements between curly braces, i.e., \\(\\{\\omega_1, \\omega_2, \\ldots\\}\\), where the \\(\\omega_i\\) are elements of \\(\\Omega\\). Set \\(A\\) is a subset of set \\(B\\) if every element of \\(A\\) is an element of \\(B\\). This is denoted as \\(A \\subseteq B\\), meaning that \\(A\\) is a subset of \\(B\\). Subsets of \\(\\Omega\\) are events. The null set or empty set, \\(\\emptyset\\), is the set with no elements, i.e., \\(\\{\\}\\). The empty set is a subset of any other set. A function \\(P\\) that assigns a real number \\(P(A)\\) to every event \\(A\\) is a probability distribution if it satisfies three properties: \\(P(A)\\geq 0\\) for all \\(A\\in \\Omega\\) \\(P(\\Omega)=P(\\omega \\in \\Omega) = 1\\) If \\(A_1, A_2, \\ldots\\) are disjoint, then \\(P\\left(\\bigcup_{i=1}^\\infty A_i \\right)=\\sum_{i=1}^\\infty P(A_i)\\). A set of events \\(\\{A_i:i\\in I\\}\\) are independent if \\[P\\left(\\cap_{i\\in J} A_i \\right)=\\prod_{i\\in J} P(A_i ) \\] for every finite subset \\(J\\subseteq I\\). 3.2 Random Variables A random variable \\(Y\\) is a mapping/function \\[Y:\\Omega\\to\\mathbb{R}\\] that assigns a real number \\(Y(\\omega)\\) to each outcome \\(\\omega\\). We typically drop the \\((\\omega)\\) part for simplicity. The cumulative distribution function (CDF) of \\(Y\\), \\(F_Y\\), is a function \\(F_Y:\\mathbb{R}\\to [0,1]\\) defined by \\[F_Y (y)=P(Y \\leq y).\\] The subscript of \\(F\\) indicates the random variable the CDF describes. E.g., \\(F_X\\) denotes the CDF of the random variable \\(X\\) and \\(F_Y\\) denotes the CDF of the random variable \\(Y\\). The subscript can be dropped when the context makes it clear what random variable the CDF describes. The support of \\(Y\\), \\(\\mathcal{S}\\), is the smallest set such that \\(P(Y\\in \\mathcal{S})=1\\). 3.2.1 Discrete random variables \\(Y\\) is a discrete random variable if it takes countably many values \\(\\{y_1, y_2, \\dots \\} = \\mathcal{S}\\). The probability mass function (pmf) for \\(Y\\) is \\(f_Y (y)=P(Y=y)\\), where \\(y\\in \\mathbb{R}\\), and must have the following properties: \\(0 \\leq f_Y(y) \\leq 1\\). \\(\\sum_{y\\in \\mathcal{S}} f_Y(y) = 1\\). Additionally, the following statements are true: \\(F_Y(c) = P(Y \\leq c) = \\sum_{y\\in \\mathcal{S}:y \\leq c} f_Y(y)\\). \\(P(Y \\in A) = \\sum_{y \\in A} f_Y(y)\\) for some event \\(A\\). \\(P(a \\leq Y \\leq b) = \\sum_{y\\in\\mathcal{S}:a\\leq y\\leq b} f_Y(y)\\). The expected value, mean, or first moment of \\(Y\\) is defined as \\[E(Y) = \\sum_{y\\in \\mathcal{S}} y f_Y(y),\\] assuming the sum is well-defined. The variance of \\(Y\\) is defined as \\[var(Y)=E(Y-E(Y))^2== \\sum_{y\\in \\mathcal{S}} (y - E(Y))^2 f_Y(y).\\] The standard deviation of Y is \\[SD(Y)=\\sqrt{var(Y) }.\\] 3.2.1.1 Example (Bernoulli) A random variable \\(Y\\sim \\mathsf{Bernoulli}(\\pi)\\) if \\(\\mathcal{S} = {0, 1}\\) and \\(P(Y = 1) = \\pi\\), where \\(\\pi\\in (0,1)\\). The pmf of a Bernoulli random variable is \\[f_Y(y) = \\pi^y (1-\\pi)^{(1-y)}.\\] Determine \\(E(Y)\\) and \\(var(Y)\\). 3.2.2 Continuous random variables \\(Y\\) is a continuous random variable if there exists a function \\(f_Y (y)\\) such that: \\(f_Y (y)\\geq 0\\) for all \\(y\\), \\(\\int_{-\\infty}^\\infty f_Y (y) dy = 1\\), \\(a\\leq b\\), \\(P(a&lt;Y&lt;b)=\\int_a^b f_Y (y) dy\\). The function \\(f_Y\\) is called the probability density function (pdf). Additionally, \\(F_Y (y)=\\int_{-\\infty}^y f_Y (y) dy\\) and \\(f_Y (y)=F&#39;_Y(y)\\) for any point \\(y\\) at which \\(F_Y\\) is differentiable. The expected value of a continuous random variables \\(Y\\) is defined as \\[E(Y)= \\int_{-\\infty}^{\\infty} y f_Y(y) dy = \\int_{y\\in\\mathcal{S}} y f_Y(y).\\] assuming the integral is well-defined. The variance of a continuous random variable \\(Y\\) is defined by \\[var(Y)=E(Y-E(Y))^2=\\int_{-\\infty}^{\\infty} (y - E(Y))^2 f_Y(y) dy = \\int_{y\\in\\mathcal{S}} (y - E(Y))^2 f_Y(y) dy.\\] The standard deviation of Y is \\[SD(Y)=\\sqrt{var(Y) }.\\] 3.2.3 Useful facts for transformation of random variables Let \\(Y\\) be a random variable and \\(c\\in\\mathbb{R}\\) be a constant. Then: \\(E(cY) = c E(Y)\\) \\(E(c + Y) = c + E(Y)\\) \\(var(cY) = c^2 var(Y)\\) \\(var(c + Y) = var(Y)\\) 3.3 Multivariate distributions 3.3.1 Basic properties Let \\(Y_1,Y_2,\\ldots,Y_n\\) denote \\(n\\) random variables with supports \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_n\\), respectively. If the random variables are jointly (all) discrete, then the joint pmf \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)\\) satisfies the following properties: \\(0\\leq f(y_1,\\ldots,y_n )\\leq 1\\), \\(\\sum_{y_1\\in\\mathcal{S}_1}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\sum_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n)\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n}y_1 \\cdots y_n f(y_1,\\ldots,y_n).\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n),\\] where \\(g\\) is a function of the random variables. If the random variables are jointly continuous, then \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,\\ldots,Y_n=y_n)\\) is the joint pdf if it satisfies the following properties: \\(f(y_1,\\ldots,y_n ) \\geq 0\\), \\(\\int_{y_1\\in\\mathcal{S}_1}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n ) dy_n \\cdots dy_1 = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\int \\cdots \\int_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n) dy_n\\ldots dy_1\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} y_1 \\cdots y_n f(y_1,\\ldots,y_n) dy_n \\ldots dy_1.\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,\\ldots,y_n) dy_n \\cdots dy_1,\\] where \\(g\\) is a function of the random variables. 3.3.2 Marginal distributions If the random variables are jointly discrete, then the marginal pmf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\sum_{y_2\\in\\mathcal{S}_2}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n).\\] Similarly, if the random variables are jointly continuous, then the marginal pdf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\int_{y_2\\in\\mathcal{S}_2}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n) dy_n \\cdots dy_2.\\] 3.3.3 Independence of random variables Random variables \\(X\\) and \\(Y\\) are independent if \\[F(x, y) = F_X(x) F_Y(y).\\] Alternatively, \\(X\\) and \\(Y\\) are independent if \\[f(x, y) = f_X(x)f_Y(y).\\] 3.3.4 Conditional distributions Let \\(X\\) and \\(Y\\) be random variables. The conditional distribution of \\(X\\) given \\(Y = y\\), denoted \\(X|Y=y\\) is \\[ f(x|y) = f(x, y)/f_{Y}(y).\\] 3.3.5 Covariance The covariance between random variables \\(X\\) and \\(Y\\) is \\[cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).\\] 3.3.6 Useful facts for transformations of multiple random variables Let \\(a\\) and \\(b\\) be scalar constants. Then: \\(E(aY)=aE(Y)\\) \\(E(a+Y)=a+E(Y)\\) \\(E(aY+bZ)=aE(Y)+bE(Z)\\) \\(var(aY)=a^2 var(Y)\\) \\(var(a+Y)=var(Y)\\) \\(cov(aY,bZ)=ab cov(Y,Z).\\) \\(var(Y+Z)=var(Y)+var(Z)+2cov(Y,Z).\\) 3.4 Random vectors 3.4.1 Definition Let \\(\\mathbf{y}=(Y_1,Y_2,\\dots,Y_n )^T\\) be an \\(n\\times1\\) vector of random variables. \\(\\mathbf{y}\\) is a random vector. A vector is always defined to be a column vector, even if the notation is ambiguous. 3.4.2 Mean, variance, and covariance The mean of a random vector is \\[E(\\mathbf{y})=\\begin{pmatrix}E(Y_1)\\\\E(Y_2)\\\\\\vdots\\\\E(Y_n)\\end{pmatrix}.\\] The variance (covariance) of a random vector is \\[\\begin{aligned} var(\\mathbf{y}) &amp;= E(\\mathbf{y}\\mathbf{y}^T )-E(\\mathbf{y})E(\\mathbf{y})^T\\\\ &amp;= \\begin{pmatrix}var(Y_1) &amp; cov(Y_1,Y_2) &amp;\\dots &amp;cov(Y_1,Y_n)\\\\cov(Y_2,Y_1 )&amp;var(Y_2)&amp;\\dots&amp;cov(Y_2,Y_n)\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ cov(Y_n,Y_1)&amp;cov(Y_n,Y_2)&amp;\\dots&amp;var(Y_n)\\end{pmatrix}\\end{aligned}.\\] Let \\(\\mathbf{x} = (X_1, X_2, \\ldots, X_n)^T\\) and \\(\\mathbf{y} = (Y_1, Y_2, \\ldots, Y_n)^T\\) be \\(n\\times 1\\) random vectors. The covariance between two random vectors is \\[cov(\\mathbf{x}, \\mathbf{y}) = E(\\mathbf{x}, \\mathbf{y}^T) - E(\\mathbf{x}) E(\\mathbf{y})^T.\\] 3.5 Properties of transformations of random vectors Define: \\(\\mathbf{a}\\) to be an \\(n\\times 1\\) vector of constants \\(A\\) to be an \\(m\\times n\\) matrix of constants \\(\\mathbf{x}=(X_1,X_2,\\ldots,X_n)^T\\) to be an \\(n\\times 1\\) random vector \\(\\mathbf{y}=(Y_1,Y_2,\\ldots,Y_n)^T\\) to be an \\(n\\times 1\\) random vector \\(\\mathbf{z}=(Z_1,Z_2,\\ldots,Z_n)^T\\) to be an \\(n\\times 1\\) random vector \\(0_{n\\times n}\\) to be an \\(n\\times n\\) matrix of zeros. Then: \\(E(A\\mathbf{y})=AE(\\mathbf{y}), E(\\mathbf{y}A^T )=E(\\mathbf{y}) A^T.\\) \\(E(\\mathbf{x}+\\mathbf{y})=E(\\mathbf{x})+E(\\mathbf{y})\\) \\(var(A\\mathbf{y})=A\\ var(\\mathbf{y}) A^T\\) \\(cov(\\mathbf{x}+\\mathbf{y},\\mathbf{z})=cov(\\mathbf{x},\\mathbf{z})+cov(\\mathbf{y},\\mathbf{z})\\) \\(cov(\\mathbf{x},\\mathbf{y}+\\mathbf{z})=cov(\\mathbf{x},\\mathbf{y})+cov(\\mathbf{x},\\mathbf{z})\\) \\(cov(A\\mathbf{x},\\mathbf{y})=A\\ cov(\\mathbf{x},\\mathbf{y})\\) \\(cov(\\mathbf{x},A\\mathbf{y})=cov(\\mathbf{x},\\mathbf{y}) A^T\\) \\(var(a)= 0_{n\\times n}\\) \\(cov(\\mathbf{a},\\mathbf{y})=0_{n\\times n}\\) \\(var(\\mathbf{a}+\\mathbf{y})=var(\\mathbf{y})\\) 3.6 Multivariate normal (Gaussian) distribution 3.6.1 Definition \\(\\mathbf{y}=(Y_1,\\dots,Y_n )^T\\) has a multivariate normal distribution with mean \\(\\mathbf{\\mu}\\) (an \\(n\\times 1\\) vector) and covariance \\(\\mathbf{\\Sigma}\\) (an \\(n\\times n\\) matrix) if the joint pdf is \\[f(\\mathbf{y})=\\frac{1}{(2\\pi)^{n/2} |\\mathbf{\\Sigma}|^{1/2} } \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{y}-\\mathbf{\\mu})\\right).\\] Note that \\(\\mathbf{\\Sigma}\\) must be symmetric and positive definite. We would denote this as \\(\\mathbf{y}\\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma)}\\). 3.6.2 Useful facts Important fact: A linear function of a multivariate normal random vector (i.e., \\(\\mathbf{a}+A\\mathbf{y}\\)) is also multivariate normal (though it could collapse to a single random variable if \\(A\\) is a \\(1\\times n\\) vector). Application: Suppose that \\(\\mathbf{y}\\sim N(\\mu,\\Sigma)\\). For an \\(m\\times n\\) matrix of constants \\(A\\), \\(A\\mathbf{y}\\sim N(A\\mu,A\\Sigma A^T)\\). 3.7 Example 1 3.7.1 Bernoulli distribution A random variable $Y() with \\(\\Omega = {0, 1}\\) and \\(P(Y=1)=\\theta\\). The pmf of a Bernoulli random variable is \\[f(y|\\theta)=\\theta^y (1-\\theta)^{(1-y)}.\\] Determine \\(E(Y)\\) Determine \\(var(Y)\\) 3.8 Example 2 Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers. Let \\(Y_1\\) denote the proportion of the capacity of the bulk tank that is available after the tank is stocked at the beginning of the week. Because of the limited supplies, \\(Y_1\\) varies from week to week. Let \\(Y_2\\) denote the proportion of the capacity of the bulk tank that is sold during the week. Because \\(Y_1\\) and \\(Y_2\\) are both proportions, both variables are between 0 and 1. Further, the amount sold, \\(y_2\\), cannot exceed the amount available, \\(y_1\\). Suppose the joint density function for \\(Y_1\\) and \\(Y_2\\) is given by \\[f(y_1,y_2 )=3y_1;\\ 0 \\leq y_2\\leq y_1\\leq 1.\\] 3.8.1 Problem 1 Determine \\(P(0\\leq Y_1\\leq 0.5;\\ 0.25\\leq Y_2)\\) 3.8.2 Problem 2 Determine \\(f_{Y_1 }\\) and \\(f_{Y_2 }\\) 3.8.3 Problem 3 Determine \\(E(Y_1)\\) and \\(E(Y_2)\\) 3.8.4 Problem 4 Determine \\(var(Y_1)\\) and \\(var(Y_2)\\) 3.8.5 Problem 5 Determine \\(E(Y_1 Y_2)\\) 3.8.6 Problem 6 Determine \\(cov(Y_1,Y_2)\\) 3.8.7 Problem 7 Determine the mean and variance of \\(\\mathbf{a}^T \\mathbf{y}\\), where \\(\\mathbf{a}=(1,-1)^T\\) and \\(\\mathbf{y}=(Y_1,Y_2 )^T\\). This is the expectation and variance of the difference between the amount of gas available and the amount of gas sold. "],["useful-matrix-facts.html", "Chapter 4 Useful matrix facts 4.1 Notation 4.2 Basic mathematical properties 4.3 Transpose and related properties 4.4 Special matrices 4.5 Matrix inverse 4.6 Matrix derivatives", " Chapter 4 Useful matrix facts A matrix is a two-dimensional array of values, symbols, or other objects (depending on the context). We will assume that our matrices contain numbers or random variables. Context will make it clear which is being represented. Matrices are commonly denoted by bold capital letters like \\(\\mathbf{A}\\) or \\(\\mathbf{B}\\), but this will sometimes be simplified to capital letters like \\(A\\) or \\(B\\). 4.1 Notation A matrix \\(\\mathbf{A}\\) with \\(m\\) rows and \\(n\\) columns (an \\(m\\times n\\) matrix) will be denoted as \\[\\mathbf{A} = \\begin{bmatrix} \\mathbf{A}_{1,1} &amp; \\mathbf{A}_{2,1} &amp; \\cdots &amp; \\mathbf{A}_{1,n} \\\\ \\mathbf{A}_{2,1} &amp; \\mathbf{A}_{2,1} &amp; \\cdots &amp; \\mathbf{A}_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbf{A}_{m,1} &amp; \\mathbf{A}_{m,2} &amp; \\cdots &amp; \\mathbf{A}_{m,n} \\\\ \\end{bmatrix}, \\] where \\(\\mathbf{A}_{i,j}\\) denotes the element in row \\(i\\) and column \\(j\\) of matrix \\(\\mathbf{A}\\). A column vector is a matrix with a single column. A row vector is a matrix with a single row. Vectors are commonly denoted with bold lowercase letters such as \\(\\mathbf{a}\\) or \\(\\mathbf{b}\\), but this may be simplified to lowercase letters such as \\(a\\) or \\(b\\). A \\(p\\times 1\\) column vector \\(\\mathbf{a}\\) may constructed as \\[\\mathbf{a} = [a_1, a_2, \\ldots, a_p]^T = \\begin{bmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_p \\end{bmatrix}^T = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{bmatrix}.\\] 4.2 Basic mathematical properties 4.2.1 Addition and subtraction Consider matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) with identical sizes \\(m\\times n\\). We add \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) by adding the element in position \\(i,j\\) of \\(\\mathbf{B}\\) with the element in position \\(i,j\\) of \\(A\\), i.e., \\[(\\mathbf{A} + \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} + \\mathbf{B}_{i,j}.\\] Similarly, if we subtract \\(\\mathbf{B}\\) from matrix \\(\\mathbf{A}\\), then we subtract the element in position \\(i,j\\) of \\(\\mathbf{B}\\) from the element in position \\(i,j\\) of \\(\\mathbf{A}\\), i.e., \\[(\\mathbf{A} - \\mathbf{B})_{i,j} = \\mathbf{A}_{i,j} - \\mathbf{B}_{i,j}.\\] Example: \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix} + \\begin{bmatrix} 2 &amp; 9 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 11 &amp; 4 \\\\ 5 &amp; 8 &amp; 7 \\\\ \\end{bmatrix}.\\] \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix} - \\begin{bmatrix} 2 &amp; 9 &amp; 1 \\\\ 1 &amp; 3 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} -1 &amp; -7 &amp; 2 \\\\ 3 &amp; 2 &amp; 5 \\\\ \\end{bmatrix}.\\] 4.2.2 Scalar multiplication A matrix multiplied by a scalar value \\(c\\in\\mathbb{R}\\) is the matrix obtained by multiplying each element of the matrix by \\(c\\). If \\(\\mathbf{A}\\) is a matrix and \\(c\\in \\mathbb{R}\\), then \\[(c\\mathbf{A})_{i,j} = c\\mathbf{A}_{i,j}.\\] Example: \\[3\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix}= \\begin{bmatrix} 3\\cdot 1 &amp; 3\\cdot 2 &amp; 3\\cdot 3 \\\\ 3\\cdot 4 &amp; 3\\cdot 5 &amp; 3\\cdot 6 \\\\ \\end{bmatrix}= \\begin{bmatrix} 3 &amp; 6 &amp; 9 \\\\ 12 &amp; 15 &amp; 18 \\\\ \\end{bmatrix}.\\] 4.2.3 Matrix multiplication Consider two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). The matrix product \\(\\mathbf{AB}\\) is only defined if the number of columns in \\(\\mathbf{A}\\) matches the number of rows in \\(\\mathbf{B}\\). Assume \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n\\times p\\) matrix. \\(\\mathbf{AB}\\) will be an \\(m\\times p\\) matrix and \\[(\\mathbf{AB})_{i,j} = \\sum_{k=1}^{n} \\mathbf{A}_{i,k}\\mathbf{B}_{k,j}.\\] Example: \\[\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 4\\\\ 2 &amp; 5\\\\ 3 &amp; 6 \\end{bmatrix}= \\begin{bmatrix} 1\\cdot 1 + 2 \\cdot 2 + 3 \\cdot 3 &amp; 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6\\\\ 4\\cdot 1 + 5 \\cdot 2 + 6 \\cdot 3 &amp; 4 \\cdot 4 + 5 \\cdot 5 + 6 \\cdot 6\\\\ \\end{bmatrix}= \\begin{bmatrix} 14 &amp; 32\\\\ 32 &amp; 77\\\\ \\end{bmatrix}.\\] 4.2.4 Associative property Addition and multiplication satisfy the associative property for matrices. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then \\[(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\] and \\[(\\mathbf{AB})\\mathbf{C}=\\mathbf{A}(\\mathbf{BC}).\\] 4.2.5 Distributive property Matrix operations satisfy the distributive property. Assuming that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to do the operations below, then \\[\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB} + \\mathbf{AC}\\quad\\mathrm{and}\\quad (\\mathbf{A}+\\mathbf{B})\\mathbf{C} = \\mathbf{AC} + \\mathbf{BC}.\\] 4.2.6 No commutative property In general, matrix multiplication does not satisfy the commutative property, i.e., \\[AB \\neq BA,\\] even when the matrix sizes allow the operation to be performed. Example: \\[\\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 5 \\end{bmatrix}\\] \\[ \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 2\\\\ 2 &amp; 4 \\end{bmatrix}\\] 4.3 Transpose and related properties 4.3.1 Definition The transpose of a matrix, denoted \\(T\\) as a superscript, exchanges the rows and columns of the matrix. More formally, the \\(i,j\\) element of \\(\\mathbf{A}^T\\) is the \\(j,i\\) element of \\(\\mathbf{A}\\), i.e., \\((\\mathbf{A}^T)_{i,j} = \\mathbf{A}_{j,i}\\). Example: \\[\\begin{bmatrix} 2 &amp; 9 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}^T = \\begin{bmatrix} 2 &amp; 4\\\\ 9 &amp; 5\\\\ 3 &amp; 6 \\end{bmatrix}.\\] 4.3.2 Transpose and mathematical operations Assume that the matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), and \\(\\mathbf{C}\\) have the sizes required to perform the operations below. Additionally, assume that \\(c\\in \\mathbb{R}\\) is a scalar constant. The following properties are true: \\(c^T = c\\) \\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\) \\((\\mathbf{AB})^T = \\mathbf{B}^T \\mathbf{A}^T\\), which can be extended to \\((\\mathbf{ABC})^T=\\mathbf{C}^T \\mathbf{B}^T \\mathbf{A}^T\\), etc. \\((\\mathbf{A}^T)^T=\\mathbf{A}\\) 4.4 Special matrices 4.4.1 Square matrices A matrix is square if the number of rows equals the number of columns. The diagonal elements of an \\(n\\times n\\) square matrix \\(\\mathbf{A}\\) are the elements \\(\\mathbf{A}_{i,i}\\) for \\(i = 1, 2, \\ldots, n\\). Any non-diagonal elements of \\(\\mathbf{A}\\) are called off-diagonal elements. 4.4.2 Identity matrix The \\(n\\times n\\) identity matrix \\(\\mathbf{I}_{n\\times n}\\) is 1 for its diagonal elements and 0 for its off-diagonal elements. Context often makes it clear what the dimensions of an identity matrix are, so \\(\\mathbf{I}_{n\\times n}\\) is often simplified to \\(\\mathbf{I}\\) or \\(I\\). Example: \\[\\mathbf{I}_{3\\times 3} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] 4.4.3 Symmetric A matrix \\(\\mathbf{A}\\) is symmetric if \\(\\mathbf{A} = \\mathbf{A}^T\\), i.e., \\(\\mathbf{A}_{i,j} = \\mathbf{A}_{j,i}\\) for all potential \\(i,j\\). A symmetric matrix must be square. 4.4.4 Idempotent A matrix is idempotent if \\(\\mathbf{AA} = \\mathbf{A}\\) An idempotent matrix must be square. 4.5 Matrix inverse An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is invertible if there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{AB}=\\mathbf{BA}=\\mathbf{I}_{n\\times n}\\). The inverse of \\(\\mathbf{A}\\) is denoted \\(\\mathbf{A}^{-1}\\). Inverse matrices only exist for square matrices. Some other properties related to the inverse operator: If \\(n\\times n\\) matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are invertible then \\((\\mathbf{AB})^{-1} = \\mathbf{B}^{-1} \\mathbf{A} ^{-1}\\). If \\(\\mathbf{A}\\) is invertible then \\((\\mathbf{A}^{-1})^T = (\\mathbf{A}^T)^{-1}\\). 4.6 Matrix derivatives We start with some basic calculus results. Let \\(f(b)\\) be a function of a scalar value \\(b\\) and \\(\\frac{df(b)}{db}\\) denote the derivative of the function with respect to \\(b\\). Assume \\(x\\) is a fixed value. Then the following is true: \\(f(b)\\) \\(\\frac{df(b)}{db}\\) \\(bx\\) \\(x\\) \\(b^2\\) \\(2b\\) \\(x b^2\\) \\(2bx\\) Now lets look at the deriviate of a scalar function with respect to a vector. Let \\(f(\\mathbf{b})\\) be a function of a \\(p\\times 1\\) column vector \\(\\mathbb{b}=[b_1, b_2, \\ldots, b_p]^T\\). The derivative of \\(f(\\mathbf{b})\\) with respect to \\(\\mathbf{b}\\) is denoted \\(\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}}\\) and \\[\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}} = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{b})}{\\partial b_1}\\\\ \\frac{\\partial f(\\mathbf{b})}{\\partial b_2}\\\\ \\vdots \\\\ \\frac{\\partial f(\\mathbf{b})}{\\partial b_p} \\end{bmatrix}.\\] Assume \\(\\mathbf{X}\\) is a fixed matrix. The following is true: \\(f(\\mathbf{b})\\) \\(\\frac{\\partial f(\\mathbf{b})}{\\partial \\mathbf{b}}\\) \\(\\mathbf{b}^T \\mathbf{X}\\) \\(\\mathbf{X}\\) \\(\\mathbf{b}^T \\mathbf{b}\\) \\(2\\mathbf{b}\\) \\(\\mathbf{b}^T \\mathbf{X} \\mathbf{b}\\) \\(2\\mathbf{X}\\mathbf{b}\\) "],["defining-a-linear-model.html", "Chapter 5 Defining a linear model 5.1 Background and terminology 5.2 Goals of regression 5.3 Definition of a linear model 5.4 Summarizing the components of a linear model 5.5 Types of regression models", " Chapter 5 Defining a linear model Based on Chapter 2 of LMWR2, Chapter 2 and 3 of ALR4 5.1 Background and terminology Regression models are used to model the relationship between: one or more response variables and one or more predictor variables. The distinction between these two variables is their purpose in the model. Predictor variables are used to predict the value of the response variable. Response variables are also known as outcome, output, or dependent variables. Predictor variables are also known as explanatory, regressor, input, dependent, or feature variables. Note: Because the variables in our model are often interrelated, describing these variables as independent or dependent variables are vague and are best avoided. A distinction is sometimes made between regression models and classification models. In that case: Regression models attempt to predict a numerical response. Classification models attempt to predict the category level a response will have. A linear model is a regression model in which the regression coefficients (to be discussed later) enter the model linearly. A linear model is just a specific type of regression model. 5.2 Goals of regression The basic goals of a regression model are to: Predict future or unknown response values based on specified values of the predictors. What will the selling price of a home be? Identify relationships (associations) between predictor variables and the response. What is the general relationship between the selling price of a home and the number of bedrooms the home has? With our regression model, we also hope to be able to: Generalize our results from the sample to the a larger population of interest. E.g., we want to extend our results from a small set of college students to all college students. Infer causality between our predictors and the response. E.g., if we give a person a vaccine, then this causes the persons risk of catching the disease to decrease. A true model doesnt exist for real data, so that isnt the goal of a regression analysis. A regression analysis should attempt to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.) 5.3 Definition of a linear model 5.3.1 Basic construction and relationships We begin by defining notation for the objects we will need and clarifying some of their important properties. \\(Y\\) denotes the response variable. The response variable is treated as a random variable. We will observe realizations of this random variable for each observation in our data set. \\(X\\) denotes a single predictor variable. \\(X_1\\), \\(X_2\\), , \\(X_{p-1}\\) will denote the predictor variables when there is more than one predictor variable. The predictor variables are treated as non-random varables. We will observe values of the predictors variables for each observation in our data set. \\(\\beta_0\\), \\(\\beta_1\\), , \\(\\beta_{p-1}\\) denote regression coefficients. Regression coefficients are statistical parameters that we will estimate from our data. Like all statistical parameters, regression coefficients are treated as fixed (non-random) but unknown values. Regression coefficients are not observable. \\(\\epsilon\\) denotes error. The error is not observable. The error is treated as a random variable. The error is assumed to have mean 0, i.e., \\(E(\\epsilon) = 0\\). Since \\(E(\\epsilon) = 0\\) and \\(X\\) is non-random, the expectation of \\(\\epsilon\\) conditional on \\(X\\) is also 0, i.e., \\(E(\\epsilon | X) = 0\\). In this context, error doesnt mean mistake or malfunction. \\(\\epsilon\\) is simply the deviation of the response from its mean. Then a linear model for \\(Y\\) is defined by the equation \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon. \\tag{5.1} \\end{equation}\\] We now emphasize the relationship between the response, the mean response, and the error. The mean of the response variable will depend on the values of the predictor variables. Thus, we can only discuss the expectation of the response variable conditional on the values of the predictor variables. This is denoted as \\(E(Y | X_1, \\ldots, X_{p-1})\\). For simplicity, assume our linear model only has a single predictor (this is an example of simple linear regression). Based on what weve presented, we have that \\[\\begin{align} E(Y|X) &amp;= E(\\beta_0 + \\beta_1 X + \\epsilon | X) \\\\ &amp;= E(\\beta_0 | X) + E(\\beta_1 X | X) + E(\\epsilon | X) \\\\ &amp;= \\beta_0 + \\beta_1 X + 0\\\\ &amp;= \\beta_0 + \\beta_1 X. \\end{align}\\] The second line follows from the fact that the expectation of a sum of random variables is the sum of the expectation of the random variables. The third line follows from the fact that the expected value of a constant (non-random) value is the constant (the regression coefficients and \\(X\\) are non-random) and by our assumption that the errors have mean 0 (unconditionally or conditionally on the predictor variable.) Thus, we see that we see that for a simple linear regression model \\[ Y = E(Y|X) + \\epsilon.\\] For a model with multiple predictors, this extends to \\[Y = E(Y|X_1, X_2, \\ldots, X_{p-1}) + \\epsilon.\\] Thus, our response may be written as the sum of the mean response conditional on the predictors, \\(E(Y|X_1, X_2, \\ldots, X_{p-1})\\), and the error. This is why previously we discussed the fact that the error is simply the deviation of the response from its mean. 5.3.2 As a system of equations A linear regression analysis will model the data using a linear model. Suppose we have sampled \\(n\\) observations from a population. We now introduce some additional notation: \\(Y_1, Y_2, \\ldots, Y_n\\) denote the response values for the \\(n\\) observations. \\(x_{i,j}\\) denotes the observed value of predictor \\(j\\) for observation \\(i\\). We use lowercase \\(x\\) to indicate that this is the observed value of the predictor. \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) denote the errors for the \\(n\\) observations. The linear model relating the responses, the predictors, and the errors is defined by the system of equations \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\tag{5.2} \\end{equation}\\] Based on our previous work, we can also write Equation (5.2) as \\[\\begin{equation} Y_i = E(Y_i | X_1 = x_{i,1}, \\ldots, X_{p-1} = x_{i,p-1}) + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\end{equation}\\] 5.3.3 Using matrix notation The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices. We define the following notation: \\(\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]^T\\) denotes the column vector containing the \\(n\\) responses. \\(\\mathbf{X}\\) denotes the matrix containing a column of 1s and the observed predictor values, specifically, \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p-1} \\end{bmatrix}.\\] \\(\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]^T\\) denotes the column vector containing the \\(p\\) regression coefficients. \\(\\boldsymbol{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]^T\\) denotes the column vector contained the \\(n\\) errors. Then the system of equations defining the linear model in (5.2) can be written as \\[\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\boldsymbol{\\epsilon}.\\] Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model. 5.4 Summarizing the components of a linear model We have already introduced a lot of objects. To aid in making sense of their notation, their purpose in the model, whether they can be observed, and whether they are modeled as a random variable (vector) or fixed, non-random values, we summarize things below. Weve already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation (5.2) is for the errors to be random. We summarize this information in the table below for the objects previously discussed using the various notations introduced. Notation Description Observable Random \\(Y\\) response variable Yes Yes \\(Y_i\\) response value for the \\(i\\)th observation Yes Yes \\(\\mathbf{y}\\) the \\(n\\times 1\\) column vector of response values Yes Yes \\(X\\) predictor variable Yes No \\(X_j\\) the \\(j\\)th predictor variable Yes No \\(x_{i,j}\\) the value of the \\(j\\)th predictor variable for the \\(i\\)th observation Yes No \\(\\mathbf{X}\\) the \\(n\\times p\\) matrix of predictor values Yes No \\(\\beta_j\\) the regression coefficient associated with the \\(j\\)th predictor variable No No \\(\\boldsymbol{\\beta}\\) the \\(p\\times 1\\) column vector of regression coefficients No No \\(\\epsilon\\) the error No Yes \\(\\epsilon_i\\) the error associated with observation \\(i\\) No Yes \\(\\boldsymbol{\\epsilon}\\) the \\(n\\times 1\\) column vector of errors No Yes 5.5 Types of regression models The are many named types of regression models. You may hear or see people use these terms when describing their model. Here is a brief overview of some common regression models. Name Defining characteristics Simple an intercept term and one predictor variable Multiple more than one predictor variable Multivariate more than one response variable Linear the regression coefficients enter the model linearly Analysis of variance (ANOVA) predictors are all categorical Analysis of covariance (ANCOVA) at least one quantitative predictor and at least one categorical predictor Generalized linear model (GLM) a type of generalized regression model when the responses do not come from a normal distribution. "],["fitting-a-linear-model.html", "Chapter 6 Fitting a linear model", " Chapter 6 Fitting a linear model "],["interpreting-a-fitted-linear-model.html", "Chapter 7 Interpreting a fitted linear model", " Chapter 7 Interpreting a fitted linear model "],["categorical-predictors.html", "Chapter 8 Categorical predictors 8.1 Indicator/dummy variables 8.2 Common of linear models with categorical predictors", " Chapter 8 Categorical predictors Categorical predictors can greatly improve the explanatory power or predictive capability of a fitted model when different patterns exist for different levels of the categorical variables. In what follows, we consider several common linear regression models that involve a categorical variable. To simplify our discussion, we only consider the setting where there is a single categorical variable to add to our model. Similarly, we only consider the setting where there is a single numeric variable. We begin by defining some notation. Let \\(X\\) denote a numeric regressor, with \\(x_i\\) denoting the value of \\(X\\) for observation \\(i\\). Let \\(F\\) denote a categorical variable with levels \\(L_1, L_2, \\ldots, L_K\\). The \\(F\\) stands for factor, while the \\(L\\) stands for level. The notation \\(f_i\\) denotes the value of \\(F\\) for observation \\(i\\). 8.1 Indicator/dummy variables We may recall that if \\(\\mathbf{X}\\) denotes our matrix of regressors and \\(\\mathbf{y}\\) our vector of responses, then (assuming the columns of \\(\\mathbf{X}\\) are linearly independent) the OLS solution for \\(\\boldsymbol{\\beta}\\) is \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\\] In order to compute the estimated coefficients, both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) must contain numeric values. How can we use a categorical predictor in our regression model when the levels are not numeric values? In order to use a categorical predictor in a regression model, we must transform it into a set of one or more indicator or dummy variables, which we explain in more detail below. An indicator function is a function that takes the value 1 of a certain property is true and 0 otherwise. An indicator variable is the variable that results from applying an indicator function to each observation in a data set. Many notations exist for indicator functions. We will adopt the notation \\[\\begin{equation*} I_S(x) = \\begin{cases} 1 &amp; \\textrm{if}\\;x \\in S\\\\ 0 &amp; \\textrm{if}\\;x \\notin S \\end{cases}, \\end{equation*}\\] which is shorthand for a function that returns 1 if \\(x\\) is in the set \\(S\\) and 0 otherwise. We let \\(D_j\\) denote the indicator (dummy) variable for factor level \\(L_j\\) of \\(F\\). The value of \\(D_j\\) for observation \\(i\\) is denoted \\(d_{i,j}\\), with \\[d_{i,j} = I_{\\{L_j\\}}(f_i),\\] i.e., \\(d_{i,j}\\) is 1 if observation \\(i\\) has factor level \\(L_j\\) and 0 otherwise. 8.2 Common of linear models with categorical predictors It is common to use notation like \\(E(Y|X_1, X_2) = \\beta_0 + \\beta_1 X_1 +\\beta_2 X_2\\) when discussing linear regression models. That notation is generally simple and convenient, but can be unclear. Asking a researcher what the estimate of \\(\\beta_2\\) is in a model is ambiguous because it will depend on the order the researcher added the variables to the model. To more closely connect each coefficient with the regressor to which it is related, we will use the notation \\(\\beta_X\\) to denote the coefficient for regressor \\(X\\) and \\(\\beta_{D_j}\\) to denote the coefficient for regressor \\(D_j\\). Similarly, \\(\\beta_{int}\\) denotes the intercept included in our model. 8.2.1 One-way ANOVA 8.2.1.1 Definition A one-way analysis of variance (ANOVA) assumes a constant mean for each level of a categorical variable. A general one-way ANOVA relating a response variable \\(Y\\) to a factor variable \\(F\\) with \\(K\\) levels may be formulated as \\[E(Y|F) = \\beta_{int} + \\beta_{D_2} D_2 + \\ldots + \\beta_{D_K} D_K.\\] Alternatively, in terms of the individual responses, we may formulate the one-way ANOVA model as \\[Y_i = \\beta_{int} + \\beta_{D_2} d_{i,2} + \\cdots + \\beta_{D_K} d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] This may bring up some questions that need answering. Why does the one-way ANOVA model only contains dummy variables for the last \\(K-1\\) levels of \\(F\\)? This is not a mistake. If we included dummy variables for all levels of \\(F\\), then the matrix of regressors would have linearly dependent columns because the sum of the dummy variables for all \\(K\\) levels would equal the column of 1s for the intercept. Why do we omit the dummy variable for the first level of \\(F\\)? This is simply convention. We could omit the dummy variable for any single level of \\(F\\). However, it is conventional to designate one level the reference level and to omit that variable. As we will see when discussing interpretation of the coefficient, the reference level becomes the level of \\(F\\) that all other levels are compared to. Could we omit the intercept instead of one of the dummy variables? Yes, you could. There is no mathematical or philosophical issues with this. However, this can create problems when you construct models including both categorical and numeric regressors. The standard approach is recommended because it typically makes our model easier to interpret and extend. 8.2.1.2 Interpretation We interpret the coefficients of our one-way ANOVA with respect to the change in the mean response. Suppose an observation of level \\(L_1\\). We can determine that the mean response is \\[\\begin{align*} E(Y|F=L_1) &amp;= \\beta_{int} + \\beta_{D_2} 0 + \\cdots + \\beta_{D_K} 0 \\\\ &amp;= \\beta_{int}. \\end{align*}\\] Similarly, when an observation has level \\(L_2\\), then \\[\\begin{align*} E(Y|F=L_2) &amp;= \\beta_{int} + \\beta_{D_2} 1 + \\beta_{D_3} 0 + \\cdots + \\beta_{D_K} 0 \\\\ &amp;= \\beta_{int} + \\beta_{D_2}. \\end{align*}\\] This helps us to see the general relationship that \\[E(Y|F=L_j) = \\beta_{int} + \\beta_{D_j},\\quad j=2,\\ldots,K.\\] In the context of a one-way ANOVA: \\(\\beta_{int}\\) represents the expected response for observations having the reference level. \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\), represents the expected change in the response when comparing observations having the reference level and level \\(L_j\\). You can verify this by computing \\(E(Y|F=L_j) - E(Y|F=L_1)\\) (for \\(j = 2, \\ldots, K\\)). 8.2.2 Main effects models A main effects model is also called a parallel lines model since the regression equations for each factor level produce lines parallel to one another. A parallel lines model is formulated as \\[ Y_i = \\beta_{int} + \\beta_{X} x_i + \\beta_{L_2} d_{i,2} + \\cdots + \\beta_{L_K} d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] When an observation has level \\(L_1\\), then the expected response is \\(\\beta_{int} + \\beta_1 X\\). More specifically, \\[E(Y|X = x, F=L_1) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 0 + \\cdots + \\beta_{L_K} 0 = \\beta_{int} + \\beta_X x.\\] Thus, \\(E(Y|X = 0, F=L_1) = \\beta_{int}.\\) When an observation has level \\(L_2\\), the expected response is \\(\\beta_{int} + \\beta_{X} X + \\beta_{L_2}\\). More formally, \\[E(Y|X = x, F=L_2) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 1 + \\beta_{L_3} 0 + \\cdots + \\beta_{L_K} 0 = \\beta_{int} + \\beta_X x + \\beta_{L_2}.\\] Thus, the mean response for observations having level \\(L_2\\) is \\(\\beta_{int} + \\beta_{L_2} + \\beta_{X} x\\). In general, \\[E(Y|X = x, F=L_j) = \\beta_{int} + \\beta_X x + \\beta_{L_j},\\quad j = 2,\\ldots,K.\\] Thus, \\[E(Y|X=x, F=L_j) - E(Y|X=x, F=L_1) = (\\beta_{int} + \\beta_X x + \\beta_{L_j}) - (\\beta_{int} + \\beta_X x) = \\beta_{L_j}.\\] In the context of parallel lines models: \\(\\beta_{int}\\) represents the expected response for observations having the reference level when the numeric regressor \\(X = 0\\). \\(\\beta_X\\) is the expected change in the response when \\(X\\) increases by 1 unit for a fixed level of \\(F\\). \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\) represents the expected change in the response when comparing observations having level \\(L_1\\) and \\(L_j\\) with \\(X\\) fixed at the same value. 8.2.3 Interaction models An interaction model is also called a separate lines model since the regression equations for each factor level produce lines that are distinct and separate. A separate lines model is formulated as \\[ Y_i = \\beta_{int} + \\beta_{X} x_i + \\beta_{L_2} d_{i,2} + \\cdots + \\beta_{L_K} d_{i,K} + + \\beta_{X L_2} x_i d_{i,2} + \\cdots + \\beta_{X L_j} x_i d_{i,K} + \\epsilon_i,\\quad i=1,2,\\ldots n.\\] When an observation has level \\(L_1\\), then the expected response is \\(\\beta_{int} + \\beta_1 X\\). More specifically, \\[E(Y|X = x, F=L_1) = \\beta_{int} + \\beta_X x + \\beta_{L_2} 0 + \\cdots + \\beta_{L_K} 0 + \\beta_{X L_2} x_i 0 + \\cdots + \\beta_{X L_K} x_i 0 = \\beta_{int} + \\beta_X x.\\] Thus, \\(E(Y|X = 0, F=L_1) = \\beta_{int}.\\) When an observation has level \\(L_j\\), for \\(j=2,\\ldots,K\\), the expected response is \\(\\beta_{int} + \\beta_{X} X + \\beta_{L_j} + \\beta_{X L_J} X.\\) More formally, \\[E(Y|X = x, F=L_j) = \\beta_{int} + \\beta_X x + \\beta_{L_j} + \\beta_{X L_j} x.\\] Note that \\[E(Y|X=0, F=L_1) = \\beta_{int}.\\] Additionally, we note that \\[E(Y|X=0, F=L_j) - E(Y|X=0, F=L_1) = (\\beta_{int} + \\beta_X 0 + \\beta_{L_j} + \\beta_{X L_J} 0) - (\\beta_{int} + \\beta_X 0) = \\beta_{L_j}.\\] In the context of separate lines models: \\(\\beta_{int}\\) represents the expected response for observations having the reference level when the numeric regressor \\(X = 0\\). \\(\\beta_{L_j}\\), for \\(j=2,\\ldots,K\\), represents the expected change in the response when comparing observations having level \\(L_1\\) and \\(L_j\\) with \\(X=0\\). \\(\\beta_X\\) represents the expected change in the response when \\(X\\) increases by 1 unit for observations having the reference level. \\(\\beta_X L_j\\), for \\(j=2,\\ldots,K\\), represents the difference in the expected rate of change when \\(X\\) increases by 1 unit for observations have the baseline level in comparison to level \\(L_j\\). 8.2.4 Extensions In the models above, we have only discussed possibilities with a single numeric variable and a single factor variable. Naturally, one can consider models with multiple factor variables, multiple numeric variables, interactions between factor variables, interactions between numeric variables, etc. The models become more complicated, but the ideas are similar. One simply has to keep track of what role each coefficient plays in the model. "],["assessing-and-addressing-collinearity.html", "Chapter 9 Assessing and addressing collinearity", " Chapter 9 Assessing and addressing collinearity "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
