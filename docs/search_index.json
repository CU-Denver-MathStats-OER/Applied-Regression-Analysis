[
["review-of-probability-random-variables-and-random-vectors.html", "Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics 3.2 Random Variables 3.3 Multivariate distributions 3.4 Random vectors 3.5 Multivariate normal (Gaussian) distribution 3.6 Example", " Chapter 3 Review of probability, random variables, and random vectors 3.1 Probability Basics 3.1.1 Definitions The sample space \\(\\Omega\\) is the set of possible outcomes of an experiment. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. A set is a (possibly empty) collection of elements. Sets are denoted as a set of elements between curly braces, i.e., \\(\\{\\omega_1, \\omega_2, \\ldots\\}\\), where the \\(\\omega_i\\) are elements of \\(\\Omega\\). Set \\(A\\) is a subset of set \\(B\\) if every element of \\(A\\) is an element of \\(B\\). This is denoted as \\(A \\subseteq B\\), meaning that \\(A\\) is a subset of \\(B\\). Subsets of \\(\\Omega\\) are events. The null set or empty set, \\(\\emptyset\\), is the set with no elements \\(\\{\\}\\). The empty set is a subset of any other set. A function \\(P\\) that assigns a real number \\(P(A)\\) to every event \\(A\\) is a probability distribution if it satisfies three properties: \\(P(A)\\geq 0\\) for all \\(A\\in \\Omega\\) \\(P(\\Omega)=P(\\omega \\in \\Omega) = 1\\) If \\(A_1, A_2, \\ldots\\) are disjoint, then \\(P\\left(\\bigcup_{i=1}^\\infty A_i \\right)=\\sum_{i=1}^\\infty P(A_i)\\). A set of events \\(\\{A_i:i\\in I\\}\\) are independent if \\[P\\left(\\cap_{i\\in J} A_i \\right)=\\prod_{i\\in J} P(A_i ) \\] for every finite subset \\(J\\subseteq I\\). 3.2 Random Variables A random variable \\(Y\\) is a mapping/function \\[Y:\\Omega\\to\\mathbb{R}\\] that assigns a real number \\(Y(\\omega)\\) to each outcome \\(\\omega\\). We typically drop the \\((\\omega)\\) part for simplicity. The cumulative distribution function (CDF) of \\(Y\\), \\(F_Y\\), is a function \\(F_Y:\\mathbb{R}\\to [0,1]\\) defined by \\[F_Y (y)=P(Y \\leq y).\\] The subscript of \\(F\\) indicates the random variable the CDF describes. E.g., \\(F_X\\) denotes the CDF of the random variable \\(X\\) and \\(F_Y\\) denotes the CDF of the random variable \\(Y\\). The subscript can be dropped when the context makes it clear what random variable the CDF describes. The support of \\(Y\\), \\(\\mathcal{S}\\), is the smallest set such that \\(P(Y\\in \\mathcal{S})=1\\). 3.2.1 Discrete random variables \\(Y\\) is a discrete random variable if it takes countably many values \\(\\{y_1, y_2, \\dots \\} = \\mathcal{S}\\). The probability mass function (pmf) for \\(Y\\) is \\(f_Y (y)=P(Y=y)\\), where \\(y\\in \\mathbb{R}\\), and must have the following properties: \\(0 \\leq f_Y(y) \\leq 1\\). \\(\\sum_{y\\in \\mathcal{S}} f_Y(y) = 1\\). Additionally, the following statements are true: \\(F_Y(c) = P(Y \\leq c) = \\sum_{y\\in \\mathcal{S}:y \\leq c} f_Y(y)\\). \\(P(Y \\in A) = \\sum_{y \\in A} f_Y(y)\\) for some event \\(A\\). \\(P(a \\leq Y \\leq b) = \\sum_{y\\in\\mathcal{S}:a\\leq y\\leq b} f_Y(y)\\). The expected value, mean, or first moment of \\(Y\\) is defined as \\[E(Y) = \\sum_{y\\in \\mathcal{S}} y f_Y(y),\\] assuming the sum is well-defined. The variance of \\(Y\\) is defined as \\[var(Y)=E(Y-E(Y))^2== \\sum_{y\\in \\mathcal{S}} (y - E(Y))^2 f_Y(y).\\] The standard deviation of Y is \\[SD(Y)=\\sqrt{var(Y) }.\\] 3.2.1.1 Example (Bernoulli) A random variable \\(Y\\sim \\mathsf{Bernoulli}(\\pi)\\) if \\(\\mathcal{S} = {0, 1}\\) and \\(P(Y = 1) = \\pi\\), where \\(\\pi\\in (0,1)\\). The pmf of a Bernoulli random variable is \\[f_Y(y) = \\pi^y (1-\\pi)^{(1-y)}.\\] Determine \\(E(Y)\\) and \\(var(Y)\\). 3.2.2 Continuous random variables \\(Y\\) is a continuous random variable if there exists a function \\(f_Y (y)\\) such that: \\(f_Y (y)\\geq 0\\) for all \\(y\\), \\(\\int_{-\\infty}^\\infty f_Y (y) dy = 1\\), \\(a\\leq b\\), \\(P(a&lt;Y&lt;b)=\\int_a^b f_Y (y) dy\\). The function \\(f_Y\\) is called the probability density function (pdf). Additionally, \\(F_Y (y)=\\int_{-\\infty}^y f_Y (y) dy\\) and \\(f_Y (y)=F&#39;_Y(y)\\) for any point \\(y\\) at which \\(F_Y\\) is differentiable. The expected value of a continuous random variables \\(Y\\) is defined as \\[E(Y)= \\int_{-\\infty}^{\\infty} y f_Y(y) dy = \\int_{y\\in\\mathcal{S}} y f_Y(y).\\] assuming integral are well-defined. The variance of a continuous random variable \\(Y\\) is defined by \\[var(Y)=E(Y-E(Y))^2=\\int_{-\\infty}^{\\infty} (y - E(Y))^2 f_Y(y) dy = \\int_{y\\in\\mathcal{S}} (y - E(Y))^2 f_Y(y).\\] The standard deviation of Y is \\[SD(Y)=\\sqrt{var(Y) }.\\] 3.2.3 Useful facts for transformation of random variables Let \\(Y\\) be a random variable and \\(a\\in\\mathbb{R}\\) be a constant. Then: \\(E(aY) = a E(Y)\\) \\(E(a + Y) = a + E(Y)\\) \\(var(aY) = a^2 var(Y)\\) \\(var(a + Y) = var(Y)\\) 3.3 Multivariate distributions 3.3.1 Basic properties Let \\(Y_1,Y_2,\\ldots,Y_n\\) denote \\(n\\) random variables with supports \\(\\mathcal{S}_1,\\mathcal{S}_2,\\ldots,\\mathcal{S}_n\\), respectively. If the random variables are jointly (all) discrete, then the joint pmf \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,…,Y_n=y_n)\\) satisfies the following properties: \\(0\\leq f(y_1,…,y_n )\\leq 1\\), \\(\\sum_{y_1\\in\\mathcal{S}_1}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,…,y_n ) = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\sum_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n)\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n}y_1 \\cdots y_n f(y_1,⋯,y_n).\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\sum_{y_1\\in\\mathcal{S}_1} \\cdots \\sum_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,⋯,y_n),\\] where \\(g\\) is a function of the random variables. If the random variables are jointly continuous, then \\(f(y_1,\\ldots,y_n)=P(Y_1=y_1,…,Y_n=y_n)\\) is the joint pdf if it satisfies the following properties: \\(f(y_1,…,y_n ) \\geq 0\\), \\(\\int_{y_1\\in\\mathcal{S}_1}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,…,y_n ) dy_n \\cdots dy_1 = 1\\), \\(P((Y_1,\\ldots,Y_n)\\in A)=\\int \\cdot \\int_{(y_1,\\ldots,y_n) \\in A} f(y_1,\\ldots,y_n) dy_n\\ldots dy_1\\). In this context, \\[E(Y_1 \\cdots Y_n)=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} y_1 \\cdots y_n f(y_1,⋯,y_n) dy_n \\cdots dy_1.\\] In general, \\[E(g(Y_1,\\ldots,Y_n))=\\int_{y_1\\in\\mathcal{S}_1} \\cdots \\int_{y_n\\in\\mathcal{S}_n} g(y_1, \\ldots, y_n) f(y_1,⋯,y_n) dy_n \\cdots dy_1,\\] where \\(g\\) is a function of the random variables. 3.3.2 Marginal distributions If the random variables are jointly discrete, then the marginal pmf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\sum_{y_2\\in\\mathcal{S}_2}\\cdots \\sum_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n).\\] Similarly, if the random variables are jointly continuous, then the marginal pdf of \\(Y_1\\) is \\[f_{Y_1}(y_1)=\\int_{y_2\\in\\mathcal{S}_2}\\cdots \\int_{y_n\\in\\mathcal{S}_n} f(y_1,\\ldots,y_n) dy_n \\cdots dy_2.\\] 3.3.3 Independence of random variables Random variables \\(X\\) and \\(Y\\) are independent if \\[F(x, y) = F_X(x) F_Y(y).\\] Alternatively, \\(X\\) and \\(Y\\) are independent if \\[f(x, y) = f_X(x)f_Y(y).\\] 3.3.4 Conditional distributions Let \\(X\\) and \\(Y\\) be random variables. The conditional distribution of \\(X\\) given \\(Y = y\\), denoted \\(X|Y=y\\) is \\[ f(x|y) = f(x, y)/f_{Y}(y).\\] ### Covariance The covariance between random variables \\(X\\) and \\(Y\\) is \\[cov(X,Y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y).\\] 3.3.5 Useful facts for transformations of multiple random variables Let \\(a\\) and \\(b\\) be scalar constants. Then: \\(E(aY)=aE(Y)\\) \\(E(a+Y)=a+E(Y)\\) \\(E(aY+bZ)=aE(Y)+bE(Z)\\) \\(var(aY)=a^2 var(Y)\\) \\(var(a+Y)=var(Y)\\) \\(cov(aY,bZ)=ab cov(Y,Z).\\) \\(var(Y+Z)=var(Y)+var(Z)+2cov(Y,Z).\\) 3.4 Random vectors 3.4.1 Definition Let \\(\\mathbf{y}=(Y_1,Y_2,\\dots,Y_n )^T\\) be an \\(n\\times1\\) vector of random variables. \\(\\mathbf{y}\\) is a random vector. A vector is always defined to be a column vector, even if the notation is ambiguous. 3.4.2 Mean, variance, and covariance \\[E(\\mathbf{y})=\\begin{pmatrix}E(Y_1)\\\\E(Y_2)\\\\\\vdots\\\\E(Y_n)\\end{pmatrix}.\\] \\[\\begin{aligned} var(\\mathbf{y}) &amp;= E(\\mathbf{y}\\mathbf{y}^T )-E(\\mathbf{y})E(\\mathbf{y})^T\\\\ &amp;= \\begin{pmatrix}var(Y_1) &amp; cov(Y_1,Y_2) &amp;\\dots &amp;cov(Y_1,Y_n)\\\\cov(Y_2,Y_1 )&amp;var(Y_2)&amp;\\dots&amp;cov(Y_2,Y_n)\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ cov(Y_n,Y_1)&amp;cov(Y_n,Y_2)&amp;…&amp;var(Y_n)\\end{pmatrix}\\end{aligned}.\\] Let \\(\\mathbf{x} = (X_1, X_2, \\ldots, X_n)^T\\) and \\(\\mathbf{y} = (Y_1, Y_2, \\ldots, Y_n)^T\\) be \\(n\\times 1\\) random vectors. \\[cov(\\mathbf{x}, \\mathbf{y}) = E(\\mathbf{x}, \\mathbf{y}^T) - E(\\mathbf{x}) E(\\mathbf{y})^T.\\] ## Properties of transformations of random vectors Define: \\(a\\) to be an \\(n\\times 1\\) vector of constants \\(A\\) to be an \\(m\\times n\\) matrix of constants \\(\\mathbf{z}=(Z_1,Z_2,…,Z_n)^T\\) to be \\(n\\times 1\\) random vector. \\(0_{n\\times n}\\) to be an \\(n\\times n\\) matrix of zeros. Then: \\(E(A\\mathbf{y})=AE(\\mathbf{y}), E(\\mathbf{y}A^T )=E(\\mathbf{y}) A^T.\\) \\(E(\\mathbf{x}+\\mathbf{y})=E(\\mathbf{x})+E(\\mathbf{y})\\) \\(var(A\\mathbf{y})=A\\ var(\\mathbf{y}) A^T\\) \\(cov(\\mathbf{x}+\\mathbf{y},\\mathbf{z})=cov(\\mathbf{x},\\mathbf{z})+cov(\\mathbf{y},\\mathbf{z})\\) \\(cov(\\mathbf{x},\\mathbf{y}+\\mathbf{z})=cov(\\mathbf{x},\\mathbf{y})+cov(\\mathbf{x},\\mathbf{z})\\) \\(cov(A\\mathbf{x},\\mathbf{y})=A\\ cov(\\mathbf{x},\\mathbf{y})\\) \\(cov(\\mathbf{x},A\\mathbf{y})=cov(\\mathbf{x},\\mathbf{y}) A^T\\) \\(var(a)= 0_{n\\times n}\\) \\(cov(\\mathbf{a},\\mathbf{y})=0_{n\\times n}\\) \\(var(\\mathbf{a}+\\mathbf{y})=var(\\mathbf{y})\\) 3.5 Multivariate normal (Gaussian) distribution 3.5.1 Definition \\(\\mathbf{y}=(Y_1,\\dots,Y_n )^T\\) has a multivariate normal distribution with mean \\(\\mathbf{\\mu}\\) (an \\(n\\times 1\\) vector) and covariance \\(\\mathbf{\\Sigma}\\) (an \\(n\\times n\\) matrix) if the joint pdf is \\[f(\\mathbf{y})=\\frac{1}{(2\\pi)^{n/2} |\\mathbf{\\Sigma}|^{1/2} } \\exp\\left(-\\frac{1}{2} (\\mathbf{y}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{y}-\\mathbf{\\mu})\\right).\\] Note that \\(\\mathbf{\\Sigma}\\) must be symmetric and positive definite. We would denote this as \\(\\mathbf{y}∼N(\\mathbf{\\mu},\\mathbf{\\Sigma)}\\). ##$ Useful facts Important fact: A linear function of a multivariate normal random vector (i.e., \\(a+A\\mathbf{y}\\)) is also multivariate normal (though it could collapse to a single random variable if \\(A\\) is a \\(1\\times n\\) vector). Application: Suppose that \\(\\mathbf{y}∼N(\\mu,\\Sigma)\\). For an \\(m\\times n\\) matrix of constants \\(A\\), \\(A\\mathbf{y}∼N(A\\mu,A\\Sigma A^T)\\). 3.6 Example Gasoline is to be stocked in a bulk tank once at the beginning of each week and then sold to individual customers. Let \\(Y_1\\) denote the proportion of the capacity of the bulk tank that is available after the tank is stocked at the beginning of the week. Because of the limited supplies, \\(Y_1\\) varies from week to week. Let \\(Y_2\\) denote the proportion of the capacity of the bulk tank that is sold during the week. Because \\(Y_1\\) and \\(Y_2\\) are both proportions, both variables are between 0 and 1. Further, the amount sold, \\(y_2\\), cannot exceed the amount available, \\(y_1\\). Suppose the joint density function for \\(Y_1\\) and \\(Y_2\\) is given by \\[f(y_1,y_2 )=3y_1;\\ 0≤y_2\\leq y_1\\leq 1.\\] 3.6.1 Problem 1 Determine \\(P(0\\leq Y_1\\leq 0.5;\\ 0.25\\leq Y_2)\\) 3.6.2 Problem 2 Determine \\(f_{Y_1 }\\) and \\(f_{Y_2 }\\) 3.6.3 Problem 3 Determine \\(E(Y_1)\\) and \\(E(Y_2)\\) 3.6.4 Problem 4 Determine \\(var(Y_1)\\) and \\(var(Y_2)\\) 3.6.5 Problem 5 Determine \\(E(Y_1 Y_2)\\) 3.6.6 Problem 6 Determine \\(cov(Y_1,Y_2)\\) 3.6.7 Problem 7 Determine the mean and variance of \\(a^T y\\), where \\(a=(1,-1)^T\\) and \\(y=(Y_1,Y_2 )^T\\). This is the expectation and variance of the difference between the amount of gas available and the amount of gas sold. "]
]
