[
["defining-a-linear-model.html", "Chapter 5 Defining a linear model 5.1 Background and terminology 5.2 Types of regression 5.3 Goals of regression 5.4 Definition of a linear model 5.5 Summarizing the components of a linear models", " Chapter 5 Defining a linear model Based on Chapter 2 of LMWR2, Chapter 2 and 3 of ALR4 5.1 Background and terminology Regression models are used to model the relationship between: one or more response variables and one or more predictor variables. The distinction between these two variables is their purpose in the regression model. Predictor variables are used to predict the value of the response variable. Response variables are also known as outcome, output, or dependent variables. Response variables are modeled as random variables. Predictor variables are also known as explanatory, regressor, input, dependent, or feature variables. Predictor variables are treated as known, fixed (non-random) variables. Note: Because the variables in our model are often interrelated, describing these variables as independent or dependent variables are vague and are best avoided. A distinction is sometimes made between regression models and classification models. In that case: Regression models attempt to predict a numerical response. Classification models attempt to predict the category level a response will have. A linear model is a regression model in which the regression coefficients (to be discussed later) enter the model linearly. A linear model is just a specific type of regression model. 5.2 Types of regression Simple regression model: a regression model with one constant predictor and one non-constant predictor. Multiple regression model: a regression model with more than one non-constant predictor. Multivariate regression model: a regression model with more than one response variable. Linear regression model: a regression model in which the regression coefficients enter the model linearly. Analysis of variance (ANOVA) model: a linear regression model with one or more categorical predictors. Analysis of covariance (ANCOVA): a linear regression model with a quantitative predictor and a categorical predictor. Generalized linear model (GLM): a type of “generalized” regression model when the responses do not come from a normal distribution. 5.3 Goals of regression The basic goals of a regression model are to: Predict future or unknown response values based on specified values of the predictors. What will the selling price of a home be? Identify relationships (associations) between predictor variables and the response. What is the general relationship between the selling price of a home and the number of bedrooms the home has? With our model, we also hope to be able to: Generalize our results from the sample to the a larger population of interest. E.g., we want to extend our results from a small set of college students to all college students. Infer causality between our predictors and the response. E.g., if we give a person a vaccine, then this causes the person’s risk of catching the disease to decrease. A note about models: There is no such thing as a “true model” for real data. The general goal of a regression analysis is to find a model that adequately describes the relationship between the response and relevant predictor variables (either in terms of prediction, association, generalization, causality, etc.) 5.4 Definition of a linear model Let \\(Y\\) denote a response variable, \\(X_1, X_2, \\ldots, X_{p-1}\\) denote \\(p-1\\) predictor variables, and \\(\\epsilon\\) denote a random variable we call error. Then a linear model for \\(Y\\) is defined by the equation \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1} + \\epsilon, \\tag{5.1} \\end{equation}\\] where \\(\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}\\) are known as regression coefficients. The model in Equation (5.1) is a statistical model because there is uncertainty in the response. Suppose we have sampled \\(n\\) observations from a population. The response values are denoted \\(Y_1, Y_2, \\ldots, Y_n\\). The value of predictor \\(j\\) for observation \\(i\\) is denoted by \\(x_{i,j}\\). Let \\(Y_1, Y_2, \\ldots, Y_n\\) denote the observed values of the response variable. . We have also measured \\(p-1\\) predictor variables \\(X_1, X_2, \\ldots, X_{p-1}\\) denote \\(p-1\\) predictor variables. The error for the \\(i\\)th observation is denoted \\(\\epsilon_i\\). The linear model for the responses is defined by the system of equations \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_{p-1} x_{i,p-1} + \\epsilon_i,\\quad i=1,2,\\ldots,n. \\tag{5.2} \\end{equation}\\] The regression coefficients are said to enter the model linearly, which is why this type of model is called a linear model. To see this more clearly, we represent the model using matrices. Let \\(\\mathbf{y} = [Y_1, Y_2, \\ldots, Y_n]^T\\) be a column vector containing the \\(n\\) responses, \\[\\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,p-1} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,p-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,p-1} \\end{bmatrix}\\] be a matrix containing the observed predictor values (and a column of 1s), and \\(\\mathbf{\\epsilon} = [\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n]^T\\) be a column vector contained the \\(n\\) errors, and \\(\\mathbf{\\beta} = [\\beta_0, \\beta_1, \\ldots, \\beta_{p-1}]^T\\) be a column vector containing the \\(p\\) regression coefficients. Then the system of equations defining the linear model in (5.2) can be written as \\[\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}.\\] Thus, a linear model can be represented as a system of linear equations using matrices. A model that cannot be represented as a system of linear equations using matrices is not a linear model. 5.5 Summarizing the components of a linear models Before we progress any further, it is very informative to consider some properties of the objects discussed in Equation (5.2), specifically, with respect to whether they are observable and whether they are model is random variables. We’ve already talked about observing the response variable and the predictor variables. So these objects are observable. However, we have no way to measure the regression coefficients or the error. These are not observable. On the other hand, we treat the response variable as a random variable. Perhaps surprisingly, we treated the predictor variables as a fixed, non-random variables. Our analyses and inference will always be conditional on the predictor variables, so we treat them as non-random. The regression coefficients are treated as fixed, non-random but unknown values. This is standard for parameters in a statistical model. The errors are also treated as random variables. In fact, since both the predictor variables and the regression coefficients are non-random, the only way for the response to be a random variable based on Equation (5.2) is for the errors to be random. We summarize this information in the table below for the objects previously discussed using the various notations introduced. Notation Description Observable Random \\(Y\\) response variable Yes Yes \\(X\\) predictor variable Yes No \\(Y_i\\) response value for the \\(i\\)th observation Yes Yes \\(X_j\\) the \\(j\\)th predictor variable Yes No \\(x_{i,j}\\) the value of the \\(j\\)the predictor for the \\(i\\)th observation Yes No \\(\\beta_j\\) the regression coefficient associated with the \\(j\\)th predictor variable No No \\(\\epsilon_i\\) the error associated with observation \\(i\\) No Yes \\(\\mathbf{y}\\) the \\(n\\times 1\\) column vector of response values Yes Yes \\(\\mathbf{X}\\) the \\(n\\times p\\) matrix of predictor values Yes No \\(\\mathbf{\\beta}\\) the \\(p\\times 1\\) column vector of regression coefficients No No \\(\\mathbf{\\epsilon}\\) the \\(n\\times 1\\) column vector of errors No Yes "]
]
