[["interpreting-a-fitted-linear-model.html", "Chapter 4 Interpreting a fitted linear model 4.1 Standard mathemtical interpretation 4.2 Going deeper", " Chapter 4 Interpreting a fitted linear model Interpreting a fitted model is a critical part of a regression analysis and aids us in determining the roles and impact each variable plays in describing the behavior of the response variable. 4.1 Standard mathemtical interpretation The standard approach to interpreting the coefficients of a fitted linear model is to consider the expected change in the response in relation to the regressors in the model. Consider the typical multiple linear regression model of the response \\[ \\begin{equation} Y=\\beta_0+\\beta_1 X_1 +\\ldots + \\beta_{p-1}X_{p-1}+\\epsilon.\\tag{4.1} \\end{equation} \\] As discussed in Chapter 3, we treat the values of our regressor variables as being fixed, known values. The error term is treated as a random variable, and consequently, the response variable is also a random variable. Additionally, we assume that the errors all have mean 0, conditional on the values of the regressor variables. More formally, we write this assumption as \\[\\begin{equation} E(\\epsilon \\mid X_1, X_2, \\ldots, X_{p-1})=0.\\tag{4.2} \\end{equation}\\] Recall that we use the notation \\(\\mathbb{X} = \\{X_1,\\ldots,X_{p-1}\\}\\) to denote the set of all regressors, which will help us simplify the derivations below. Thus, the assumption in Equation (4.2) can be expressed as \\(E(\\epsilon \\mid \\mathbb{X})=0\\). Using the assumption in Equation (4.2) and applying it to the model in Equation (4.1), we see that \\[ \\begin{align} &amp; E(Y\\mid X_1, X_2, \\ldots, X_{p-1}) \\\\ &amp;= E(Y \\mid \\mathbb{X}) \\\\ &amp;= E(\\beta_0+\\beta_1 X_1 +\\ldots + \\beta_{p-1}X_{p-1}+\\epsilon \\mid \\mathbb{X}) \\\\ &amp;= E(\\beta_0+\\beta_1 X_1 +\\ldots + \\beta_{p-1}X_{p-1}\\mid \\mathbb{X}) + E(\\epsilon \\mid \\mathbb{X}) \\\\ &amp;=\\beta_0+\\beta_1 X_1 +\\ldots + \\beta_{p-1}X_{p-1} \\end{align} \\] since all terms in the first summand of line 4 are fixed, non-random values conditional on \\(\\mathbb{X}\\) and the second summand is 0 by assumption. If you are rusty with properties of random variables, consider reviewing the material in Appendix B. Using the facts above we discuss interpretation of simple linear regression models, multiple linear regression models with basic numeric predictors as regressors, and interpretation for parallel and separate lines regression model. 4.1.1 Coefficient interpretation in simple linear regression Suppose we have a simple linear regression model, so that \\[\\begin{equation} E(Y\\mid X)=\\beta_0 + \\beta_1 X. \\tag{4.3} \\end{equation}\\] The interpretations of the coefficients are: \\(\\beta_0\\) is the expected response when the regressor is 0, i.e., \\(\\beta_0=E(Y\\mid X=0)\\). \\(\\beta_1\\) is the expected change in the response when the regressor increases 1 unit, i.e., \\(\\beta_1=E(Y\\mid X=x^*+1)-E(Y\\mid X=x^*)\\), where \\(x^*\\) is a fixed real number. Regarding the interpretation of \\(\\beta_0\\), from the regression model in Equation (4.3), notice that \\[ \\begin{align} E(Y\\mid X = 0) &amp;= \\beta_0 + \\beta_1 \\cdot 0 \\\\ &amp;= \\beta_0. \\end{align} \\] This is why \\(\\beta_0\\) is the expected value of the response variable when the regressor is zero. Similarly, for \\(\\beta_1\\), we notice that \\[ \\begin{align} E(Y\\mid X=x^*+1)-E(Y\\mid X=x^*) &amp;= [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] \\\\ &amp;= \\beta_1. \\end{align} \\] Thus, \\(\\beta_1\\) literally equals the change in the expected response when the regressor increases by 1 unit. 4.1.2 Coefficient interpretation for first-order multiple linear regression models Suppose we have a multiple linear regression model, so that \\[\\begin{equation} E(Y\\mid X_1,\\ldots,X_{p-1})=\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_{p-1} X_{p-1}.\\tag{4.4} \\end{equation}\\] Extending the definition of \\(\\mathbb{X}\\), we denote the set of regressors without \\(X_j\\) as \\(\\mathbb{X}_{-j} = \\mathbb{X}\\setminus\\{X_j\\}\\). The interpretations of the coefficients more the model in Equation (4.4) are: \\(\\beta_0\\) is the expected response when all regressors are 0, i.e., \\(\\beta_0=E(Y\\mid X_1=0,\\ldots,X_{p-1}=0)\\). \\(\\beta_j\\) is the expected change in the response when regressor \\(j\\) increases 1 unit and the other regressors stay the same, i.e., \\(\\beta_j=E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*+1)-E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j}^*)\\) where \\(\\mathbf{x}_{-j}^*=[x^*_1,\\ldots,x_{j-1}^*,x_{j+1}^*,\\ldots,x_{p-1}^*]\\in \\mathbb{R}^{p-2}\\) is a vector with \\(p-2\\) fixed values (the number of regressors excluding \\(X_j\\)) and \\(x_j^*\\) is a fixed real number. Regarding the interpretation of \\(\\beta_0\\), from the regression model in Equation (4.4), notice that \\[ \\begin{align} E(Y\\mid X=x^*+1)-E(Y\\mid X=x^*) &amp;= [\\beta_0 + \\beta_1 (x^* + 1)] - [\\beta_0 + \\beta_1 x^*] \\\\ &amp;= \\beta_1. \\end{align} \\] It is quite common for the mathematical interpretation of the intercept to be nonsensical because we are extrapolating outside the range of the observed data. e.g., we might predict the height of a newborn child (age = 0) to be negative, which isnt possible in reality. For \\(\\beta_j\\), we notice that \\[ \\begin{align} &amp; E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j}+1)-E(Y\\mid \\mathbb{X}_{-j} = \\mathbf{x}^*_{-j}, X_{j+1} = x_{j})\\\\ &amp;= \\biggl[\\beta_0 + \\sum_{k=1}^{j-1}\\beta_kx^*_k + \\beta_j(x^*_j+1) + \\sum_{k=j+1}^{p-1}\\beta_kx^*_k\\biggl] \\\\ &amp;\\quad -\\biggl[\\beta_0 + \\sum_{k=1}^{j-1}\\beta_kx^*_k + \\beta_jx^*_j + \\sum_{k=j+1}^{p-1}\\beta_kx^*_k\\biggl]\\\\ &amp;= \\beta_j. \\end{align} \\] A notable problem with the mathematical interpretation of multiple regression models is that a single predictor can be used more than once in the model. E.g., in the regression model \\(E(Y\\mid X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\), \\(X\\) is used in both the second and third terms. So it is not possible to increase \\(X\\) while keeping \\(X^2\\) fixed. The mathematical interpretation given in this section is applicable to first-order linear regression models, where a first-order linear regression model is a multiple linear regression model in which no regressor is a function of any other regressor. Another notable problem is that the regressors used in our model our often observational in nature, meaning that we do not control them. Thus, it doesnt make sense to say we increase variable \\(X\\) by 1 unit. An alternative approach, suggested in Faraway (2014), is to consider the expected response difference between observations that are identical with respect to all attributes except the variable under consideration, which varies by only a single unit. While mathematically, the result is the same, the interpretation is more philosophically palatable. 4.1.3 Penguins examples To apply the examples given above, we interpret the models fit to the penguins data introduced in Section 3.4 and continued in Section 3.6. From Section 3.4, the fitted simple linear regression model is \\[ \\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g})=26.9+0.004 \\,\\mathtt{body\\_mass\\_g}. \\] The expected bill length of a penguin with a body mass of 0 grams is 26.9 mm. We discussed the absurdity of this interpretation in Section 3.4. If two penguins were identical except that one penguin was 1 gram heavier, then we would expect the heavier penguin to have a bill length 0.004 mm longer. From Section 3.6, the fitted multiple linear regression model is \\[ \\hat{E}(\\mathtt{bill\\_length\\_mm}\\mid \\mathtt{body\\_mass\\_g}, \\mathtt{flipper\\_length\\_mm})=-3.44+0.0007 \\,\\mathtt{body\\_mass\\_g}+0.22\\,\\mathtt{flipper\\_length\\_mm}. \\] We discuss basic interpretation of multiple linear regression model in Chapter @ref(interpreting-a-fitted linear-model). Some interpretations of the coefficients are: We expect a penguin with a body mass of 0 grams and a flipper length of 0 mm to to have a bill length of -3.44 mm. For two penguins that are identical except that one penguin has a body mass 1 gram larger, we expect the heavier penguin to have a bill length 0.0007 mm longer than the other penguin. For two penguins that are identical except that one penguin has a flipper length 1 mm longer, we expect the penguin with longer flippers to have a bill length 0.22 mm longer. 4.2 Going deeper 4.2.1 Orthogonality Let \\[\\mathbf{X}_[j]=[x_{1,j},\\ldots,x_{n,j}]\\] denote the \\(n\\times 1\\) column vector of observed values for regressor \\(X_j\\). (We cant use the notation \\(\\mathbf{x}_j\\) because that is the \\(p\\times 1\\) vector of regressor values for the \\(j\\)th observation). Regressors, \\(\\mathbf{X}_{[j]}\\) and \\(\\mathbf{X}_{[k]}\\) are orthogonal if \\(\\mathbf{X}_{[j]}^T \\mathbf{X}_{[k]}=0\\). Let \\(\\boldsymbol{1}_{n\\times1}\\) denote an \\(n\\times 1\\) column vector of 1s. The definition of orthogonal vectors above implies that \\(\\mathbf{X}_{[j]}\\) is orthogonal to \\(\\boldsymbol{1}_{n\\times1}\\) if \\[ \\mathbf{X}_{[j]}^T \\boldsymbol{1}_{n\\times1} = \\sum_{i=1}^n x_{i,j} = 0,\\] i.e., if the values in \\(\\mathbf{X}_{[j]}\\) sum to zero. Let \\(\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{i,j}\\) denote the sample mean of \\(\\mathbf{X}_{[j]}\\) and \\(\\bar{\\mathbf{x}}_j = \\bar{x}_j \\boldsymbol{1}_{n\\times 1}\\) denote the column vector that repeats \\(\\bar{x}_j\\) \\(n\\) times. Centering \\(\\mathbf{X}_{[j]}\\) involves subtracting the sample mean of \\(\\mathbf{X}_{[j]}\\) from \\(\\mathbf{X}_{[j]}\\), i.e., \\(\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j\\). Regressors \\(\\mathbf{X}_{[j]}\\) and \\(\\mathbf{X}_{[k]}\\) are uncorrelated if they are orthogonal after being centered, i.e., if \\[ (\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j)^T (\\mathbf{X}_{[k]} - \\bar{\\mathbf{x}}_k)=0. \\] Note that the sample covariance between vectors \\(\\mathbf{X}_{[j]}\\) and \\(\\mathbf{X}_{[k]}\\) is \\[ \\begin{align*} \\widehat{\\mathrm{cov}}(\\mathbf{X}_{[j]}, \\mathbf{X}_{[k]}) &amp;= \\frac{1}{n-1}\\sum_{i=1}^n (x_{i,j} - \\bar{x}_j)(x_{i,k} - \\bar{x}_k) \\\\ &amp;= \\frac{1}{n-1}(\\mathbf{X}_{[j]} - \\bar{\\mathbf{x}}_j)^T (\\mathbf{X}_{[k]} - \\bar{\\mathbf{x}}_k). \\end{align*} \\] Thus, two centered regressors are orthogonal if their covariance is zero. It is a desirable to have orthogonal regressors in your fitted model because they simplify estimating the relationship between the regressors and the response. Specifically: If a regressor is orthogonal to all other regressors (and the column of 1s) in a model, adding or removing the orthogonal regressor from your model will not impact the estimated regression coefficients of the other regressors. Since most linear regression models include an intercept, we should assess whether our regressors are orthogonal to other regressors and the column of 1s. We consider a simple example with \\(n=5\\) observations to demonstrate how orthogonality of regressors impacts the estimated regression coefficients. In the code below: y is a vector of response values. ones is the column vector of 1s. X1 is a column vector of regressor values. X2 is a column vector of regressor values chosen to be orthogonal to x1 but not to ones. X3 is a column vector of regressor values orthogonal to both x1 and ones. X4 is a column vector of regressor values orthogonal to ones, x1, and x3, but not x2. X5 is a column vector of regressor value sorthogonal to ones and x1, but not the other regressor vectors. In the code below, we define vectors y, X1, and X2. y &lt;- c(1, 4, 6, 8, 9) # create an arbitrary response vector X1 &lt;- c(7, 5, 5, 7, 7) # create regressor 1 X2 &lt;- c(-1, 2, -3, 1, 5/7) # create regressor 2 to be orthogonal to x1 Note that the crossprod function computes the crossproduct of two vectors or matrices, so that crossprod(A, B) computes \\(\\mathbf{A}^T B\\), where the vectors or matrices must have the correct dimension for the multiplication to be performed. The regressor vectors X1 and X2 are orthogonal since their crossproduct \\(\\mathbf{X}_{[1]}^T \\mathbf{X}_{[2]}\\) (in R, crossprod(X1, X2)) equals zero, as shown in the code below. # crossproduct is zero, so X1 and X2 are orthogonal crossprod(X1, X2) ## [,1] ## [1,] 0 In the code below, we regress y on x1 without an intercept (lmod1). The estimated coefficient for X1 is \\(\\hat{\\beta}_1=0.893\\). Next, we then regress y on X1 and X2 without an intercept (lmod2). The estimated coefficients for X1 and X2 are \\(\\hat{\\beta}_1=0.893\\) and \\(\\hat{\\beta}_2=0.221\\), respectively. Because X1 and X2 are orthogonal (and because there are no other regressors to consider in the model), the estimated coefficient for X1 stays the same in both models. # y regressed on X1 without an intercept lmod1 &lt;- lm(y ~ x1 - 1) coef(lmod1) ## x1 ## 0.893401 # y regressed on X1 and X2 without an intercept lmod2 &lt;- lm(y ~ x1 + x2 - 1) coef(lmod2) ## x1 x2 ## 0.8934010 0.2210526 The previous models (lmod1 and lmod2) neglect an important characteristic of a typical linear model: we usually include an intercept coefficient (a columns of 1s as a regressor) in our model. If the regressors are not orthogonal to the column of 1s in our \\(\\mathbf{X}\\) matrix, then the coefficients for the other regressors in the model will change when the regressors are added or removed from the model because they are not orthogonal to the column of 1s. However, neither X1 nor X2 is orthogonal with the column of ones. We define the vector ones below, which is a column of 1s, and compute the crossproduct between ones and the two regressors. Since the crossproducts are not zero, X1 and X2 are not orthogonal to the column of ones. ones &lt;- rep(1, 5) # column of 1s crossprod(ones, X1) # not zero, so not orthogonal ## [,1] ## [1,] 31 crossprod(ones, X2) # not zero, so not orthogonal ## [,1] ## [1,] -0.2857143 We create lmod3 by adding adding a column of ones to lmod2 (i.e., if we include the intercept in the model). The the coefficients for both X1 and X2 change when going from lmod2 to lmod3 because these regressors are not orthogonal to the column of 1s. Comparing the coefficients lmod2 above and lmod3, \\(\\hat{\\beta}_1\\) changes from \\(0.893\\) to \\(0.397\\) and \\(\\hat{\\beta}_2\\) changes from \\(0.221\\) to \\(0.279\\). coef(lmod2) # coefficients for lmod2 ## x1 x2 ## 0.8934010 0.2210526 # y regressed on X1 and X2 with an intercept lmod3 &lt;- lm(y ~ x1 + x2) coef(lmod3) # coefficients for lmod3 ## (Intercept) x1 x2 ## 3.1547101 0.3969746 0.2791657 For orthogonality of our regressors to be most impactful, the models regressors should be orthogonal to each other and the column of 1s. In that context, adding or removing any of the regressors doesnt impact the estimated coefficients of the other regressors. In the code below, we define centered regressors x3 and x4 to be uncorrelated, i.e., X3 and X4 have sample mean zero and are orthogonal to each other. X3 &lt;- c(0, -1, 1, 0, 0) # sample mean is zero X4 &lt;- c(0, 0, 0, 1, -1) # sample mean is zero cov(X3, X4) # 0, so X3 and X4 are uncorrelated and orthogonal ## [1] 0 If we fit linear regression models with any combination of ones, X3, or X4 as regressors, the associated regression coefficients will not change. To demonstrate this, we consider all possible combinations of the three variables in the models below. We do not run the code to save space, but we summarize the results below. coef(lm(y ~ 1)) # only column of 1s coef(lm(y ~ x3 - 1)) # only x3 coef(lm(y ~ x4 - 1)) # only x4 coef(lm(y ~ x3)) # 1s and x3 coef(lm(y ~ x4)) # 1s and x4 coef(lm(y ~ x3 + x4 - 1)) # x3 and x4 coef(lm(y ~ x3 + x4)) # 1s, x3, and x4 We simply note that in each of the previous models, because all of the regressors (and the column of 1s) are orthogonal to each other, adding or removing any regressor doesnt impact the estimated coefficients for the other regressors in the model. Thus, the estimated coefficients were \\(\\hat{\\beta}_{0}=5.6\\), \\(\\hat{\\beta}_{3}=1.0\\), \\(\\hat{\\beta}_{4}=-0.5\\) when the relevant regressor was included in the model. The easiest way to determine which vectors are orthogonal to each other and the intercept is to compute the crossproduct of the \\(\\mathbf{X}\\) matrix for the largest set of regressors you are considering. Consider the matrix of crossproducts for the columns of 1s, x1, x2, x3, and x4. crossprod(model.matrix(~ X1 + X2 + X3 + X4)) ## (Intercept) X1 X2 X3 X4 ## (Intercept) 5.0000000 31 -0.2857143 0 0.0000000 ## X1 31.0000000 197 0.0000000 0 0.0000000 ## X2 -0.2857143 0 15.5102041 -5 0.2857143 ## X3 0.0000000 0 -5.0000000 2 0.0000000 ## X4 0.0000000 0 0.2857143 0 2.0000000 Consider the sequence of models below. coef(lm(y ~ 1)) ## (Intercept) ## 5.6 The model with only an intercept has an estimated coefficient of \\(\\hat{\\beta}_{int}=5.6\\). If we add the X1 to the model with an intercept, then both coefficients change because they are not orthogonal to each other. lmod4 &lt;- lm(y ~ x1) # model with 1s and x1 coef(lmod4) ## (Intercept) x1 ## 2.5 0.5 If we add X2 to lmod4, we might think that only \\(\\hat{\\beta}_{0}\\) will change because X1 and X2 are orthogonal to each other. However, because X2 is not orthogonal to all of the other regressors in the model (X1 and the column of 1s), both \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_1\\) will change. The easiest way to realize this is to look at lmod2 above with only x1 and x2. When we add the column of 1s to lmod2, both \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) will change because neither regressor is orthogonal to the column of 1s needed to include the intercept term. coef(lm(y ~ x1 + x2)) ## (Intercept) x1 x2 ## 3.1547101 0.3969746 0.2791657 However, note that X3 is orthogonal to the column of 1s and X1. Thus, if we add X3 to lmod4, which includes both a column of 1s and X1, X3 will not change the estimated coefficients for the intercept or X1. coef(lm(y ~ x1 + x3)) ## (Intercept) x1 x3 ## 2.5 0.5 1.0 Additionally, since X4 is orthogonal to the column of 1s, x1, and x3, adding X4 to the previous model will not change the estimated coefficients for any of the other variables already in the model. coef(lm(y ~ x1 + x3 + x4)) ## (Intercept) x1 x3 x4 ## 2.5 0.5 1.0 -0.5 Lastly, if we can partition our \\(\\mathbf{X}\\) matrix such that \\(\\mathbf{X}^T \\mathbf{X}\\) is a block diagonal matrix, then none of the blocks of variables will affect the estimated coefficients of the other variables. Define a new regressor X5 below. X5 is orthogonal to the column of 1s and X1, but not X4. X5 &lt;- c(1, 0, 0, -1, 0) # orthogonal to ones, x1, not x4 # note block of 0s crossprod(cbind(ones, X1, X4, X5)) ## ones X1 X4 X5 ## ones 5 31 0 0 ## X1 31 197 0 0 ## X4 0 0 2 -1 ## X5 0 0 -1 2 Note the block of zeros in the lower left and uper right corners of the crossproduct matrix above. The block containing ones and X1 is orthogonal to the block containing X4 and X5. This means that if we fit the model with only the column of 1s and X1, the model only with X4 and X5, and then fit the model with the column of 1s, x1, x4, and x5, then the coefficients \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) are not impacted when X4 and X5 are added to the model. Similarly, \\(\\hat{\\beta}_{4}\\) and \\(\\hat{\\beta}_{5}\\) are not impacted when the column of 1s and X1 are added to the model with X4 and X5. See the output below. lm(y ~ x1) # model with 1s and x1 ## ## Call: ## lm(formula = y ~ x1) ## ## Coefficients: ## (Intercept) x1 ## 2.5 0.5 lm(y ~ x4 + x5 - 1) # model with x4 and x5 only ## ## Call: ## lm(formula = y ~ x4 + x5 - 1) ## ## Coefficients: ## x4 x5 ## -3 -5 lm(y ~ x1 + x4 + x5) # model with 1s, x1, x4, x5 ## ## Call: ## lm(formula = y ~ x1 + x4 + x5) ## ## Coefficients: ## (Intercept) x1 x4 x5 ## 2.5 0.5 -3.0 -5.0 &gt; References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
