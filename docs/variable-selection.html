<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 Variable Selection | Joshua French</title>
  <meta name="description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 Variable Selection | Joshua French" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Variable Selection | Joshua French" />
  
  <meta name="twitter:description" content="A collection of R notebooks that progressively introduction how to fit and use linear models." />
  

<meta name="author" content="Chapter 14 Variable Selection | Joshua French" />


<meta name="date" content="2022-06-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="assumptions-stated-and-prioritized.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis with Linear Regression</a></li>

<li class="divider"></li>
<li><a href="index.html#preliminaries">Preliminaries<span></span></a></li>
<li class="chapter" data-level="1" data-path="r-foundations.html"><a href="r-foundations.html"><i class="fa fa-check"></i><b>1</b> R Foundations<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-foundations.html"><a href="r-foundations.html#what-is-r"><i class="fa fa-check"></i><b>1.1</b> What is R?<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-foundations.html"><a href="r-foundations.html#where-to-get-r-and-r-studio-desktop"><i class="fa fa-check"></i><b>1.2</b> Where to get R (and R Studio Desktop)<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-foundations.html"><a href="r-foundations.html#r-studio-layout"><i class="fa fa-check"></i><b>1.3</b> R Studio Layout<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-foundations.html"><a href="r-foundations.html#running-code-scripts-and-comments"><i class="fa fa-check"></i><b>1.4</b> Running code, scripts, and comments<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-foundations.html"><a href="r-foundations.html#example"><i class="fa fa-check"></i><b>1.4.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-foundations.html"><a href="r-foundations.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages<span></span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="r-foundations.html"><a href="r-foundations.html#example-1"><i class="fa fa-check"></i><b>1.5.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="r-foundations.html"><a href="r-foundations.html#getting-help"><i class="fa fa-check"></i><b>1.6</b> Getting help<span></span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="r-foundations.html"><a href="r-foundations.html#example-2"><i class="fa fa-check"></i><b>1.6.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="r-foundations.html"><a href="r-foundations.html#data-types-and-structures"><i class="fa fa-check"></i><b>1.7</b> Data types and structures<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-data-types"><i class="fa fa-check"></i><b>1.7.1</b> Basic data types<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="r-foundations.html"><a href="r-foundations.html#other-important-object-types"><i class="fa fa-check"></i><b>1.7.2</b> Other important object types<span></span></a></li>
<li class="chapter" data-level="1.7.3" data-path="r-foundations.html"><a href="r-foundations.html#data-structures"><i class="fa fa-check"></i><b>1.7.3</b> Data structures<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-foundations.html"><a href="r-foundations.html#assignment"><i class="fa fa-check"></i><b>1.8</b> Assignment<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-foundations.html"><a href="r-foundations.html#example-3"><i class="fa fa-check"></i><b>1.8.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-foundations.html"><a href="r-foundations.html#vectors"><i class="fa fa-check"></i><b>1.9</b> Vectors<span></span></a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="r-foundations.html"><a href="r-foundations.html#creation"><i class="fa fa-check"></i><b>1.9.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.9.2" data-path="r-foundations.html"><a href="r-foundations.html#creating-patterned-vectors"><i class="fa fa-check"></i><b>1.9.2</b> Creating patterned vectors<span></span></a></li>
<li class="chapter" data-level="1.9.3" data-path="r-foundations.html"><a href="r-foundations.html#example-4"><i class="fa fa-check"></i><b>1.9.3</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.4" data-path="r-foundations.html"><a href="r-foundations.html#example-5"><i class="fa fa-check"></i><b>1.9.4</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.5" data-path="r-foundations.html"><a href="r-foundations.html#categorical-vectors"><i class="fa fa-check"></i><b>1.9.5</b> Categorical vectors<span></span></a></li>
<li class="chapter" data-level="1.9.6" data-path="r-foundations.html"><a href="r-foundations.html#example-6"><i class="fa fa-check"></i><b>1.9.6</b> Example<span></span></a></li>
<li class="chapter" data-level="1.9.7" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-vector"><i class="fa fa-check"></i><b>1.9.7</b> Extracting parts of a vector<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="r-foundations.html"><a href="r-foundations.html#helpful-functions"><i class="fa fa-check"></i><b>1.10</b> Helpful functions<span></span></a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="r-foundations.html"><a href="r-foundations.html#general-functions"><i class="fa fa-check"></i><b>1.10.1</b> General functions<span></span></a></li>
<li class="chapter" data-level="1.10.2" data-path="r-foundations.html"><a href="r-foundations.html#example-7"><i class="fa fa-check"></i><b>1.10.2</b> Example<span></span></a></li>
<li class="chapter" data-level="1.10.3" data-path="r-foundations.html"><a href="r-foundations.html#functions-related-to-statistical-distributions"><i class="fa fa-check"></i><b>1.10.3</b> Functions related to statistical distributions<span></span></a></li>
<li class="chapter" data-level="1.10.4" data-path="r-foundations.html"><a href="r-foundations.html#example-8"><i class="fa fa-check"></i><b>1.10.4</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="r-foundations.html"><a href="r-foundations.html#data-frames"><i class="fa fa-check"></i><b>1.11</b> Data Frames<span></span></a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="r-foundations.html"><a href="r-foundations.html#creation-1"><i class="fa fa-check"></i><b>1.11.1</b> Creation<span></span></a></li>
<li class="chapter" data-level="1.11.2" data-path="r-foundations.html"><a href="r-foundations.html#extracting-parts-of-a-data-frame"><i class="fa fa-check"></i><b>1.11.2</b> Extracting parts of a data frame<span></span></a></li>
<li class="chapter" data-level="1.11.3" data-path="r-foundations.html"><a href="r-foundations.html#example-9"><i class="fa fa-check"></i><b>1.11.3</b> Example<span></span></a></li>
<li class="chapter" data-level="1.11.4" data-path="r-foundations.html"><a href="r-foundations.html#importing-data"><i class="fa fa-check"></i><b>1.11.4</b> Importing Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="r-foundations.html"><a href="r-foundations.html#logical-statements"><i class="fa fa-check"></i><b>1.12</b> Logical statements<span></span></a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="r-foundations.html"><a href="r-foundations.html#basic-comparisons"><i class="fa fa-check"></i><b>1.12.1</b> Basic comparisons<span></span></a></li>
<li class="chapter" data-level="1.12.2" data-path="r-foundations.html"><a href="r-foundations.html#example-10"><i class="fa fa-check"></i><b>1.12.2</b> Example<span></span></a></li>
<li class="chapter" data-level="1.12.3" data-path="r-foundations.html"><a href="r-foundations.html#and-and-or-statements"><i class="fa fa-check"></i><b>1.12.3</b> And and Or statements<span></span></a></li>
<li class="chapter" data-level="1.12.4" data-path="r-foundations.html"><a href="r-foundations.html#example-11"><i class="fa fa-check"></i><b>1.12.4</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.13" data-path="r-foundations.html"><a href="r-foundations.html#subsetting-with-logical-statements"><i class="fa fa-check"></i><b>1.13</b> Subsetting with logical statements<span></span></a>
<ul>
<li class="chapter" data-level="1.13.1" data-path="r-foundations.html"><a href="r-foundations.html#example-12"><i class="fa fa-check"></i><b>1.13.1</b> Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.14" data-path="r-foundations.html"><a href="r-foundations.html#ecosystem-debate"><i class="fa fa-check"></i><b>1.14</b> Ecosystem debate<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>2</b> Data exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-exploration.html"><a href="data-exploration.html#data-analysis-process"><i class="fa fa-check"></i><b>2.1</b> Data analysis process<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-exploration.html"><a href="data-exploration.html#problem-formulation"><i class="fa fa-check"></i><b>2.1.1</b> Problem Formulation<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="data-exploration.html"><a href="data-exploration.html#data-collection"><i class="fa fa-check"></i><b>2.1.2</b> Data collection<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-exploration.html"><a href="data-exploration.html#data-exploration-1"><i class="fa fa-check"></i><b>2.2</b> Data exploration<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-exploration.html"><a href="data-exploration.html#numerical-summaries-of-data"><i class="fa fa-check"></i><b>2.2.1</b> Numerical summaries of data<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="data-exploration.html"><a href="data-exploration.html#visual-summaries-of-data"><i class="fa fa-check"></i><b>2.2.2</b> Visual summaries of data<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="data-exploration.html"><a href="data-exploration.html#what-to-look-for"><i class="fa fa-check"></i><b>2.2.3</b> What to look for<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-exploration.html"><a href="data-exploration.html#kidney-example"><i class="fa fa-check"></i><b>2.3</b> Kidney Example<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-exploration.html"><a href="data-exploration.html#numerically-summarizing-the-data"><i class="fa fa-check"></i><b>2.3.1</b> Numerically summarizing the data<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="data-exploration.html"><a href="data-exploration.html#cleaning-the-data"><i class="fa fa-check"></i><b>2.3.2</b> Cleaning the data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-exploration.html"><a href="data-exploration.html#visualizing-data-with-base-graphics"><i class="fa fa-check"></i><b>2.4</b> Visualizing data with <strong>base</strong> graphics<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-exploration.html"><a href="data-exploration.html#histograms"><i class="fa fa-check"></i><b>2.4.1</b> Histograms<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-exploration.html"><a href="data-exploration.html#density-plots"><i class="fa fa-check"></i><b>2.4.2</b> Density plots<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-exploration.html"><a href="data-exploration.html#index-plots"><i class="fa fa-check"></i><b>2.4.3</b> Index plots<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-exploration.html"><a href="data-exploration.html#bivariate-scatter-plots"><i class="fa fa-check"></i><b>2.4.4</b> Bivariate scatter plots<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-exploration.html"><a href="data-exploration.html#bivariate-boxplots"><i class="fa fa-check"></i><b>2.4.5</b> Bivariate boxplots<span></span></a></li>
<li class="chapter" data-level="2.4.6" data-path="data-exploration.html"><a href="data-exploration.html#multiple-plots-in-one-figure"><i class="fa fa-check"></i><b>2.4.6</b> Multiple plots in one figure<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data-exploration.html"><a href="data-exploration.html#visualizing-data-with-ggplot2"><i class="fa fa-check"></i><b>2.5</b> Visualizing data with <strong>ggplot2</strong><span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-histogram"><i class="fa fa-check"></i><b>2.5.1</b> A <strong>ggplot2</strong> histogram<span></span></a></li>
<li class="chapter" data-level="2.5.2" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-density-plot"><i class="fa fa-check"></i><b>2.5.2</b> A <strong>ggplot2</strong> density plot<span></span></a></li>
<li class="chapter" data-level="2.5.3" data-path="data-exploration.html"><a href="data-exploration.html#a-ggplot2-scatter-plot"><i class="fa fa-check"></i><b>2.5.3</b> A <strong>ggplot2</strong> scatter plot<span></span></a></li>
<li class="chapter" data-level="2.5.4" data-path="data-exploration.html"><a href="data-exploration.html#scaling-ggplot2-plots"><i class="fa fa-check"></i><b>2.5.4</b> Scaling <strong>ggplot2</strong> plots<span></span></a></li>
<li class="chapter" data-level="2.5.5" data-path="data-exploration.html"><a href="data-exploration.html#facetting-in-ggplot2"><i class="fa fa-check"></i><b>2.5.5</b> Facetting in <code>ggplot2</code><span></span></a></li>
<li class="chapter" data-level="2.5.6" data-path="data-exploration.html"><a href="data-exploration.html#summary-of-ggplot2"><i class="fa fa-check"></i><b>2.5.6</b> Summary of <strong>ggplot2</strong><span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data-exploration.html"><a href="data-exploration.html#summary-of-data-exploration"><i class="fa fa-check"></i><b>2.6</b> Summary of data exploration<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html"><i class="fa fa-check"></i><b>3</b> Review of probability, random variables, and random vectors<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#probability-basics"><i class="fa fa-check"></i><b>3.1</b> Probability Basics<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#random-variables"><i class="fa fa-check"></i><b>3.2</b> Random Variables<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#discrete-random-variables"><i class="fa fa-check"></i><b>3.2.1</b> Discrete random variables<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.2.2</b> Continuous random variables<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts-for-transformation-of-random-variables"><i class="fa fa-check"></i><b>3.2.3</b> Useful facts for transformation of random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#multivariate-distributions"><i class="fa fa-check"></i><b>3.3</b> Multivariate distributions<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#basic-properties"><i class="fa fa-check"></i><b>3.3.1</b> Basic properties<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#marginal-distributions"><i class="fa fa-check"></i><b>3.3.2</b> Marginal distributions<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#independence-of-random-variables"><i class="fa fa-check"></i><b>3.3.3</b> Independence of random variables<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#conditional-distributions"><i class="fa fa-check"></i><b>3.3.4</b> Conditional distributions<span></span></a></li>
<li class="chapter" data-level="3.3.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#covariance"><i class="fa fa-check"></i><b>3.3.5</b> Covariance<span></span></a></li>
<li class="chapter" data-level="3.3.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts-for-transformations-of-multiple-random-variables"><i class="fa fa-check"></i><b>3.3.6</b> Useful facts for transformations of multiple random variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#random-vectors"><i class="fa fa-check"></i><b>3.4</b> Random vectors<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#definition"><i class="fa fa-check"></i><b>3.4.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#mean-variance-and-covariance"><i class="fa fa-check"></i><b>3.4.2</b> Mean, variance, and covariance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#properties-of-transformations-of-random-vectors"><i class="fa fa-check"></i><b>3.5</b> Properties of transformations of random vectors<span></span></a></li>
<li class="chapter" data-level="3.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#multivariate-normal-gaussian-distribution"><i class="fa fa-check"></i><b>3.6</b> Multivariate normal (Gaussian) distribution<span></span></a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#definition-1"><i class="fa fa-check"></i><b>3.6.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="3.6.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#useful-facts"><i class="fa fa-check"></i><b>3.6.2</b> Useful facts<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#example-1-1"><i class="fa fa-check"></i><b>3.7</b> Example 1<span></span></a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#bernoulli-distribution"><i class="fa fa-check"></i><b>3.7.1</b> Bernoulli distribution<span></span></a></li>
<li class="chapter" data-level="3.7.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#binomial-distribution"><i class="fa fa-check"></i><b>3.7.2</b> Binomial distribution<span></span></a></li>
<li class="chapter" data-level="3.7.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#poisson-distribution"><i class="fa fa-check"></i><b>3.7.3</b> Poisson Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#example-2-1"><i class="fa fa-check"></i><b>3.8</b> Example 2<span></span></a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-1"><i class="fa fa-check"></i><b>3.8.1</b> Problem 1<span></span></a></li>
<li class="chapter" data-level="3.8.2" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-2"><i class="fa fa-check"></i><b>3.8.2</b> Problem 2<span></span></a></li>
<li class="chapter" data-level="3.8.3" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-3"><i class="fa fa-check"></i><b>3.8.3</b> Problem 3<span></span></a></li>
<li class="chapter" data-level="3.8.4" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-4"><i class="fa fa-check"></i><b>3.8.4</b> Problem 4<span></span></a></li>
<li class="chapter" data-level="3.8.5" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-5"><i class="fa fa-check"></i><b>3.8.5</b> Problem 5<span></span></a></li>
<li class="chapter" data-level="3.8.6" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-6"><i class="fa fa-check"></i><b>3.8.6</b> Problem 6<span></span></a></li>
<li class="chapter" data-level="3.8.7" data-path="review-of-probability-random-variables-and-random-vectors.html"><a href="review-of-probability-random-variables-and-random-vectors.html#problem-7"><i class="fa fa-check"></i><b>3.8.7</b> Problem 7<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html"><i class="fa fa-check"></i><b>4</b> Useful matrix facts<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#notation"><i class="fa fa-check"></i><b>4.1</b> Notation<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#basic-mathematical-properties"><i class="fa fa-check"></i><b>4.2</b> Basic mathematical properties<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#addition-and-subtraction"><i class="fa fa-check"></i><b>4.2.1</b> Addition and subtraction<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#scalar-multiplication"><i class="fa fa-check"></i><b>4.2.2</b> Scalar multiplication<span></span></a></li>
<li class="chapter" data-level="4.2.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-multiplication"><i class="fa fa-check"></i><b>4.2.3</b> Matrix multiplication<span></span></a></li>
<li class="chapter" data-level="4.2.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#associative-property"><i class="fa fa-check"></i><b>4.2.4</b> Associative property<span></span></a></li>
<li class="chapter" data-level="4.2.5" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#distributive-property"><i class="fa fa-check"></i><b>4.2.5</b> Distributive property<span></span></a></li>
<li class="chapter" data-level="4.2.6" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#no-commutative-property"><i class="fa fa-check"></i><b>4.2.6</b> No commutative property<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#transpose-and-related-properties"><i class="fa fa-check"></i><b>4.3</b> Transpose and related properties<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#definition-2"><i class="fa fa-check"></i><b>4.3.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#transpose-and-mathematical-operations"><i class="fa fa-check"></i><b>4.3.2</b> Transpose and mathematical operations<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#special-matrices"><i class="fa fa-check"></i><b>4.4</b> Special matrices<span></span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#square-matrices"><i class="fa fa-check"></i><b>4.4.1</b> Square matrices<span></span></a></li>
<li class="chapter" data-level="4.4.2" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#identity-matrix"><i class="fa fa-check"></i><b>4.4.2</b> Identity matrix<span></span></a></li>
<li class="chapter" data-level="4.4.3" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#symmetric"><i class="fa fa-check"></i><b>4.4.3</b> Symmetric<span></span></a></li>
<li class="chapter" data-level="4.4.4" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#idempotent"><i class="fa fa-check"></i><b>4.4.4</b> Idempotent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-inverse"><i class="fa fa-check"></i><b>4.5</b> Matrix inverse<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="useful-matrix-facts.html"><a href="useful-matrix-facts.html#matrix-derivatives"><i class="fa fa-check"></i><b>4.6</b> Matrix derivatives<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html"><i class="fa fa-check"></i><b>5</b> Defining and fitting a linear model<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#background-and-terminology"><i class="fa fa-check"></i><b>5.1</b> Background and terminology<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#goals-of-regression"><i class="fa fa-check"></i><b>5.2</b> Goals of regression<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#definition-of-a-linear-model"><i class="fa fa-check"></i><b>5.3</b> Definition of a linear model<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#basic-construction-and-relationships"><i class="fa fa-check"></i><b>5.3.1</b> Basic construction and relationships<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#as-a-system-of-equations"><i class="fa fa-check"></i><b>5.3.2</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="5.3.3" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#using-matrix-notation"><i class="fa fa-check"></i><b>5.3.3</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#summarizing-the-components-of-a-linear-model"><i class="fa fa-check"></i><b>5.4</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#types-of-regression-models"><i class="fa fa-check"></i><b>5.5</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#standard-linear-model-assumptions-and-implications"><i class="fa fa-check"></i><b>5.6</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#mathematical-interpretation-of-coefficients"><i class="fa fa-check"></i><b>5.7</b> Mathematical interpretation of coefficients<span></span></a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#coefficient-interpretation-in-simple-linear-regression"><i class="fa fa-check"></i><b>5.7.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="5.7.2" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#coefficient-interpretation-in-multiple-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Coefficient interpretation in multiple linear regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="defining-and-fitting-a-linear-model.html"><a href="defining-and-fitting-a-linear-model.html#exercises"><i class="fa fa-check"></i><b>5.8</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html"><i class="fa fa-check"></i><b>6</b> Linear model estimation<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#a-simple-motivating-example"><i class="fa fa-check"></i><b>6.1</b> A simple motivating example<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#defining-a-linear-model"><i class="fa fa-check"></i><b>6.2</b> Defining a linear model<span></span></a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss-necessary-components"><i class="fa fa-check"></i><b>6.2.1</b> Necessary components and notation<span></span></a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#standard-definition-of-linear-model"><i class="fa fa-check"></i><b>6.2.2</b> Standard definition of linear model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>6.3</b> Estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss"><i class="fa fa-check"></i><b>6.3.1</b> Fitted values, residuals, and RSS<span></span></a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimators-of-the-simple-linear-regression-parameters"><i class="fa fa-check"></i><b>6.3.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-slr"><i class="fa fa-check"></i><b>6.4</b> Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#estimation-of-the-multiple-linear-regression-coefficients"><i class="fa fa-check"></i><b>6.5</b> Estimation of the multiple linear regression coefficients)<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#using-matrix-notation-to-represent-a-linear-model"><i class="fa fa-check"></i><b>6.5.1</b> Using matrix notation to represent a linear model<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ss:fv-resid-rss-mlr"><i class="fa fa-check"></i><b>6.5.2</b> Residuals, fitted values, and RSS for multiple linear regression<span></span></a></li>
<li class="chapter" data-level="6.5.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#ols-estimator-of-the-linear-model-parameters"><i class="fa fa-check"></i><b>6.5.3</b> OLS estimator of the linear model parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr"><i class="fa fa-check"></i><b>6.6</b> Penguins multiple linear regression example<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#categorical-predictors"><i class="fa fa-check"></i><b>6.7</b> Categorical predictors<span></span></a></li>
<li class="chapter" data-level="6.8" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#s:penguins-mlr2"><i class="fa fa-check"></i><b>6.8</b> Penguins multiple linear regression example with categorical predictor<span></span></a></li>
<li class="chapter" data-level="6.9" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#evaluating-model-fit"><i class="fa fa-check"></i><b>6.9</b> Evaluating model fit<span></span></a></li>
<li class="chapter" data-level="6.10" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-notation"><i class="fa fa-check"></i><b>6.10</b> Summary of notation<span></span></a></li>
<li class="chapter" data-level="6.11" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summary-of-functions-used-in-this-chapter"><i class="fa fa-check"></i><b>6.11</b> Summary of functions used in this chapter<span></span></a></li>
<li class="chapter" data-level="6.12" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#summarizing-the-components-of-a-linear-model-1"><i class="fa fa-check"></i><b>6.12</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="6.13" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#going-deeper"><i class="fa fa-check"></i><b>6.13</b> Going Deeper<span></span></a>
<ul>
<li class="chapter" data-level="6.13.1" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manually-estimating-the-simple-linear-regression-coefficients"><i class="fa fa-check"></i><b>6.13.1</b> Manually estimating the simple linear regression coefficients<span></span></a></li>
<li class="chapter" data-level="6.13.2" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#manually-estimating-the-multiple-linear-regression-coefficients"><i class="fa fa-check"></i><b>6.13.2</b> Manually estimating the multiple linear regression coefficients<span></span></a></li>
<li class="chapter" data-level="6.13.3" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#parameter-estimation-and-matrix-decompositions"><i class="fa fa-check"></i><b>6.13.3</b> Parameter estimation and matrix decompositions<span></span></a></li>
<li class="chapter" data-level="6.13.4" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#updating-a-model"><i class="fa fa-check"></i><b>6.13.4</b> Updating a model<span></span></a></li>
<li class="chapter" data-level="6.13.5" data-path="linear-model-estimation.html"><a href="linear-model-estimation.html#more-discussion-of-formula-for-model-building"><i class="fa fa-check"></i><b>6.13.5</b> More discussion of <code>formula</code> for model-building<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html"><i class="fa fa-check"></i><b>7</b> Parameter estimation for linear models<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#ols-estimation-of-the-simple-linear-regression-model"><i class="fa fa-check"></i><b>7.1</b> OLS estimation of the simple linear regression model<span></span></a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#visualizing-the-rss-as-a-function-of-the-estimated-coefficients"><i class="fa fa-check"></i><b>7.1.1</b> Visualizing the RSS as a function of the estimated coefficients<span></span></a></li>
<li class="chapter" data-level="7.1.2" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#ols-estimators-of-the-simple-linear-regression-parameters-1"><i class="fa fa-check"></i><b>7.1.2</b> OLS estimators of the simple linear regression parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#penguins-simple-linear-regression-example"><i class="fa fa-check"></i><b>7.2</b> Penguins simple linear regression example<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#fitting-a-linear-model-using-r"><i class="fa fa-check"></i><b>7.3</b> Fitting a linear model using R<span></span></a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="parameter-estimation-for-linear-models.html"><a href="parameter-estimation-for-linear-models.html#derivation-of-ols-simple-linear-regression-estimators"><i class="fa fa-check"></i><b>7.3.1</b> Derivation of OLS simple linear regression estimators<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html"><i class="fa fa-check"></i><b>8</b> Interpreting a fitted linear model<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#orthogonality"><i class="fa fa-check"></i><b>8.1</b> Orthogonality<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#example-fuel-consumption-data"><i class="fa fa-check"></i><b>8.2</b> Example: Fuel Consumption Data<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#setting-up-a-linear-model"><i class="fa fa-check"></i><b>8.3</b> Setting Up a Linear Model<span></span></a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#adjusting-units"><i class="fa fa-check"></i><b>8.3.1</b> Adjusting Units<span></span></a></li>
<li class="chapter" data-level="8.3.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#fitting-a-model"><i class="fa fa-check"></i><b>8.3.2</b> Fitting a Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#interpreting-the-coefficients"><i class="fa fa-check"></i><b>8.4</b> Interpreting the Coefficients<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#example-berkeley-guidance-study"><i class="fa fa-check"></i><b>8.5</b> Example: Berkeley Guidance Study<span></span></a></li>
<li class="chapter" data-level="8.6" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#dictionary-of-data"><i class="fa fa-check"></i><b>8.6</b> Dictionary of Data<span></span></a></li>
<li class="chapter" data-level="8.7" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#analysing-relations-between-regressors"><i class="fa fa-check"></i><b>8.7</b> Analysing Relations Between Regressors<span></span></a></li>
<li class="chapter" data-level="8.8" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#adjusting-the-regressors"><i class="fa fa-check"></i><b>8.8</b> Adjusting the Regressors<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#comparing-different-models"><i class="fa fa-check"></i><b>8.9</b> Comparing Different Models<span></span></a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-wt9-and-wt18"><i class="fa fa-check"></i><b>8.9.1</b> BMI relation to WT2, WT9 and WT18<span></span></a></li>
<li class="chapter" data-level="8.9.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-dw9-and-dw18"><i class="fa fa-check"></i><b>8.9.2</b> BMI relation to WT2, DW9 and DW18<span></span></a></li>
<li class="chapter" data-level="8.9.3" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#bmi-relation-to-wt2-wt9-wt18-dw9-and-dw18"><i class="fa fa-check"></i><b>8.9.3</b> BMI relation to WT2, WT9, WT18, DW9 and DW18<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#effect-plots"><i class="fa fa-check"></i><b>8.10</b> Effect Plots<span></span></a></li>
<li class="chapter" data-level="8.11" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#regressors-on-logarithmic-scale"><i class="fa fa-check"></i><b>8.11</b> Regressors on Logarithmic Scale<span></span></a></li>
<li class="chapter" data-level="8.12" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-natural-log-scale."><i class="fa fa-check"></i><b>8.12</b> Below we create an effects plot on a natural log scale.<span></span></a></li>
<li class="chapter" data-level="8.13" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-log10-scale."><i class="fa fa-check"></i><b>8.13</b> Below we create an effects plot on a log10 scale.<span></span></a></li>
<li class="chapter" data-level="8.14" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#below-we-create-an-effects-plot-on-a-regular-scale."><i class="fa fa-check"></i><b>8.14</b> Below we create an effects plot on a regular scale.<span></span></a></li>
<li class="chapter" data-level="8.15" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#interpreting-coefficients-with-log-scale-on-regressor"><i class="fa fa-check"></i><b>8.15</b> Interpreting Coefficients with Log Scale on Regressor<span></span></a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#natural-log-scale"><i class="fa fa-check"></i><b>8.15.1</b> Natural Log Scale<span></span></a></li>
<li class="chapter" data-level="8.15.2" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#common-log-base-10-scale"><i class="fa fa-check"></i><b>8.15.2</b> Common Log (base 10) Scale<span></span></a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#log-level-interpretation"><i class="fa fa-check"></i><b>8.16</b> Log-Level Interpretation<span></span></a></li>
<li class="chapter" data-level="8.17" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#log-log-interpretation"><i class="fa fa-check"></i><b>8.17</b> Log-log Interpretation<span></span></a></li>
<li class="chapter" data-level="8.18" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#summary-of-interpretations-simple-linear-regression"><i class="fa fa-check"></i><b>8.18</b> Summary of Interpretations (Simple Linear Regression)<span></span></a></li>
<li class="chapter" data-level="8.19" data-path="interpreting-a-fitted-linear-model.html"><a href="interpreting-a-fitted-linear-model.html#more-practice"><i class="fa fa-check"></i><b>8.19</b> More Practice<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html"><i class="fa fa-check"></i><b>9</b> Categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#indicatordummy-variables"><i class="fa fa-check"></i><b>9.1</b> Indicator/dummy variables<span></span></a></li>
<li class="chapter" data-level="9.2" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#common-of-linear-models-with-categorical-predictors"><i class="fa fa-check"></i><b>9.2</b> Common of linear models with categorical predictors<span></span></a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#one-way-anova"><i class="fa fa-check"></i><b>9.2.1</b> One-way ANOVA<span></span></a></li>
<li class="chapter" data-level="9.2.2" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#main-effects-models"><i class="fa fa-check"></i><b>9.2.2</b> Main effects models<span></span></a></li>
<li class="chapter" data-level="9.2.3" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#interaction-models"><i class="fa fa-check"></i><b>9.2.3</b> Interaction models<span></span></a></li>
<li class="chapter" data-level="9.2.4" data-path="categorical-predictors-1.html"><a href="categorical-predictors-1.html#extensions"><i class="fa fa-check"></i><b>9.2.4</b> Extensions<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="assessing-and-addressing-collinearity.html"><a href="assessing-and-addressing-collinearity.html"><i class="fa fa-check"></i><b>10</b> Assessing and addressing collinearity<span></span></a></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="chapter" data-level="11" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html"><i class="fa fa-check"></i><b>11</b> Defining and fitting a linear model<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#background-and-terminology-1"><i class="fa fa-check"></i><b>11.1</b> Background and terminology<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#goals-of-regression-1"><i class="fa fa-check"></i><b>11.2</b> Goals of regression<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#definition-of-a-linear-model-1"><i class="fa fa-check"></i><b>11.3</b> Definition of a linear model<span></span></a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#basic-construction-and-relationships-1"><i class="fa fa-check"></i><b>11.3.1</b> Basic construction and relationships<span></span></a></li>
<li class="chapter" data-level="11.3.2" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#as-a-system-of-equations-1"><i class="fa fa-check"></i><b>11.3.2</b> As a system of equations<span></span></a></li>
<li class="chapter" data-level="11.3.3" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#using-matrix-notation-1"><i class="fa fa-check"></i><b>11.3.3</b> Using matrix notation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#summarizing-the-components-of-a-linear-model-2"><i class="fa fa-check"></i><b>11.4</b> Summarizing the components of a linear model<span></span></a></li>
<li class="chapter" data-level="11.5" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#types-of-regression-models-1"><i class="fa fa-check"></i><b>11.5</b> Types of regression models<span></span></a></li>
<li class="chapter" data-level="11.6" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#standard-linear-model-assumptions-and-implications-1"><i class="fa fa-check"></i><b>11.6</b> Standard linear model assumptions and implications<span></span></a></li>
<li class="chapter" data-level="11.7" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#mathematical-interpretation-of-coefficients-1"><i class="fa fa-check"></i><b>11.7</b> Mathematical interpretation of coefficients<span></span></a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#coefficient-interpretation-in-simple-linear-regression-1"><i class="fa fa-check"></i><b>11.7.1</b> Coefficient interpretation in simple linear regression<span></span></a></li>
<li class="chapter" data-level="11.7.2" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#coefficient-interpretation-in-multiple-linear-regression-1"><i class="fa fa-check"></i><b>11.7.2</b> Coefficient interpretation in multiple linear regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="defining-and-fitting-a-linear-model-1.html"><a href="defining-and-fitting-a-linear-model-1.html#exercises-1"><i class="fa fa-check"></i><b>11.8</b> Exercises<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="linear-model-inference.html"><a href="linear-model-inference.html"><i class="fa fa-check"></i><b>12</b> Linear model inference<span></span></a></li>
<li class="chapter" data-level="13" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html"><i class="fa fa-check"></i><b>13</b> Assumptions Stated and Prioritized<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-concisely-stated"><i class="fa fa-check"></i><b>13.1</b> Standard assumptions concisely stated<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#standard-assumptions-prioritized"><i class="fa fa-check"></i><b>13.2</b> Standard assumptions prioritized<span></span></a></li>
<li class="chapter" data-level="13.3" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#loading-the-data"><i class="fa fa-check"></i><b>13.3</b> Loading the Data<span></span></a></li>
<li class="chapter" data-level="13.4" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#question-1-why-might-it-be-a-bad-a-idea-to-simply-include-all-regressors"><i class="fa fa-check"></i><b>13.4</b> Question 1: Why might it be a bad a idea to simply include all regressors?<span></span></a></li>
<li class="chapter" data-level="13.5" data-path="assumptions-stated-and-prioritized.html"><a href="assumptions-stated-and-prioritized.html#question-2-what-metricsmethods-have-we-used-to-compare-models-thus-far"><i class="fa fa-check"></i><b>13.5</b> Question 2: What metrics/methods have we used to compare models thus far?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="variable-selection.html"><a href="variable-selection.html"><i class="fa fa-check"></i><b>14</b> Variable Selection<span></span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="variable-selection.html"><a href="variable-selection.html#testing-based-procedures"><i class="fa fa-check"></i><b>14.1</b> Testing-Based Procedures<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="variable-selection.html"><a href="variable-selection.html#backward-elimination"><i class="fa fa-check"></i><b>14.2</b> Backward elimination<span></span></a></li>
<li class="chapter" data-level="14.3" data-path="variable-selection.html"><a href="variable-selection.html#question-3-which-of-the-predictors-would-you-remove-from-the-full-model-what-criteria-did-you-use-to-make-that-decision"><i class="fa fa-check"></i><b>14.3</b> Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision?<span></span></a></li>
<li class="chapter" data-level="14.4" data-path="variable-selection.html"><a href="variable-selection.html#forward-selection"><i class="fa fa-check"></i><b>14.4</b> Forward Selection<span></span></a></li>
<li class="chapter" data-level="14.5" data-path="variable-selection.html"><a href="variable-selection.html#stepwise-regression"><i class="fa fa-check"></i><b>14.5</b> Stepwise Regression<span></span></a></li>
<li class="chapter" data-level="14.6" data-path="variable-selection.html"><a href="variable-selection.html#model-hierarchy"><i class="fa fa-check"></i><b>14.6</b> Model Hierarchy<span></span></a></li>
<li class="chapter" data-level="14.7" data-path="variable-selection.html"><a href="variable-selection.html#criterion-based-procedures"><i class="fa fa-check"></i><b>14.7</b> Criterion-Based Procedures<span></span></a></li>
<li class="chapter" data-level="14.8" data-path="variable-selection.html"><a href="variable-selection.html#akaikes-information-criterion-aic"><i class="fa fa-check"></i><b>14.8</b> Akaike’s Information Criterion (AIC)<span></span></a>
<ul>
<li class="chapter" data-level="14.8.1" data-path="variable-selection.html"><a href="variable-selection.html#interpreting-aic"><i class="fa fa-check"></i><b>14.8.1</b> Interpreting AIC<span></span></a></li>
<li class="chapter" data-level="14.8.2" data-path="variable-selection.html"><a href="variable-selection.html#exhaustive-model-searches"><i class="fa fa-check"></i><b>14.8.2</b> Exhaustive Model Searches<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="variable-selection.html"><a href="variable-selection.html#question-4-interpret-the-output-from-the-code-above.-is-this-consistent-with-the-model-we-obtained-using-backward-elimination"><i class="fa fa-check"></i><b>14.9</b> Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination?<span></span></a>
<ul>
<li class="chapter" data-level="14.9.1" data-path="variable-selection.html"><a href="variable-selection.html#computing-the-aic"><i class="fa fa-check"></i><b>14.9.1</b> Computing the AIC<span></span></a></li>
<li class="chapter" data-level="14.9.2" data-path="variable-selection.html"><a href="variable-selection.html#question-5-interpret-the-output-from-the-aic-plots-above.-what-is-the-best-model-according-to-this-metric"><i class="fa fa-check"></i><b>14.9.2</b> Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric?<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.10" data-path="variable-selection.html"><a href="variable-selection.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>14.10</b> Bayesian Information Criterion (BIC)<span></span></a></li>
<li class="chapter" data-level="14.11" data-path="variable-selection.html"><a href="variable-selection.html#question-6-interpret-the-output-from-the-bic-plot-above.-what-is-the-best-model-according-to-this-metric"><i class="fa fa-check"></i><b>14.11</b> Question 6: Interpret the output from the BIC plot above. What is the best model according to this metric?<span></span></a>
<ul>
<li class="chapter" data-level="14.11.1" data-path="variable-selection.html"><a href="variable-selection.html#pros-of-using-mles-to-estimate-population-parameters"><i class="fa fa-check"></i><b>14.11.1</b> Pros of Using MLE’s to Estimate Population Parameters<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="variable-selection.html"><a href="variable-selection.html#model-selection-what-is-the-right-number-of-regressors-we-should-include"><i class="fa fa-check"></i><b>14.12</b> Model Selection: What is the right number of regressors we should include?<span></span></a></li>
<li class="chapter" data-level="14.13" data-path="variable-selection.html"><a href="variable-selection.html#motivating-example-predicting-life-expectancy"><i class="fa fa-check"></i><b>14.13</b> Motivating Example: Predicting Life Expectancy<span></span></a></li>
<li class="chapter" data-level="14.14" data-path="variable-selection.html"><a href="variable-selection.html#loading-the-data-1"><i class="fa fa-check"></i><b>14.14</b> Loading the Data<span></span></a></li>
<li class="chapter" data-level="14.15" data-path="variable-selection.html"><a href="variable-selection.html#recap-from-last-class"><i class="fa fa-check"></i><b>14.15</b> Recap from Last Class<span></span></a></li>
<li class="chapter" data-level="14.16" data-path="variable-selection.html"><a href="variable-selection.html#testing-based-procedures-1"><i class="fa fa-check"></i><b>14.16</b> Testing Based Procedures<span></span></a></li>
<li class="chapter" data-level="14.17" data-path="variable-selection.html"><a href="variable-selection.html#search-strategies"><i class="fa fa-check"></i><b>14.17</b> Search Strategies<span></span></a>
<ul>
<li class="chapter" data-level="14.17.1" data-path="variable-selection.html"><a href="variable-selection.html#finding-the-best-subsets"><i class="fa fa-check"></i><b>14.17.1</b> Finding the Best Subsets<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.18" data-path="variable-selection.html"><a href="variable-selection.html#review-of-criterion-based-procedures-thus-far"><i class="fa fa-check"></i><b>14.18</b> Review of Criterion-Based Procedures Thus Far<span></span></a></li>
<li class="chapter" data-level="14.19" data-path="variable-selection.html"><a href="variable-selection.html#akaikes-information-criterion-aic-and-bayesian-information-criteria-bic"><i class="fa fa-check"></i><b>14.19</b> Akaike’s Information Criterion (AIC) and Bayesian Information Criteria (BIC)<span></span></a>
<ul>
<li class="chapter" data-level="14.19.1" data-path="variable-selection.html"><a href="variable-selection.html#optional-if-you-want-to-compare-with-formulas"><i class="fa fa-check"></i><b>14.19.1</b> Optional if You Want to Compare With Formulas<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.20" data-path="variable-selection.html"><a href="variable-selection.html#adjusted-r2"><i class="fa fa-check"></i><b>14.20</b> Adjusted <span class="math inline">\(R^2\)</span><span></span></a></li>
<li class="chapter" data-level="14.21" data-path="variable-selection.html"><a href="variable-selection.html#mean-square-error-mse"><i class="fa fa-check"></i><b>14.21</b> Mean Square Error (MSE)<span></span></a></li>
<li class="chapter" data-level="14.22" data-path="variable-selection.html"><a href="variable-selection.html#mallows-c_p-statistic"><i class="fa fa-check"></i><b>14.22</b> Mallow’s <span class="math inline">\(C_p\)</span> Statistic<span></span></a>
<ul>
<li class="chapter" data-level="14.22.1" data-path="variable-selection.html"><a href="variable-selection.html#computing-mallows-c_p-statistic"><i class="fa fa-check"></i><b>14.22.1</b> Computing Mallow’s <span class="math inline">\(C_p\)</span> Statistic<span></span></a></li>
<li class="chapter" data-level="14.22.2" data-path="variable-selection.html"><a href="variable-selection.html#question-7-interpret-the-output-from-the-c_p-plot-above.-what-is-the-best-model-according-to-this-metric"><i class="fa fa-check"></i><b>14.22.2</b> Question 7: Interpret the output from the <span class="math inline">\(C_p\)</span> plot above. What is the best model according to this metric?<span></span></a></li>
<li class="chapter" data-level="14.22.3" data-path="variable-selection.html"><a href="variable-selection.html#root-mean-squared-error"><i class="fa fa-check"></i><b>14.22.3</b> Root Mean Squared Error<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.23" data-path="variable-selection.html"><a href="variable-selection.html#cross-validation"><i class="fa fa-check"></i><b>14.23</b> Cross-validation<span></span></a>
<ul>
<li class="chapter" data-level="14.23.1" data-path="variable-selection.html"><a href="variable-selection.html#leave-one-out-crossvalidation"><i class="fa fa-check"></i><b>14.23.1</b> Leave-One Out Crossvalidation<span></span></a></li>
<li class="chapter" data-level="14.23.2" data-path="variable-selection.html"><a href="variable-selection.html#k-fold-crossvalidation"><i class="fa fa-check"></i><b>14.23.2</b> <span class="math inline">\(k\)</span>-Fold Crossvalidation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14.24" data-path="variable-selection.html"><a href="variable-selection.html#example-of-k-fold-crossvalidation"><i class="fa fa-check"></i><b>14.24</b> Example of <span class="math inline">\(k\)</span>-fold Crossvalidation<span></span></a></li>
<li class="chapter" data-level="14.25" data-path="variable-selection.html"><a href="variable-selection.html#theres-still-more-to-consider"><i class="fa fa-check"></i><b>14.25</b> There’s Still More to Consider!<span></span></a></li>
<li class="chapter" data-level="14.26" data-path="variable-selection.html"><a href="variable-selection.html#summary"><i class="fa fa-check"></i><b>14.26</b> Summary<span></span></a></li>
<li class="chapter" data-level="14.27" data-path="variable-selection.html"><a href="variable-selection.html#exercise"><i class="fa fa-check"></i><b>14.27</b> Exercise<span></span></a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Joshua French</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variable-selection" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">Chapter 14</span> Variable Selection<a href="variable-selection.html#variable-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>There are two aspects to variable selection:</p>
<ul>
<li>The strategy used to search for the “optimal” model.</li>
<li>The criterion used to compare models.</li>
</ul>
<div id="testing-based-procedures" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Testing-Based Procedures<a href="variable-selection.html#testing-based-procedures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="backward-elimination" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Backward elimination<a href="variable-selection.html#backward-elimination" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Backward elimination</strong> is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed.</p>
</div>
<div id="question-3-which-of-the-predictors-would-you-remove-from-the-full-model-what-criteria-did-you-use-to-make-that-decision" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Question 3: Which of the predictors would you remove from the full model? What criteria did you use to make that decision?<a href="variable-selection.html#question-3-which-of-the-predictors-would-you-remove-from-the-full-model-what-criteria-did-you-use-to-make-that-decision" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="variable-selection.html#cb3-1" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> ., <span class="at">data =</span> statedata)</span>
<span id="cb3-2"><a href="variable-selection.html#cb3-2" aria-hidden="true" tabindex="-1"></a>faraway<span class="sc">::</span><span class="fu">sumary</span>(lmod)</span></code></pre></div>
<pre><code>##                Estimate  Std. Error t value  Pr(&gt;|t|)
## (Intercept)  7.0943e+01  1.7480e+00 40.5859 &lt; 2.2e-16
## Population   5.1800e-05  2.9187e-05  1.7748   0.08318
## Income      -2.1804e-05  2.4443e-04 -0.0892   0.92934
## Illiteracy   3.3820e-02  3.6628e-01  0.0923   0.92687
## Murder      -3.0112e-01  4.6621e-02 -6.4590  8.68e-08
## HS.Grad      4.8929e-02  2.3323e-02  2.0979   0.04197
## Frost       -5.7350e-03  3.1432e-03 -1.8246   0.07519
## Area        -7.3832e-08  1.6682e-06 -0.0443   0.96491
## 
## n = 50, p = 8, Residual SE = 0.74478, R-Squared = 0.74</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="variable-selection.html#cb5-1" aria-hidden="true" tabindex="-1"></a>back1 <span class="ot">&lt;-</span> <span class="fu">update</span>(lmod, . <span class="sc">~</span> . <span class="sc">-</span> Area)</span>
<span id="cb5-2"><a href="variable-selection.html#cb5-2" aria-hidden="true" tabindex="-1"></a>faraway<span class="sc">::</span><span class="fu">sumary</span>(back1)</span></code></pre></div>
<pre><code>##                Estimate  Std. Error t value  Pr(&gt;|t|)
## (Intercept)  7.0989e+01  1.3875e+00 51.1652 &lt; 2.2e-16
## Population   5.1883e-05  2.8788e-05  1.8023   0.07852
## Income      -2.4440e-05  2.3429e-04 -0.1043   0.91740
## Illiteracy   2.8459e-02  3.4163e-01  0.0833   0.93400
## Murder      -3.0182e-01  4.3344e-02 -6.9634 1.454e-08
## HS.Grad      4.8472e-02  2.0667e-02  2.3454   0.02369
## Frost       -5.7758e-03  2.9702e-03 -1.9446   0.05839
## 
## n = 50, p = 7, Residual SE = 0.73608, R-Squared = 0.74</code></pre>
<ul>
<li>This does not imply the other variables are not related to the response.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="variable-selection.html#cb7-1" aria-hidden="true" tabindex="-1"></a>lmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> Illiteracy, <span class="at">data =</span> statedata)</span>
<span id="cb7-2"><a href="variable-selection.html#cb7-2" aria-hidden="true" tabindex="-1"></a>faraway<span class="sc">::</span><span class="fu">sumary</span>(lmod1)</span></code></pre></div>
<pre><code>##             Estimate Std. Error  t value  Pr(&gt;|t|)
## (Intercept) 72.39495    0.33834 213.9734 &lt; 2.2e-16
## Illiteracy  -1.29602    0.25701  -5.0427 6.969e-06
## 
## n = 50, p = 2, Residual SE = 1.09659, R-Squared = 0.35</code></pre>
</div>
<div id="forward-selection" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Forward Selection<a href="variable-selection.html#forward-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Forward selection</strong> starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor.</p>
<ul>
<li>For example, first add the predictor with the smallest <span class="math inline">\(p\)</span>-value.</li>
<li>Then compare models with the first predictor plus a second predictor and add the predictor which has the smallest <span class="math inline">\(p\)</span>-value.</li>
</ul>
</div>
<div id="stepwise-regression" class="section level2 hasAnchor" number="14.5">
<h2><span class="header-section-number">14.5</span> Stepwise Regression<a href="variable-selection.html#stepwise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Stepwise regression</strong> is a combination of backward elimination and forward selection.</p>
<ul>
<li>This addresses the situation where variables are added or removed early in the process and we want to change our mind.
Stepwise selection can miss the optimal model because we do not consider all possible models due to the one-at-a-time nature of adding/removing regressors.</li>
<li><span class="math inline">\(p\)</span>-values should not be taken as very accurate in stepwise searches because we are bound to see small <span class="math inline">\(p\)</span>-values due to chance alone.</li>
<li>Stepwise selection tends to produce simpler models that are not necessarily the best for prediction.</li>
</ul>
</div>
<div id="model-hierarchy" class="section level2 hasAnchor" number="14.6">
<h2><span class="header-section-number">14.6</span> Model Hierarchy<a href="variable-selection.html#model-hierarchy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We must respect hierarchy in models when it is naturally present.</p>
<ul>
<li>In polynomial models, <span class="math inline">\(X^2\)</span> is a higher order term than <span class="math inline">\(X\)</span>.</li>
<li>A lower order term should be retained if a higher order term is retained to increase the flexibility.
<ul>
<li>The model <span class="math inline">\(Y=\beta_0+\beta_2 X^2+\epsilon\)</span>, the maximum/minimum value MUST occur <span class="math inline">\(x=0\)</span></li>
<li>For the model <span class="math inline">\(y=\beta_0+\beta_1 X+\beta_2 X^2+\epsilon\)</span>, the maximum/minimum value can occur anywhere along the real line (depending on what the data suggest).</li>
</ul></li>
<li>If we fit the model <span class="math inline">\(y=\beta_0+\beta_1 X+\beta_2 X^2+\epsilon\)</span> and <span class="math inline">\(\beta_1\)</span> is not significant, it would NOT make sense to remove <span class="math inline">\(X\)</span> from the model but still keep <span class="math inline">\(X^2\)</span>.</li>
</ul>
</div>
<div id="criterion-based-procedures" class="section level2 hasAnchor" number="14.7">
<h2><span class="header-section-number">14.7</span> Criterion-Based Procedures<a href="variable-selection.html#criterion-based-procedures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Akaike’s Information Criterion (AIC)</strong> and the <strong>Bayesian Information Criterion (BIC)</strong> are two information-based criteria for variable selection.</p>
</div>
<div id="akaikes-information-criterion-aic" class="section level2 hasAnchor" number="14.8">
<h2><span class="header-section-number">14.8</span> Akaike’s Information Criterion (AIC)<a href="variable-selection.html#akaikes-information-criterion-aic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math inline">\(\mbox{AIC}(\mathcal{M})=-2L(\mathcal{M})+2p_{\mathcal{M}}\)</span>, where <span class="math inline">\(\mathcal{M}\)</span> is the model, <span class="math inline">\(L(\mathcal{M})\)</span> is the <strong>log-likelihood</strong> of the model using the <strong>MLE</strong> estimates of the parameters, and <span class="math inline">\(p_{\mathcal{M}}\)</span> is the number of regression coefficients in model <span class="math inline">\(\mathcal{M}\)</span>.</p>
<p>For linear regression models, <span class="math inline">\(-2L(\mathcal{M})=n\log{(\mbox{RSS}_{\mathcal{M}}/n)} + c\)</span>, where <span class="math inline">\(c\)</span> is a constant that depends only on the observed data and not on the model, and <span class="math inline">\(\mbox{RSS}_{\mathcal{M}}\)</span> is the RSS of model <span class="math inline">\(\mathcal{M}\)</span>. The constant <span class="math inline">\(c\)</span> is the same for a given data set, so they can be ignored when comparing models that based on the same data set.</p>
<div id="interpreting-aic" class="section level3 hasAnchor" number="14.8.1">
<h3><span class="header-section-number">14.8.1</span> Interpreting AIC<a href="variable-selection.html#interpreting-aic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The formula for AIC is derived from a metric that can be used to measure how far a model is from the true model.</p>
<ul>
<li>As <span class="math inline">\(\mbox{RSS}_{\mathcal{M}}\)</span> gets smaller (better fit), <span class="math inline">\(n\log{(\mbox{RSS}_{\mathcal{M}}/n)}\)</span> gets smaller (becomes more negative).
<ul>
<li>Adding more predictors (that are not collinear) will improve the fit.</li>
</ul></li>
<li>As <span class="math inline">\(p_{\mathcal{M}}\)</span> gets bigger, the second term of AIC gets larger.
<ul>
<li>The second component penalizes the model according its complexity.</li>
<li>The more parameters, the larger the penalty.</li>
</ul></li>
<li>Models with more parameters will fit better (reducing the RSS), but will be penalized more for having additional parameters.</li>
<li>AIC provides a balance between fit and simplicity.
<ul>
<li>AIC identifies good fitting models (small RSS) that are simple (not a lot of predictors).</li>
</ul></li>
<li><strong>We choose the model the minimizes the AIC</strong>.</li>
</ul>
</div>
<div id="exhaustive-model-searches" class="section level3 hasAnchor" number="14.8.2">
<h3><span class="header-section-number">14.8.2</span> Exhaustive Model Searches<a href="variable-selection.html#exhaustive-model-searches" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>leaps</code> package searches all possible combinations of predictors.</p>
<ul>
<li>For each value of <span class="math inline">\(p\)</span> (number of predictors), it finds the variables that give the minimum RSS.</li>
<li>For each value of <span class="math inline">\(p\)</span>, the model that minimizes the RSS will have the smallest AIC, BIC, adjusted <span class="math inline">\(R_a^2\)</span>, and Mallow’s <span class="math inline">\(C_p\)</span> (we’ll discuss these soon).</li>
<li>By default, <code>regsubsets</code> only goes up to <span class="math inline">\(p=9\)</span>. You have to set <code>nvmax = j</code>, where <span class="math inline">\(j\)</span> is the number of regressors you want to consider.</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="variable-selection.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># may need to install.package the first time</span></span>
<span id="cb9-2"><a href="variable-selection.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps) <span class="co"># you need to load package every time you want to use it</span></span>
<span id="cb9-3"><a href="variable-selection.html#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="variable-selection.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model selection by exhaustive search</span></span>
<span id="cb9-5"><a href="variable-selection.html#cb9-5" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Life.Exp <span class="sc">~</span> ., <span class="at">data =</span> statedata)</span>
<span id="cb9-6"><a href="variable-selection.html#cb9-6" aria-hidden="true" tabindex="-1"></a>rs <span class="ot">&lt;-</span> <span class="fu">summary</span>(b) <span class="co"># summarize model that minimizes RSS for each p</span></span>
<span id="cb9-7"><a href="variable-selection.html#cb9-7" aria-hidden="true" tabindex="-1"></a>rs<span class="sc">$</span>which <span class="co"># nicer output</span></span></code></pre></div>
<pre><code>##   (Intercept) Population Income Illiteracy Murder HS.Grad Frost  Area
## 1        TRUE      FALSE  FALSE      FALSE   TRUE   FALSE FALSE FALSE
## 2        TRUE      FALSE  FALSE      FALSE   TRUE    TRUE FALSE FALSE
## 3        TRUE      FALSE  FALSE      FALSE   TRUE    TRUE  TRUE FALSE
## 4        TRUE       TRUE  FALSE      FALSE   TRUE    TRUE  TRUE FALSE
## 5        TRUE       TRUE   TRUE      FALSE   TRUE    TRUE  TRUE FALSE
## 6        TRUE       TRUE   TRUE       TRUE   TRUE    TRUE  TRUE FALSE
## 7        TRUE       TRUE   TRUE       TRUE   TRUE    TRUE  TRUE  TRUE</code></pre>
</div>
</div>
<div id="question-4-interpret-the-output-from-the-code-above.-is-this-consistent-with-the-model-we-obtained-using-backward-elimination" class="section level2 hasAnchor" number="14.9">
<h2><span class="header-section-number">14.9</span> Question 4: Interpret the output from the code above. Is this consistent with the model we obtained using backward elimination?<a href="variable-selection.html#question-4-interpret-the-output-from-the-code-above.-is-this-consistent-with-the-model-we-obtained-using-backward-elimination" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="computing-the-aic" class="section level3 hasAnchor" number="14.9.1">
<h3><span class="header-section-number">14.9.1</span> Computing the AIC<a href="variable-selection.html#computing-the-aic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="variable-selection.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What output is stored after running regsubsets</span></span>
<span id="cb11-2"><a href="variable-selection.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(rs)</span></code></pre></div>
<pre><code>##        Length Class      Mode     
## which  56     -none-     logical  
## rsq     7     -none-     numeric  
## rss     7     -none-     numeric  
## adjr2   7     -none-     numeric  
## cp      7     -none-     numeric  
## bic     7     -none-     numeric  
## outmat 49     -none-     character
## obj    28     regsubsets list</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="variable-selection.html#cb13-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(statedata) <span class="co">#number observation n=50</span></span>
<span id="cb13-2"><a href="variable-selection.html#cb13-2" aria-hidden="true" tabindex="-1"></a>rss <span class="ot">&lt;-</span> rs<span class="sc">$</span>rss <span class="co"># rss calculated for each model</span></span>
<span id="cb13-3"><a href="variable-selection.html#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="variable-selection.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute AIC using the formula</span></span>
<span id="cb13-5"><a href="variable-selection.html#cb13-5" aria-hidden="true" tabindex="-1"></a>AIC <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">log</span>(rss<span class="sc">/</span>n) <span class="sc">+</span> (<span class="dv">2</span><span class="sc">:</span><span class="dv">8</span>)<span class="sc">*</span><span class="dv">2</span>  <span class="co"># we start at 2 since include intercept in all</span></span>
<span id="cb13-6"><a href="variable-selection.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AIC <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>), <span class="at">ylab =</span> <span class="st">&quot;AIC&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="question-5-interpret-the-output-from-the-aic-plots-above.-what-is-the-best-model-according-to-this-metric" class="section level3 hasAnchor" number="14.9.2">
<h3><span class="header-section-number">14.9.2</span> Question 5: Interpret the output from the AIC plots above. What is the best model according to this metric?<a href="variable-selection.html#question-5-interpret-the-output-from-the-aic-plots-above.-what-is-the-best-model-according-to-this-metric" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="bayesian-information-criterion-bic" class="section level2 hasAnchor" number="14.10">
<h2><span class="header-section-number">14.10</span> Bayesian Information Criterion (BIC)<a href="variable-selection.html#bayesian-information-criterion-bic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Bayesian Information Criterion (BIC) is another criteria that is often and is almost the same as AIC.</p>
<p><span class="math display">\[BIC(\mathcal{M})=-2L(\mathcal{M})+\log{(n)}p_{\mathcal{M}}.\]</span></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="variable-selection.html#cb14-1" aria-hidden="true" tabindex="-1"></a>(BIC <span class="ot">&lt;-</span> rs<span class="sc">$</span>bic) <span class="co"># Exactly values from rs summary</span></span></code></pre></div>
<pre><code>## [1] -39.22051 -42.62472 -46.70678 -47.03640 -43.13738 -39.23342 -35.32373</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="variable-selection.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#(BIC2 &lt;- n * log(rss/n) + log(n)* (2:8))  # Using the formula</span></span>
<span id="cb16-2"><a href="variable-selection.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co">#BIC - BIC2 # The two differ by a constant</span></span>
<span id="cb16-3"><a href="variable-selection.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BIC <span class="sc">~</span> <span class="fu">I</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>), <span class="at">ylab =</span> <span class="st">&quot;BIC&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="question-6-interpret-the-output-from-the-bic-plot-above.-what-is-the-best-model-according-to-this-metric" class="section level2 hasAnchor" number="14.11">
<h2><span class="header-section-number">14.11</span> Question 6: Interpret the output from the BIC plot above. What is the best model according to this metric?<a href="variable-selection.html#question-6-interpret-the-output-from-the-bic-plot-above.-what-is-the-best-model-according-to-this-metric" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>The <code>car</code> package has a <code>subsets</code> function that takes the generates nice, labeled BIC plots generated from the <code>regsubsets</code> function.</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="variable-selection.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span></code></pre></div>
<pre><code>## Loading required package: carData</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="variable-selection.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(b, <span class="at">statistic =</span> <span class="st">&quot;bic&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre><code>##            Abbreviation
## Population            P
## Income               In
## Illiteracy           Il
## Murder                M
## HS.Grad               H
## Frost                 F
## Area                  A</code></pre>
<hr />
<p># Appendix</p>
<p>## Maximum Likelihood Estimates (MLE)</p>
<p>The <strong>likelihood function</strong> <span class="math inline">\(L(\theta)= L( \theta \mid x_1, x_2, \ldots x_n)\)</span> gives the likelihood of the parameter <span class="math inline">\(\theta\)</span> given the observed data. A <strong>maximum likelihood estimate (MLE)</strong>, <span class="math inline">\(\mathbf{\hat{\theta}_{\rm MLE}}\)</span>,}} is
a value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function.</p>
<p><strong>MLE is a process for finding the best parameter(s) for a model based on a given dataset</strong></p>
<p>Let <span class="math inline">\(f(x; \theta)\)</span> denote the pdf of a random variable <span class="math inline">\(X\)</span> with associated parameter <span class="math inline">\(\theta\)</span>. Suppose <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span> are random samples from this distribution, and <span class="math inline">\(x_1, x_2, \ldots , x_n\)</span> are the
corresponding observed values.</p>
<p><span class="math display">\[ L(\theta \mid x_1, x_2, \ldots , x_n) = f(x_1; \theta) f(x_2; \theta) \ldots f(x_n; \theta) = \prod_{i=1}^n f(x_i; \theta).\]</span></p>
<p>### Example: Finding and MLE</p>
<p>Find the MLE for <span class="math inline">\(\lambda\)</span> where <span class="math inline">\(x_1, x_2, \ldots , x_n\)</span> comes from <span class="math inline">\(X \sim \mbox{Exp}(\lambda)\)</span> with <span class="math inline">\(f(x; \lambda) = \lambda e^{-\lambda x}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Find a formula for the likelihood function.</li>
</ol>
<p><span class="math display">\[ L(\lambda \mid x_1, x_2, \ldots , x_n) = \left(\lambda e^{-\lambda x_1} \right)\left(\lambda e^{-\lambda x_2} \right) \ldots \left(\lambda e^{-\lambda x_n} \right) = \lambda^n e^{-\lambda \sum_{i=1}^n x_i} .\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Optimize the likelihood function. Find the value of <span class="math inline">\(\lambda\)</span> that makes the observed data most likely to occur.</li>
</ol>
<p><span class="math display">\[\frac{d}{d \lambda} \left( \lambda^n e^{-\lambda \sum_{i=1}^n x_i} \right) \]</span></p>
<p>Often this is really messy to solve. Taking the natural log of both sides often simplifies the calculation.</p>
<p>The <strong>log-likelihood</strong> function is <span class="math inline">\(y = \ln{L(\theta \mid x_1, x_2, \ldots , x_n) }\)</span> (often written with <span class="math inline">\(\log\)</span> though we mean <span class="math inline">\(\ln\)</span>).</p>
<ul>
<li>Since the natural log is an increasing function, the value of <span class="math inline">\(\theta\)</span> that maximizes (or minimizes) <span class="math inline">\(L(\theta \mid x_1, x_2, \ldots , x_n)\)</span> is the same value of <span class="math inline">\(\theta\)</span> that maximizes (or minimizes) <span class="math inline">\(y = \ln{L(\theta \mid x_1, x_2, \ldots , x_n) }\)</span>.</li>
</ul>
<p><span class="math display">\[y = \ln{\left(\lambda^n e^{-\lambda \sum_{i=1}^n x_i}\right)} = n \ln{(\lambda)}- \lambda \sum_{i=1}^n x_i\]</span>
It actually is easier to optimize the log-likelihood function in this case:</p>
<p><span class="math display">\[
  \begin{aligned}
\frac{d}{d \lambda} \ln{\left(\lambda^n e^{-\lambda \sum_{i=1}^n x_i}\right)} &amp;= \frac{d}{d \lambda} \left( n \ln{(\lambda)}- \lambda \sum_{i=1}^n x_i \right) \\
&amp;= \frac{n}{\lambda} - \sum_{i=1}^n x_i
\end{aligned}
\]</span></p>
<p>We have a critical value at <span class="math inline">\(\lambda = \frac{\sum x_i}{n} = \bar{x}\)</span> which is the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood function. If we assume the sample was randomly selected from an exponential distribution, then given the observed data, the most likely value for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\bar{x}\)</span>. This makes practical sense since if <span class="math inline">\(X \sim \mbox{Exp}(\lambda)\)</span>, <span class="math inline">\(E(X) = \mu = \lambda\)</span>.</p>
<div id="pros-of-using-mles-to-estimate-population-parameters" class="section level3 hasAnchor" number="14.11.1">
<h3><span class="header-section-number">14.11.1</span> Pros of Using MLE’s to Estimate Population Parameters<a href="variable-selection.html#pros-of-using-mles-to-estimate-population-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>MLE’s give estimates that make practical sense (see example above).</li>
<li><strong>Consistency</strong>: As the sample size gets larger and larger, MLE’s converge to the actual value of the parameter.</li>
<li><strong>Normality</strong>: As we get more data, MLE’s converge to a normal distribution.</li>
<li><strong>Efficiency</strong>: They have the smallest possible variance for a consistent estimator.</li>
</ul>
</div>
</div>
<div id="model-selection-what-is-the-right-number-of-regressors-we-should-include" class="section level2 hasAnchor" number="14.12">
<h2><span class="header-section-number">14.12</span> Model Selection: What is the right number of regressors we should include?<a href="variable-selection.html#model-selection-what-is-the-right-number-of-regressors-we-should-include" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Variable selection is intended to (objectively) find the “best” subset of predictors. So why not throw the whole kitchen sink into our model?</p>
</div>
<div id="motivating-example-predicting-life-expectancy" class="section level2 hasAnchor" number="14.13">
<h2><span class="header-section-number">14.13</span> Motivating Example: Predicting Life Expectancy<a href="variable-selection.html#motivating-example-predicting-life-expectancy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>state</code> datasets in the base R package (no package needed to access the data) contains various data sets with data from all 50 states. We’ll be working with the dataset <code>state.x77</code> which has 50 observations (one for each state) with the following variables of interest:</p>
<ul>
<li><code>Population</code>: Population estimate as of July 1, 1975</li>
<li><code>Income</code>: Per capita income (1974)</li>
<li><code>Illiteracy</code>: Illiteracy rate (1970, percent of population)</li>
<li><code>Life Exp</code>: life expectancy in years (1969–71)</li>
<li><code>Murder</code>: murder and non-negligent manslaughter rate per 100,000 population (1976)</li>
<li><code>HS Grad</code>: percent high-school graduates (1970)</li>
<li><code>Frost</code>: mean number of days with minimum temperature below freezing (1931–1960) in capital or large city</li>
<li><code>Area</code>: land area in square miles</li>
</ul>
</div>
<div id="loading-the-data-1" class="section level2 hasAnchor" number="14.14">
<h2><span class="header-section-number">14.14</span> Loading the Data<a href="variable-selection.html#loading-the-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="variable-selection.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(state)</span>
<span id="cb21-2"><a href="variable-selection.html#cb21-2" aria-hidden="true" tabindex="-1"></a>statedata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(state.x77, <span class="at">row.names =</span> state.abb)</span>
<span id="cb21-3"><a href="variable-selection.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">#head(statedata)</span></span>
<span id="cb21-4"><a href="variable-selection.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(statedata)</span></code></pre></div>
<pre><code>##    Population        Income       Illiteracy       Life.Exp    
##  Min.   :  365   Min.   :3098   Min.   :0.500   Min.   :67.96  
##  1st Qu.: 1080   1st Qu.:3993   1st Qu.:0.625   1st Qu.:70.12  
##  Median : 2838   Median :4519   Median :0.950   Median :70.67  
##  Mean   : 4246   Mean   :4436   Mean   :1.170   Mean   :70.88  
##  3rd Qu.: 4968   3rd Qu.:4814   3rd Qu.:1.575   3rd Qu.:71.89  
##  Max.   :21198   Max.   :6315   Max.   :2.800   Max.   :73.60  
##      Murder          HS.Grad          Frost             Area       
##  Min.   : 1.400   Min.   :37.80   Min.   :  0.00   Min.   :  1049  
##  1st Qu.: 4.350   1st Qu.:48.05   1st Qu.: 66.25   1st Qu.: 36985  
##  Median : 6.850   Median :53.25   Median :114.50   Median : 54277  
##  Mean   : 7.378   Mean   :53.11   Mean   :104.46   Mean   : 70736  
##  3rd Qu.:10.675   3rd Qu.:59.15   3rd Qu.:139.75   3rd Qu.: 81163  
##  Max.   :15.100   Max.   :67.30   Max.   :188.00   Max.   :566432</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="variable-selection.html#cb23-1" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> ., <span class="at">data =</span> statedata)</span>
<span id="cb23-2"><a href="variable-selection.html#cb23-2" aria-hidden="true" tabindex="-1"></a>faraway<span class="sc">::</span><span class="fu">sumary</span>(lmod)</span></code></pre></div>
<pre><code>##                Estimate  Std. Error t value  Pr(&gt;|t|)
## (Intercept)  7.0943e+01  1.7480e+00 40.5859 &lt; 2.2e-16
## Population   5.1800e-05  2.9187e-05  1.7748   0.08318
## Income      -2.1804e-05  2.4443e-04 -0.0892   0.92934
## Illiteracy   3.3820e-02  3.6628e-01  0.0923   0.92687
## Murder      -3.0112e-01  4.6621e-02 -6.4590  8.68e-08
## HS.Grad      4.8929e-02  2.3323e-02  2.0979   0.04197
## Frost       -5.7350e-03  3.1432e-03 -1.8246   0.07519
## Area        -7.3832e-08  1.6682e-06 -0.0443   0.96491
## 
## n = 50, p = 8, Residual SE = 0.74478, R-Squared = 0.74</code></pre>
</div>
<div id="recap-from-last-class" class="section level2 hasAnchor" number="14.15">
<h2><span class="header-section-number">14.15</span> Recap from Last Class<a href="variable-selection.html#recap-from-last-class" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Choosing more variables is not always preferable.</li>
<li>A solid yet simple model is often preferred.</li>
<li>When deciding how many predictors to include (exclude), its complicated! We should consider several criteria.</li>
</ul>
</div>
<div id="testing-based-procedures-1" class="section level2 hasAnchor" number="14.16">
<h2><span class="header-section-number">14.16</span> Testing Based Procedures<a href="variable-selection.html#testing-based-procedures-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><strong>Backward elimination</strong> is the simplest of all variable selection procedures. We start with all predictors and remove the least significant predictor. Stop once all the noise has been removed.</li>
<li><strong>Forward selection</strong> starts with the null model (only an intercept), and adds regressors one at a time until we can no longer improve the error criterion by adding a single regressor.</li>
<li><strong>Stepwise regression</strong> is a combination of backward elimination and forward selection.</li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="variable-selection.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This handy function does stepwise regression</span></span>
<span id="cb25-2"><a href="variable-selection.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation based on AIC</span></span>
<span id="cb25-3"><a href="variable-selection.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">step</span>(lmod, <span class="at">direction =</span> <span class="st">&quot;both&quot;</span>)</span></code></pre></div>
<pre><code>## Start:  AIC=-22.18
## Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + 
##     Frost + Area
## 
##              Df Sum of Sq    RSS     AIC
## - Area        1    0.0011 23.298 -24.182
## - Income      1    0.0044 23.302 -24.175
## - Illiteracy  1    0.0047 23.302 -24.174
## &lt;none&gt;                    23.297 -22.185
## - Population  1    1.7472 25.044 -20.569
## - Frost       1    1.8466 25.144 -20.371
## - HS.Grad     1    2.4413 25.738 -19.202
## - Murder      1   23.1411 46.438  10.305
## 
## Step:  AIC=-24.18
## Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + 
##     Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Illiteracy  1    0.0038 23.302 -26.174
## - Income      1    0.0059 23.304 -26.170
## &lt;none&gt;                    23.298 -24.182
## - Population  1    1.7599 25.058 -22.541
## + Area        1    0.0011 23.297 -22.185
## - Frost       1    2.0488 25.347 -21.968
## - HS.Grad     1    2.9804 26.279 -20.163
## - Murder      1   26.2721 49.570  11.569
## 
## Step:  AIC=-26.17
## Life.Exp ~ Population + Income + Murder + HS.Grad + Frost
## 
##              Df Sum of Sq    RSS     AIC
## - Income      1     0.006 23.308 -28.161
## &lt;none&gt;                    23.302 -26.174
## - Population  1     1.887 25.189 -24.280
## + Illiteracy  1     0.004 23.298 -24.182
## + Area        1     0.000 23.302 -24.174
## - Frost       1     3.037 26.339 -22.048
## - HS.Grad     1     3.495 26.797 -21.187
## - Murder      1    34.739 58.041  17.456
## 
## Step:  AIC=-28.16
## Life.Exp ~ Population + Murder + HS.Grad + Frost
## 
##              Df Sum of Sq    RSS     AIC
## &lt;none&gt;                    23.308 -28.161
## + Income      1     0.006 23.302 -26.174
## + Illiteracy  1     0.004 23.304 -26.170
## + Area        1     0.001 23.307 -26.163
## - Population  1     2.064 25.372 -25.920
## - Frost       1     3.122 26.430 -23.877
## - HS.Grad     1     5.112 28.420 -20.246
## - Murder      1    34.816 58.124  15.528</code></pre>
<pre><code>## 
## Call:
## lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, 
##     data = statedata)
## 
## Coefficients:
## (Intercept)   Population       Murder      HS.Grad        Frost  
##   7.103e+01    5.014e-05   -3.001e-01    4.658e-02   -5.943e-03</code></pre>
</div>
<div id="search-strategies" class="section level2 hasAnchor" number="14.17">
<h2><span class="header-section-number">14.17</span> Search Strategies<a href="variable-selection.html#search-strategies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An exhaustive search looks at all possible models using all available regressors.</p>
<ul>
<li>This is not feasible unless the number of regressors is relatively small.</li>
<li>If the number of regressors (including the intercept) is <span class="math inline">\(p\)</span>, there are <span class="math inline">\(2^p\)</span> possible models.</li>
</ul>
<p>Because of our error criteria, our search often simplifies to finding the model that minimizes <span class="math inline">\(\mbox{RSS}_{\mathcal{M}}\)</span> for each value of <span class="math inline">\(p_{\mathcal{M}}\)</span>. This is the best subset searching strategy.</p>
<div id="finding-the-best-subsets" class="section level3 hasAnchor" number="14.17.1">
<h3><span class="header-section-number">14.17.1</span> Finding the Best Subsets<a href="variable-selection.html#finding-the-best-subsets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>leaps</code> package performs a thorough search for the best subsets of predictors for each model size.</p>
<ul>
<li>Since the algorithm returns a best model for each size, the results do not depend on the a penalty model (such as AIC and BIC).</li>
<li>For each model size ,it finds the variables that give the minimum RSS.</li>
<li>By default, <code>regsubsets</code> only goes up to <span class="math inline">\(p=9\)</span>. You have to set <code>nvmax = j</code>, where <span class="math inline">\(j\)</span> is the number of regressors you want to consider.</li>
</ul>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="variable-selection.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># may need to install.package the first time</span></span>
<span id="cb28-2"><a href="variable-selection.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps) <span class="co"># you need to load package every time you want to use it</span></span>
<span id="cb28-3"><a href="variable-selection.html#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="variable-selection.html#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># model selection by best subset search</span></span>
<span id="cb28-5"><a href="variable-selection.html#cb28-5" aria-hidden="true" tabindex="-1"></a>best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Life.Exp <span class="sc">~</span> ., <span class="at">data =</span> statedata)</span>
<span id="cb28-6"><a href="variable-selection.html#cb28-6" aria-hidden="true" tabindex="-1"></a>bsum <span class="ot">&lt;-</span> <span class="fu">summary</span>(best) <span class="co"># summarize model that minimizes RSS for each p</span></span>
<span id="cb28-7"><a href="variable-selection.html#cb28-7" aria-hidden="true" tabindex="-1"></a>bsum<span class="sc">$</span>which <span class="co"># nicer output</span></span></code></pre></div>
<pre><code>##   (Intercept) Population Income Illiteracy Murder HS.Grad Frost  Area
## 1        TRUE      FALSE  FALSE      FALSE   TRUE   FALSE FALSE FALSE
## 2        TRUE      FALSE  FALSE      FALSE   TRUE    TRUE FALSE FALSE
## 3        TRUE      FALSE  FALSE      FALSE   TRUE    TRUE  TRUE FALSE
## 4        TRUE       TRUE  FALSE      FALSE   TRUE    TRUE  TRUE FALSE
## 5        TRUE       TRUE   TRUE      FALSE   TRUE    TRUE  TRUE FALSE
## 6        TRUE       TRUE   TRUE       TRUE   TRUE    TRUE  TRUE FALSE
## 7        TRUE       TRUE   TRUE       TRUE   TRUE    TRUE  TRUE  TRUE</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="variable-selection.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What output is stored after running regsubsets</span></span>
<span id="cb30-2"><a href="variable-selection.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(bsum)</span></code></pre></div>
<pre><code>##        Length Class      Mode     
## which  56     -none-     logical  
## rsq     7     -none-     numeric  
## rss     7     -none-     numeric  
## adjr2   7     -none-     numeric  
## cp      7     -none-     numeric  
## bic     7     -none-     numeric  
## outmat 49     -none-     character
## obj    28     regsubsets list</code></pre>
</div>
</div>
<div id="review-of-criterion-based-procedures-thus-far" class="section level2 hasAnchor" number="14.18">
<h2><span class="header-section-number">14.18</span> Review of Criterion-Based Procedures Thus Far<a href="variable-selection.html#review-of-criterion-based-procedures-thus-far" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>RSS (and <span class="math inline">\(R^2\)</span>) is a measurement of the error between the data and a model.</p></li>
<li><p>RSS will decrease when we add more predictors, regardless if they predict anything.</p></li>
<li><p>Therefore <span class="math inline">\(R^2 = 1 - \mbox{RSS}/\mbox{TSS}\)</span> increases, regardless.</p></li>
<li><p><span class="math inline">\(p\)</span>-values should not be taken as very accurate in stepwise or best subset searches because we’ll see small <span class="math inline">\(p\)</span>-values due to chance alone.</p></li>
<li><p>So we shouldn’t just consider RSS or <span class="math inline">\(R^2\)</span>since we’ll always choose the most complicated model.</p></li>
<li><p>We shouldn’t just consider <span class="math inline">\(p\)</span>-values since we will always get false positives.</p></li>
<li><p>We only want to add predictors if they significantly help improve the prediction.</p></li>
</ul>
</div>
<div id="akaikes-information-criterion-aic-and-bayesian-information-criteria-bic" class="section level2 hasAnchor" number="14.19">
<h2><span class="header-section-number">14.19</span> Akaike’s Information Criterion (AIC) and Bayesian Information Criteria (BIC)<a href="variable-selection.html#akaikes-information-criterion-aic-and-bayesian-information-criteria-bic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><span class="math display">\[\mbox{AIC}(\mathcal{M})= n\log{(\mbox{RSS}_{\mathcal{M}}/n)} +2p_{\mathcal{M}} +c.\]</span>
<span class="math display">\[\mbox{BIC}(\mathcal{M})= n\log{(\mbox{RSS}_{\mathcal{M}}/n)} + \log{(n)} p_{\mathcal{M}} +c.\]</span></p>
<ul>
<li>Both AIC and BIC are criteria that balance fit and complexity.</li>
<li>As <span class="math inline">\(RSS\)</span> goes down (yay!), AIC and BIC goes down.</li>
<li>As <span class="math inline">\(p_{\mathcal{M}}\)</span> goes up, there is a penalty for making things more complicated.</li>
<li>BIC assigns a bigger penalty for adding more predictors, so it will slightly favor simple models to complex models (compared to AIC).</li>
<li>The constant <span class="math inline">\(c\)</span> is the same for all models created from the same data, so it can be ignored.</li>
<li><strong>We choose the model the minimizes the AIC and/or BIC</strong>.</li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="variable-selection.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Storing values we&#39;ll use</span></span>
<span id="cb32-2"><a href="variable-selection.html#cb32-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">8</span> <span class="co"># number of predictors (including intercept)</span></span>
<span id="cb32-3"><a href="variable-selection.html#cb32-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(statedata) <span class="co"># n=50 observations</span></span>
<span id="cb32-4"><a href="variable-selection.html#cb32-4" aria-hidden="true" tabindex="-1"></a>rss <span class="ot">&lt;-</span> bsum<span class="sc">$</span>rss <span class="co"># rss of each best subset</span></span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="variable-selection.html#cb33-1" aria-hidden="true" tabindex="-1"></a>BIC <span class="ot">&lt;-</span> bsum<span class="sc">$</span>bic <span class="co"># Exactly values from rs summary</span></span>
<span id="cb33-2"><a href="variable-selection.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BIC <span class="sc">~</span> p, <span class="at">ylab =</span> <span class="st">&quot;BIC&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors (incl intercept)&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<ul>
<li>The <code>car</code> package has a <code>subsets</code> function that takes the generates nice, labeled BIC (or other statistics, not AIC though) plots generated from the <code>regsubsets</code> function.</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="variable-selection.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb34-2"><a href="variable-selection.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;bic&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>) <span class="co"># stat can be “bic”, “cp”, “adjr2”, “rsq”, “rss”</span></span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre><code>##            Abbreviation
## Population            P
## Income               In
## Illiteracy           Il
## Murder                M
## HS.Grad               H
## Frost                 F
## Area                  A</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="variable-selection.html#cb36-1" aria-hidden="true" tabindex="-1"></a>AIC <span class="ot">&lt;-</span> BIC <span class="sc">+</span> p <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">-</span> <span class="fu">log</span>(n)) <span class="co"># Compute AIC from BIC</span></span>
<span id="cb36-2"><a href="variable-selection.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(AIC <span class="sc">~</span> p, <span class="at">ylab =</span> <span class="st">&quot;BIC&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors (incl intercept)&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div id="optional-if-you-want-to-compare-with-formulas" class="section level3 hasAnchor" number="14.19.1">
<h3><span class="header-section-number">14.19.1</span> Optional if You Want to Compare With Formulas<a href="variable-selection.html#optional-if-you-want-to-compare-with-formulas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="variable-selection.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes BIC from the formula (ignoring the constant c)</span></span>
<span id="cb37-2"><a href="variable-selection.html#cb37-2" aria-hidden="true" tabindex="-1"></a>BIC2 <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">log</span>(rss<span class="sc">/</span>n) <span class="sc">+</span> <span class="fu">log</span>(n) <span class="sc">*</span> p  <span class="co"># include the intercept when giving p</span></span>
<span id="cb37-3"><a href="variable-selection.html#cb37-3" aria-hidden="true" tabindex="-1"></a>BIC <span class="sc">-</span> BIC2  <span class="co"># This tells you what the constant c is.</span></span></code></pre></div>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="variable-selection.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes AIC from the formula (ignoring the constant c)</span></span>
<span id="cb38-2"><a href="variable-selection.html#cb38-2" aria-hidden="true" tabindex="-1"></a>AIC2 <span class="ot">&lt;-</span> n <span class="sc">*</span> <span class="fu">log</span>(rss<span class="sc">/</span>n) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> p  <span class="co"># include the intercept when giving p</span></span>
<span id="cb38-3"><a href="variable-selection.html#cb38-3" aria-hidden="true" tabindex="-1"></a>AIC <span class="sc">-</span> AIC2  <span class="co"># This tells you what the constant c is.</span></span></code></pre></div>
</div>
</div>
<div id="adjusted-r2" class="section level2 hasAnchor" number="14.20">
<h2><span class="header-section-number">14.20</span> Adjusted <span class="math inline">\(R^2\)</span><a href="variable-selection.html#adjusted-r2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>adjusted <span class="math inline">\(R^2\)</span></strong> is another criterion that penalizes for the number of parameters in the model. Adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R_a^2\)</span>** is a better criterion for assessing model fit than <span class="math inline">\(R^2\)</span>.</p>
<p>For model <span class="math inline">\(\mathcal{M}\)</span> with <span class="math inline">\(p_{\mathcal{M}}\)</span> regression coefficients,</p>
<p><span class="math display">\[R_a^2=1- \frac{\mbox{RSS}_\mathcal{M}/(n-p_\mathcal{M})}{\mbox{TSS}/(n-1)} = 1 - \left(\frac{n-1}{n-p_{\mathcal{M}}} \right) \left( 1-R^2 \right) = 1 - \frac{\hat{\sigma}^2_{\mathcal{M}}}{\hat{\sigma}^2_{\rm null}}.\]</span></p>
<p><strong>Adding a regressor to a model only increases <span class="math inline">\(R_a^2\)</span> if the regressor has some predictive value.</strong></p>
<ul>
<li><p>Minimizing the variance of the prediction error amounts to minimizing <span class="math inline">\(\hat{\sigma}^2_{\mathcal{M}}\)</span>.</p></li>
<li><p>The smaller that <span class="math inline">\(\hat{\sigma}^2_{\mathcal{M}}\)</span> becomes the larger <span class="math inline">\(R^2_a\)</span> becomes.</p></li>
<li><p><strong>We favor models that produce larger <span class="math inline">\(R_a^2\)</span>.</strong></p>
<h3 id="computing-the-adjusted-r2" class="hasAnchor">Computing the Adjusted <span class="math inline">\(R^2\)</span><a href="variable-selection.html#computing-the-adjusted-r2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="variable-selection.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co">#faraway::sumary(lmod) #gives R^2 for full model</span></span>
<span id="cb39-2"><a href="variable-selection.html#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(lmod) # both R^2 and R_a^2 for full model</span></span>
<span id="cb39-3"><a href="variable-selection.html#cb39-3" aria-hidden="true" tabindex="-1"></a>(adjr <span class="ot">&lt;-</span> bsum<span class="sc">$</span>adjr) <span class="co">#pulls R_a^2 from regsubsets for each subset</span></span></code></pre></div>
<pre><code>## [1] 0.6015893 0.6484991 0.6939230 0.7125690 0.7061129 0.6993268 0.6921823</code></pre></li>
</ul>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="variable-selection.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(adjr <span class="sc">~</span> p, <span class="at">ylab =</span> <span class="fu">expression</span>({R<span class="sc">^</span><span class="dv">2</span>}[a]),</span>
<span id="cb41-2"><a href="variable-selection.html#cb41-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="variable-selection.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;adjr2&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>##            Abbreviation
## Population            P
## Income               In
## Illiteracy           Il
## Murder                M
## HS.Grad               H
## Frost                 F
## Area                  A</code></pre>
</div>
<div id="mean-square-error-mse" class="section level2 hasAnchor" number="14.21">
<h2><span class="header-section-number">14.21</span> Mean Square Error (MSE)<a href="variable-selection.html#mean-square-error-mse" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>Mean Square Error (MSE)</strong> of an estimator measures the average squared distance between the estimator and the parameter:</p>
<p><span class="math display">\[\mbox{MSE} (\hat{\theta})  = E \left( (\hat{\theta} - \theta)^2 \right) = \mbox{Var} (\hat{\theta}) + \left( \mbox{Bias}(\hat{\theta})\right)^2\]</span></p>
<ul>
<li>MSE is a criterion the combines bias and efficiency.</li>
<li>If two estimators are unbiased, one is more efficient than the other if and only if it has a smaller MSE.</li>
<li>We favor models with smaller mean squared error, but the search algorithm is very important, otherwise you just use the model with the most regressors.</li>
</ul>
</div>
<div id="mallows-c_p-statistic" class="section level2 hasAnchor" number="14.22">
<h2><span class="header-section-number">14.22</span> Mallow’s <span class="math inline">\(C_p\)</span> Statistic<a href="variable-selection.html#mallows-c_p-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Mallow’s <span class="math inline">\(C_p\)</span> statistic is a criterion designed to quantify the predictive usefulness of a model. Mallow’s <span class="math inline">\(C_p\)</span> statistic is used to estimate the average <strong>mean square error</strong> of the prediction,</p>
<p><span class="math display">\[ \frac{1}{\sigma^2} \sum_i MSE(\hat{y}_i) =  \frac{1}{\sigma^2} \sum_iE\big( (\hat{y}_i - E(y_i))^2 \big)\]</span></p>
<p>The average of the mean square errors can be approximated by Mallow’s <span class="math inline">\(C_p\)</span> Statistic:</p>
<p><span class="math display">\[C_{p_{\mathcal{M}}} = \frac{\mbox{RSS}_{\mathcal{M}}}{\hat{\sigma}^2} + 2p_{\mathcal{M}} - n\]</span>.</p>
<ul>
<li>For the model with all regressors (model <span class="math inline">\(\Omega\)</span> with <span class="math inline">\(p_{\Omega}\)</span> regression coefficients), we have <span class="math inline">\(C_{p_{\Omega}}=p_{\Omega}\)</span></li>
<li>If a model with <span class="math inline">\(p_{\mathcal{M}}\)</span> regression coefficients fits the data well and has little or no bias, then <span class="math inline">\(E(C_{p_{\mathcal{M}}}) \approx p_{\mathcal{M}}\)</span>.
<ul>
<li>A model with a biased fit will have <span class="math inline">\(C_{p_{\mathcal{M}}}\)</span> much larger than <span class="math inline">\(p_{\mathcal{M}}\)</span>.</li>
<li>Models with <span class="math inline">\(C_{p_{\mathcal{M}}}\)</span> less than <span class="math inline">\(p_{\mathcal{M}}\)</span> do not show evidence of bias.</li>
</ul></li>
<li>It is common to plot <span class="math inline">\(C_{p_{\mathcal{M}}}\)</span> versus <span class="math inline">\(p_{\mathcal{M}}\)</span> and compare this to <span class="math inline">\(45^{\circ}\)</span> line <span class="math inline">\(C_{p_{\mathcal{M}}}= p_{\mathcal{M}}\)</span> .</li>
<li><strong>We favor models with small <span class="math inline">\(p_{\mathcal{M}}\)</span> and <span class="math inline">\(C_{p_{\mathcal{M}}}\)</span> close to <span class="math inline">\(p_\mathcal{M}\)</span>.</strong></li>
</ul>
<div id="computing-mallows-c_p-statistic" class="section level3 hasAnchor" number="14.22.1">
<h3><span class="header-section-number">14.22.1</span> Computing Mallow’s <span class="math inline">\(C_p\)</span> Statistic<a href="variable-selection.html#computing-mallows-c_p-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="variable-selection.html#cb44-1" aria-hidden="true" tabindex="-1"></a>cp <span class="ot">&lt;-</span> bsum<span class="sc">$</span>cp <span class="co"># Display the C_p for each value of p</span></span>
<span id="cb44-2"><a href="variable-selection.html#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cp <span class="sc">~</span> p, <span class="at">ylab =</span> <span class="fu">expression</span>({C_p}),</span>
<span id="cb44-3"><a href="variable-selection.html#cb44-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb44-4"><a href="variable-selection.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>) <span class="co"># plots line y=x</span></span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="variable-selection.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;cp&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>##            Abbreviation
## Population            P
## Income               In
## Illiteracy           Il
## Murder                M
## HS.Grad               H
## Frost                 F
## Area                  A</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="variable-selection.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="question-7-interpret-the-output-from-the-c_p-plot-above.-what-is-the-best-model-according-to-this-metric" class="section level3 hasAnchor" number="14.22.2">
<h3><span class="header-section-number">14.22.2</span> Question 7: Interpret the output from the <span class="math inline">\(C_p\)</span> plot above. What is the best model according to this metric?<a href="variable-selection.html#question-7-interpret-the-output-from-the-c_p-plot-above.-what-is-the-best-model-according-to-this-metric" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Four predictors (including the intercept) seems about right.</li>
<li>Five predictors could be a suitable choice too.</li>
</ul>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="variable-selection.html#cb48-1" aria-hidden="true" tabindex="-1"></a>bmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> Population <span class="sc">+</span> Murder <span class="sc">+</span> HS.Grad <span class="sc">+</span> Frost, <span class="at">data =</span> statedata)</span></code></pre></div>
</div>
<div id="root-mean-squared-error" class="section level3 hasAnchor" number="14.22.3">
<h3><span class="header-section-number">14.22.3</span> Root Mean Squared Error<a href="variable-selection.html#root-mean-squared-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>RMSE (root mean squared error)</strong> is simply the square root of the MSE, and is sometimes used in place of the MSE.</p>
<ul>
<li>The RMSE or MSE will produce identical variable selection results since they are 1-1 transformations of each other.</li>
</ul>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="14.23">
<h2><span class="header-section-number">14.23</span> Cross-validation<a href="variable-selection.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous example, we can pat ourselves on the back and say we removed four predictors and that causes only a minor reduction in fit. Well done, but a better question might be: <strong>what would the effect of removing these variables be on a new independent sample?</strong></p>
<ul>
<li><p>Well, we just used all of our sample data to construct this model.</p></li>
<li><p>We need to see how well our data does with new data (not used in construction of the model).</p></li>
<li><p>How can we see how good our model works?</p>
<p><strong>Cross-validation</strong> breaks the data into a <strong>training dataset</strong> and a <strong>test dataset</strong> to get a more accurate assessment of the predictive accuracy of a model.</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>A model is fit to the training dataset.</li>
<li>The fitted model is used to predict the responses of the test dataset.</li>
<li>An error criterion (e.g, the MSE) is calculated for the test dataset.</li>
</ol>
<p><strong>When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE (or RMSE).</strong></p>
<p>## Methods For Splitting the Data</p>
<p>There are many variations of how to choose the training and testing datasets for crossvalidation.</p>
<div id="leave-one-out-crossvalidation" class="section level3 hasAnchor" number="14.23.1">
<h3><span class="header-section-number">14.23.1</span> Leave-One Out Crossvalidation<a href="variable-selection.html#leave-one-out-crossvalidation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Leave-one-out crossvalidation</strong> uses each observation (individually) as a test data set, using the other <span class="math inline">\(n-1\)</span> observations as the training data.</p>
<div id="should-we-inlcude-population" class="section level4 hasAnchor" number="14.23.1.1">
<h4><span class="header-section-number">14.23.1.1</span> Should We Inlcude Population?<a href="variable-selection.html#should-we-inlcude-population" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="variable-selection.html#cb49-1" aria-hidden="true" tabindex="-1"></a>ersq <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb49-2"><a href="variable-selection.html#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="variable-selection.html#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n){</span>
<span id="cb49-4"><a href="variable-selection.html#cb49-4" aria-hidden="true" tabindex="-1"></a>  train.ds <span class="ot">&lt;-</span> statedata[<span class="sc">-</span>i, ]</span>
<span id="cb49-5"><a href="variable-selection.html#cb49-5" aria-hidden="true" tabindex="-1"></a>  test.ds <span class="ot">&lt;-</span> statedata[i, ]</span>
<span id="cb49-6"><a href="variable-selection.html#cb49-6" aria-hidden="true" tabindex="-1"></a>  tmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> Population <span class="sc">+</span> Murder <span class="sc">+</span> HS.Grad <span class="sc">+</span> Frost, <span class="at">data =</span> train.ds)</span>
<span id="cb49-7"><a href="variable-selection.html#cb49-7" aria-hidden="true" tabindex="-1"></a>  predy <span class="ot">&lt;-</span> <span class="fu">predict</span>(tmod, <span class="at">new =</span> test.ds)</span>
<span id="cb49-8"><a href="variable-selection.html#cb49-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> test.ds[<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb49-9"><a href="variable-selection.html#cb49-9" aria-hidden="true" tabindex="-1"></a>  ersq[i] <span class="ot">&lt;-</span> (predy <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb49-10"><a href="variable-selection.html#cb49-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb49-11"><a href="variable-selection.html#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="variable-selection.html#cb49-12" aria-hidden="true" tabindex="-1"></a>(tmse <span class="ot">&lt;-</span> <span class="fu">sum</span>(ersq))</span></code></pre></div>
<pre><code>## [1] 29.64823</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="variable-selection.html#cb51-1" aria-hidden="true" tabindex="-1"></a>ersq <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb51-2"><a href="variable-selection.html#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="variable-selection.html#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n){</span>
<span id="cb51-4"><a href="variable-selection.html#cb51-4" aria-hidden="true" tabindex="-1"></a>  train.ds <span class="ot">&lt;-</span> statedata[<span class="sc">-</span>i, ]</span>
<span id="cb51-5"><a href="variable-selection.html#cb51-5" aria-hidden="true" tabindex="-1"></a>  test.ds <span class="ot">&lt;-</span> statedata[i, ]</span>
<span id="cb51-6"><a href="variable-selection.html#cb51-6" aria-hidden="true" tabindex="-1"></a>  tmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(Life.Exp <span class="sc">~</span> Murder <span class="sc">+</span> HS.Grad <span class="sc">+</span> Frost, <span class="at">data =</span> train.ds)</span>
<span id="cb51-7"><a href="variable-selection.html#cb51-7" aria-hidden="true" tabindex="-1"></a>  predy <span class="ot">&lt;-</span> <span class="fu">predict</span>(tmod, <span class="at">new =</span> test.ds)</span>
<span id="cb51-8"><a href="variable-selection.html#cb51-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> test.ds[<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb51-9"><a href="variable-selection.html#cb51-9" aria-hidden="true" tabindex="-1"></a>  ersq[i] <span class="ot">&lt;-</span> (predy <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb51-10"><a href="variable-selection.html#cb51-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb51-11"><a href="variable-selection.html#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="variable-selection.html#cb51-12" aria-hidden="true" tabindex="-1"></a>(tmse <span class="ot">&lt;-</span> <span class="fu">sum</span>(ersq))</span></code></pre></div>
<pre><code>## [1] 31.19675</code></pre>
</div>
</div>
<div id="k-fold-crossvalidation" class="section level3 hasAnchor" number="14.23.2">
<h3><span class="header-section-number">14.23.2</span> <span class="math inline">\(k\)</span>-Fold Crossvalidation<a href="variable-selection.html#k-fold-crossvalidation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong><span class="math inline">\(k\)</span>-fold crossvalidation</strong> breaks the data into <span class="math inline">\(k\)</span> unique sets.</p>
<ul>
<li>For each set, the other <span class="math inline">\(k-1\)</span> sets are used as training data, and then the fitted model is used to predict the responses for the <span class="math inline">\(k\)</span>th testing set.</li>
<li>We must fit <span class="math inline">\(k\)</span> models to determine the mean squared error.</li>
</ul>
</div>
</div>
<div id="example-of-k-fold-crossvalidation" class="section level2 hasAnchor" number="14.24">
<h2><span class="header-section-number">14.24</span> Example of <span class="math inline">\(k\)</span>-fold Crossvalidation<a href="variable-selection.html#example-of-k-fold-crossvalidation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s comparison the full model to model with <code>Population</code>, <code>Murder</code>, <code>HS.Grad</code>, and <code>Frost</code> predictors using the RMSE criterion and both 10-fold crossvalidation and leave-one-out crossvalidation.</p>
<p>The <code>caret</code> package (short for Classification And REgression Training) contains functions to streamline the model training process for regression and classification problems.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="variable-selection.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="variable-selection.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define training/test (control) data</span></span>
<span id="cb56-2"><a href="variable-selection.html#cb56-2" aria-hidden="true" tabindex="-1"></a>cv_10fold <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>) <span class="co"># 10-fold crossvalidation train/test data</span></span>
<span id="cb56-3"><a href="variable-selection.html#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="variable-selection.html#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up fill and model with our 4 regressors</span></span>
<span id="cb56-5"><a href="variable-selection.html#cb56-5" aria-hidden="true" tabindex="-1"></a>f1 <span class="ot">=</span> Life.Exp <span class="sc">~</span> . <span class="co"># formula for full model</span></span>
<span id="cb56-6"><a href="variable-selection.html#cb56-6" aria-hidden="true" tabindex="-1"></a>f2 <span class="ot">=</span> Life.Exp<span class="sc">~</span>Population <span class="sc">+</span> Murder <span class="sc">+</span> HS.Grad <span class="sc">+</span> Frost</span>
<span id="cb56-7"><a href="variable-selection.html#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="variable-selection.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Using training data to construct each model</span></span>
<span id="cb56-9"><a href="variable-selection.html#cb56-9" aria-hidden="true" tabindex="-1"></a>modela <span class="ot">&lt;-</span> <span class="fu">train</span>(f1, <span class="at">data =</span> statedata, <span class="at">trControl=</span>cv_10fold, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="co">#full</span></span>
<span id="cb56-10"><a href="variable-selection.html#cb56-10" aria-hidden="true" tabindex="-1"></a>modelb <span class="ot">&lt;-</span> <span class="fu">train</span>(f2, <span class="at">data =</span> statedata, <span class="at">trControl=</span>cv_10fold, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="co">#with 4 reg</span></span>
<span id="cb56-11"><a href="variable-selection.html#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(modela) <span class="co"># full, 10-fold</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 50 samples
##  7 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 46, 45, 45, 46, 45, 45, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.8244699  0.6377933  0.6876161
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="variable-selection.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(modelb) <span class="co"># reduced, 10-fold</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 50 samples
##  4 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 45, 43, 45, 45, 43, 46, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.7468532  0.6645374  0.6660142
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="variable-selection.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># leave-one-out crossvalidation train/test data</span></span>
<span id="cb60-2"><a href="variable-selection.html#cb60-2" aria-hidden="true" tabindex="-1"></a>cv_loo <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;LOOCV&quot;</span>)</span>
<span id="cb60-3"><a href="variable-selection.html#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="variable-selection.html#cb60-4" aria-hidden="true" tabindex="-1"></a>modelfull <span class="ot">&lt;-</span> <span class="fu">train</span>(f1, <span class="at">data =</span> statedata, <span class="at">trControl=</span>cv_loo, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="co">#full</span></span>
<span id="cb60-5"><a href="variable-selection.html#cb60-5" aria-hidden="true" tabindex="-1"></a>modelred <span class="ot">&lt;-</span> <span class="fu">train</span>(f2, <span class="at">data =</span> statedata, <span class="at">trControl=</span>cv_loo, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="co">#with 4 reg</span></span>
<span id="cb60-6"><a href="variable-selection.html#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(modelfull) <span class="co"># full, leave one out</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 50 samples
##  7 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 49, 49, 49, 49, 49, 49, ... 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.9090885  0.5469535  0.7196334
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="variable-selection.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(modelred) <span class="co"># reduced, leave one out</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 50 samples
##  4 predictor
## 
## No pre-processing
## Resampling: Leave-One-Out Cross-Validation 
## Summary of sample sizes: 49, 49, 49, 49, 49, 49, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE      
##   0.770042  0.6657615  0.6377097
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
</div>
<div id="theres-still-more-to-consider" class="section level2 hasAnchor" number="14.25">
<h2><span class="header-section-number">14.25</span> There’s Still More to Consider!<a href="variable-selection.html#theres-still-more-to-consider" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="variable-selection.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(car) #needed but we already loaded</span></span>
<span id="cb64-2"><a href="variable-selection.html#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">influencePlot</span>(lmod)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<pre><code>##       StudRes       Hat       CookD
## AK -1.6061632 0.8095223 1.320803928
## CA -0.1590567 0.4088569 0.002239186
## HI  2.7352416 0.3787617 0.493948906
## ME -2.2322062 0.1218190 0.078915835</code></pre>
<p>Let’s remove Alaska since it is a high leverage point. Then identify best subset using <span class="math inline">\(R^2_a\)</span> as our search criterion.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="variable-selection.html#cb66-1" aria-hidden="true" tabindex="-1"></a>best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Life.Exp <span class="sc">~</span>., <span class="at">data =</span> statedata, <span class="at">subset =</span> (state.abb <span class="sc">!=</span> <span class="st">&quot;AK&quot;</span>))</span>
<span id="cb66-2"><a href="variable-selection.html#cb66-2" aria-hidden="true" tabindex="-1"></a>bsum <span class="ot">&lt;-</span> <span class="fu">summary</span>(best)</span>
<span id="cb66-3"><a href="variable-selection.html#cb66-3" aria-hidden="true" tabindex="-1"></a>bsum<span class="sc">$</span>which[<span class="fu">which.max</span>(bsum<span class="sc">$</span>adjr), ]</span></code></pre></div>
<pre><code>## (Intercept)  Population      Income  Illiteracy      Murder     HS.Grad 
##        TRUE        TRUE       FALSE       FALSE        TRUE        TRUE 
##       Frost        Area 
##        TRUE        TRUE</code></pre>
</div>
<div id="summary" class="section level2 hasAnchor" number="14.26">
<h2><span class="header-section-number">14.26</span> Summary<a href="variable-selection.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are other mechanisms for choosing the training and test datasets, but these are the most common.</p>
<ul>
<li><strong>When using cross-validation as your selection criterion, we prefer the model that produces the lowest MSE or RMSE.</strong></li>
<li>You typically don’t do an exhaustive search or stepwise selection search.</li>
<li>You often use one of the other selection criteria/search strategies to narrow down the possible models to a few final candidate models and then use cross-validation to make a final decision.</li>
<li>Iteration and experimentation are essential to finding better models BUT be very careful not to overtrain your model to the sample data!</li>
</ul>
</div>
<div id="exercise" class="section level2 hasAnchor" number="14.27">
<h2><span class="header-section-number">14.27</span> Exercise<a href="variable-selection.html#exercise" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For the teengamb data in the faraway package, use the methods learned in this chapter to identify the “best” models.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="variable-selection.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(faraway)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;faraway&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:lattice&#39;:
## 
##     melanoma</code></pre>
<pre><code>## The following objects are masked from &#39;package:car&#39;:
## 
##     logit, vif</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="variable-selection.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(teengamb) <span class="co"># load data</span></span>
<span id="cb72-2"><a href="variable-selection.html#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(teengamb)</span></code></pre></div>
<pre><code>##       sex             status          income           verbal     
##  Min.   :0.0000   Min.   :18.00   Min.   : 0.600   Min.   : 1.00  
##  1st Qu.:0.0000   1st Qu.:28.00   1st Qu.: 2.000   1st Qu.: 6.00  
##  Median :0.0000   Median :43.00   Median : 3.250   Median : 7.00  
##  Mean   :0.4043   Mean   :45.23   Mean   : 4.642   Mean   : 6.66  
##  3rd Qu.:1.0000   3rd Qu.:61.50   3rd Qu.: 6.210   3rd Qu.: 8.00  
##  Max.   :1.0000   Max.   :75.00   Max.   :15.000   Max.   :10.00  
##      gamble     
##  Min.   :  0.0  
##  1st Qu.:  1.1  
##  Median :  6.0  
##  Mean   : 19.3  
##  3rd Qu.: 19.4  
##  Max.   :156.0</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="variable-selection.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit full model</span></span>
<span id="cb74-2"><a href="variable-selection.html#cb74-2" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(gamble <span class="sc">~</span> ., <span class="at">data =</span> teengamb)</span>
<span id="cb74-3"><a href="variable-selection.html#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sumary</span>(lmod) <span class="co"># determine least significant predictor</span></span></code></pre></div>
<pre><code>##               Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)  22.555651  17.196803  1.3116   0.19677
## sex         -22.118330   8.211115 -2.6937   0.01011
## status        0.052234   0.281112  0.1858   0.85349
## income        4.961979   1.025392  4.8391 1.792e-05
## verbal       -2.959493   2.172150 -1.3625   0.18031
## 
## n = 47, p = 5, Residual SE = 22.69034, R-Squared = 0.53</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="variable-selection.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Values we&#39;ll need</span></span>
<span id="cb76-2"><a href="variable-selection.html#cb76-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nobs</span>(lmod)</span>
<span id="cb76-3"><a href="variable-selection.html#cb76-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">5</span></span></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="variable-selection.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># perform backward elimination using update function on previous model</span></span>
<span id="cb77-2"><a href="variable-selection.html#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use alpha_crit = 0.05</span></span>
<span id="cb77-3"><a href="variable-selection.html#cb77-3" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">update</span>(lmod, . <span class="sc">~</span> . <span class="sc">-</span> status)</span>
<span id="cb77-4"><a href="variable-selection.html#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sumary</span>(lmod)</span></code></pre></div>
<pre><code>##              Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)  24.13897   14.76859  1.6345  0.109459
## sex         -22.96022    6.77057 -3.3912  0.001502
## income        4.89809    0.95512  5.1283 6.644e-06
## verbal       -2.74682    1.82528 -1.5049  0.139667
## 
## n = 47, p = 4, Residual SE = 22.43416, R-Squared = 0.53</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="variable-selection.html#cb79-1" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">update</span>(lmod, . <span class="sc">~</span> . <span class="sc">-</span> verbal)</span>
<span id="cb79-2"><a href="variable-selection.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sumary</span>(lmod)</span></code></pre></div>
<pre><code>##              Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)   4.04083    6.39435  0.6319  0.530698
## sex         -21.63439    6.80880 -3.1774  0.002717
## income        5.17158    0.95105  5.4378 2.245e-06
## 
## n = 47, p = 3, Residual SE = 22.75428, R-Squared = 0.5</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="variable-selection.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model selection in terms of AIC</span></span>
<span id="cb81-2"><a href="variable-selection.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb81-3"><a href="variable-selection.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="co"># model selection by exhaustive search</span></span>
<span id="cb81-4"><a href="variable-selection.html#cb81-4" aria-hidden="true" tabindex="-1"></a>best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(gamble <span class="sc">~</span> ., <span class="at">data =</span> teengamb)</span>
<span id="cb81-5"><a href="variable-selection.html#cb81-5" aria-hidden="true" tabindex="-1"></a>bsum <span class="ot">&lt;-</span> <span class="fu">summary</span>(best) <span class="co"># summarize model that minimizes RSS for each p</span></span>
<span id="cb81-6"><a href="variable-selection.html#cb81-6" aria-hidden="true" tabindex="-1"></a>bsum<span class="sc">$</span>which <span class="co"># best subset models (in terms of RSS)</span></span></code></pre></div>
<pre><code>##   (Intercept)   sex status income verbal
## 1        TRUE FALSE  FALSE   TRUE  FALSE
## 2        TRUE  TRUE  FALSE   TRUE  FALSE
## 3        TRUE  TRUE  FALSE   TRUE   TRUE
## 4        TRUE  TRUE   TRUE   TRUE   TRUE</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="variable-selection.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate AIC of each model</span></span>
<span id="cb83-2"><a href="variable-selection.html#cb83-2" aria-hidden="true" tabindex="-1"></a>aic <span class="ot">&lt;-</span> bsum<span class="sc">$</span>bic <span class="sc">+</span> (<span class="dv">2</span> <span class="sc">-</span> <span class="fu">log</span>(n)) <span class="sc">*</span> p</span>
<span id="cb83-3"><a href="variable-selection.html#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot AIC vs p</span></span>
<span id="cb83-4"><a href="variable-selection.html#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(aic <span class="sc">~</span> p, <span class="at">xlab =</span> <span class="st">&quot;Number of Predictors&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;AIC&quot;</span>, <span class="at">pch =</span><span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="variable-selection.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate BIC of each model</span></span>
<span id="cb84-2"><a href="variable-selection.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="co"># plot BIC vs p</span></span>
<span id="cb84-3"><a href="variable-selection.html#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb84-4"><a href="variable-selection.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;bic&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<pre><code>##        Abbreviation
## sex              sx
## status           st
## income            i
## verbal            v</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="variable-selection.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct Cp plot</span></span>
<span id="cb86-2"><a href="variable-selection.html#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;cp&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>##        Abbreviation
## sex              sx
## status           st
## income            i
## verbal            v</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="variable-selection.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="variable-selection.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Construct adjusted R^2 plot</span></span>
<span id="cb89-2"><a href="variable-selection.html#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="fu">subsets</span>(best, <span class="at">statistic =</span> <span class="st">&quot;adjr2&quot;</span>, <span class="at">legend =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
<pre><code>##        Abbreviation
## sex              sx
## status           st
## income            i
## verbal            v</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="variable-selection.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># backward elimination</span></span>
<span id="cb91-2"><a href="variable-selection.html#cb91-2" aria-hidden="true" tabindex="-1"></a>lmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(gamble <span class="sc">~</span> ., <span class="at">data =</span> teengamb)</span>
<span id="cb91-3"><a href="variable-selection.html#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="fu">step</span>(lmod) <span class="co">#with AIC</span></span></code></pre></div>
<pre><code>## Start:  AIC=298.18
## gamble ~ sex + status + income + verbal
## 
##          Df Sum of Sq   RSS    AIC
## - status  1      17.8 21642 296.21
## &lt;none&gt;                21624 298.18
## - verbal  1     955.7 22580 298.21
## - sex     1    3735.8 25360 303.67
## - income  1   12056.2 33680 317.00
## 
## Step:  AIC=296.21
## gamble ~ sex + income + verbal
## 
##          Df Sum of Sq   RSS    AIC
## &lt;none&gt;                21642 296.21
## - verbal  1    1139.8 22781 296.63
## - sex     1    5787.9 27429 305.35
## - income  1   13236.1 34878 316.64</code></pre>
<pre><code>## 
## Call:
## lm(formula = gamble ~ sex + income + verbal, data = teengamb)
## 
## Coefficients:
## (Intercept)          sex       income       verbal  
##      24.139      -22.960        4.898       -2.747</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="variable-selection.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">step</span>(lmod, <span class="at">k =</span> <span class="fu">log</span>(n)) <span class="co"># with BIC</span></span></code></pre></div>
<pre><code>## Start:  AIC=307.43
## gamble ~ sex + status + income + verbal
## 
##          Df Sum of Sq   RSS    AIC
## - status  1      17.8 21642 303.62
## - verbal  1     955.7 22580 305.61
## &lt;none&gt;                21624 307.43
## - sex     1    3735.8 25360 311.07
## - income  1   12056.2 33680 324.40
## 
## Step:  AIC=303.62
## gamble ~ sex + income + verbal
## 
##          Df Sum of Sq   RSS    AIC
## - verbal  1    1139.8 22781 302.18
## &lt;none&gt;                21642 303.62
## - sex     1    5787.9 27429 310.90
## - income  1   13236.1 34878 322.19
## 
## Step:  AIC=302.18
## gamble ~ sex + income
## 
##          Df Sum of Sq   RSS    AIC
## &lt;none&gt;                22781 302.18
## - sex     1    5227.3 28009 308.04
## - income  1   15309.8 38091 322.49</code></pre>
<pre><code>## 
## Call:
## lm(formula = gamble ~ sex + income, data = teengamb)
## 
## Coefficients:
## (Intercept)          sex       income  
##       4.041      -21.634        5.172</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="variable-selection.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb97-2"><a href="variable-selection.html#cb97-2" aria-hidden="true" tabindex="-1"></a>f1 <span class="ot">=</span> gamble <span class="sc">~</span> sex <span class="sc">+</span> income</span>
<span id="cb97-3"><a href="variable-selection.html#cb97-3" aria-hidden="true" tabindex="-1"></a>f2 <span class="ot">=</span> gamble <span class="sc">~</span> sex <span class="sc">+</span> verbal <span class="sc">+</span> income</span>
<span id="cb97-4"><a href="variable-selection.html#cb97-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-5"><a href="variable-selection.html#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 5-fold crossvalidation train/test data</span></span>
<span id="cb97-6"><a href="variable-selection.html#cb97-6" aria-hidden="true" tabindex="-1"></a>cv_5fold <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb97-7"><a href="variable-selection.html#cb97-7" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">train</span>(f1, <span class="at">data =</span> teengamb, <span class="at">trControl =</span> cv_5fold,</span>
<span id="cb97-8"><a href="variable-selection.html#cb97-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb97-9"><a href="variable-selection.html#cb97-9" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">train</span>(f2, <span class="at">data =</span> teengamb, <span class="at">trControl =</span> cv_5fold,</span>
<span id="cb97-10"><a href="variable-selection.html#cb97-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb97-11"><a href="variable-selection.html#cb97-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-12"><a href="variable-selection.html#cb97-12" aria-hidden="true" tabindex="-1"></a><span class="co"># compare mse (rmse) for the two models using 5-fold cv</span></span>
<span id="cb97-13"><a href="variable-selection.html#cb97-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model1) <span class="co"># p = 3</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 47 samples
##  2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 39, 38, 38, 37, 36 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   22.48806  0.5639449  17.14073
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="variable-selection.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2) <span class="co"># p = 4</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 47 samples
##  3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 36, 36, 39, 39, 38 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   23.61409  0.5475795  16.45731
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>We prefer the model with smaller RMSE and MSE. This can switch depending on the random 5-fold data set selected.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="variable-selection.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># trying an interaction model</span></span>
<span id="cb101-2"><a href="variable-selection.html#cb101-2" aria-hidden="true" tabindex="-1"></a>f3 <span class="ot">=</span> gamble <span class="sc">~</span> sex<span class="sc">*</span>income</span>
<span id="cb101-3"><a href="variable-selection.html#cb101-3" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">train</span>(f3, <span class="at">data =</span> teengamb, <span class="at">trControl =</span> cv_5fold,</span>
<span id="cb101-4"><a href="variable-selection.html#cb101-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb101-5"><a href="variable-selection.html#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="variable-selection.html#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model3) <span class="co"># even better</span></span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 47 samples
##  2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 38, 37, 37, 37, 39 
## Resampling results:
## 
##   RMSE      Rsquared   MAE    
##   18.99806  0.5795592  12.4163
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>residualPlots(lm(f3, data = teengamb))</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="variable-selection.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># try a transformed model</span></span>
<span id="cb103-2"><a href="variable-selection.html#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="co"># trying another model</span></span>
<span id="cb103-3"><a href="variable-selection.html#cb103-3" aria-hidden="true" tabindex="-1"></a>f4 <span class="ot">=</span> <span class="fu">sqrt</span>(gamble) <span class="sc">~</span> sex<span class="sc">*</span>income</span>
<span id="cb103-4"><a href="variable-selection.html#cb103-4" aria-hidden="true" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">train</span>(f4, <span class="at">data =</span> teengamb, <span class="at">trControl =</span> cv_5fold,</span>
<span id="cb103-5"><a href="variable-selection.html#cb103-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb103-6"><a href="variable-selection.html#cb103-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model4)</span></code></pre></div>
<pre><code>## Linear Regression 
## 
## 47 samples
##  2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 38, 37, 38, 39, 36 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   2.047325  0.599735  1.676699
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="variable-selection.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">residualPlots</span>(<span class="fu">lm</span>(f4, <span class="at">data =</span> teengamb))</span></code></pre></div>
<p><img src="Data-Analysis-with-Linear-Regression_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<pre><code>##            Test stat Pr(&gt;|Test stat|)
## sex           0.5637           0.5759
## income       -0.8070           0.4242
## Tukey test   -0.8196           0.4124</code></pre>
<p>Not comparable to previous model since the response is transformed. Should really go through variable selection process again.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assumptions-stated-and-prioritized.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data-Analysis-with-Linear-Regression.pdf", "Data-Analysis-with-Linear-Regression.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
